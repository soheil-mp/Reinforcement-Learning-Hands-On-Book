{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Deep Learning with PyTorch</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "This lesson is dedicated to implementation of complex DL models in just a bunch of lines of Python code. The lesson doesn't pretend to be a complete DL manual, as the field is very wide and dynamic. The goal is to make you familiar with the PyTorch library specifics and implementation details, assuming that you're already familiar with DL fundamentals.\n",
    "\n",
    "<img src=\"assets/pytorch1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 01. Tensors\n",
    "\n",
    "---\n",
    "\n",
    "Below are some terminology that we are going to use in this lesson:\n",
    "- A **tensor** is a multi-dimensional array.\n",
    "- A **scalar** (single number) is like a point, which is zero-dimensional. \n",
    "- A **vector** is one-dimensional like a line segment.\n",
    "- A **matrix** is a two-dimensional object. \n",
    "- Three-dimensional number collections can be represented by a parallelepiped of numbers, but don't have a separate name in the same way as matrix. We can keep this term for collections of higher dimensions, which are named **multi-dimensional matrices or tensors**.\n",
    "\n",
    "<img width=\"750px\" src=\"assets/tensor.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 01.1. Creation of Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know, the centeral purpose of NumPy library is to handle multi-dimensional arrays in a generic way. In NumPy, such arrays aren't called tensors, but, in fact, they are tensors. Tensors are used very widely in scientific computations, as generic storage for data. For example, a color image could be encoded as a 3D tensor with dimensions of width, height, and color plane.\n",
    "\n",
    "<br>\n",
    "\n",
    "A tensor is characterized by dimension and type. PyTorch supports **eight types**:\n",
    "\n",
    "1. Three float types (16-bit, 32-bit, and 64-bit) \n",
    "2. Five integer types (8-bit signed, 8-bit unsigned, 16-bit, 32-bit, and 64-bit). \n",
    "\n",
    "<br>\n",
    "\n",
    "Tensors are represented by different **type classes**. The most common ones are as follow:\n",
    "1. torch.FloatTensor (corresponding to a 32-bit float)\n",
    "2. torch.ByteTensor (an 8-bit unsigned integer)\n",
    "3. torch.LongTensor (a 64-bit signed integer). The rest can be found in the documentation.\n",
    "\n",
    "<br>\n",
    "\n",
    "There are three ways for **creating a tensor:**\n",
    "1. Calling a constructor with a type.\n",
    "2. Converting an array or a list into a tensor which the type will be same as the array's type.\n",
    "3. Calling a build-in function like torch.zeros()\n",
    "\n",
    "To give you examples of these methods, let's look at a simple session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 2.0000e+00],\n",
      "        [0.0000e+00, 2.0000e+00],\n",
      "        [4.2039e-45, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# Create a 3x2 float type tensor\n",
    "a = torch.FloatTensor(3, 2)  # PyTorch allocates memory for the tensor, but doesn't initialize it with anything\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear the tensor's content\n",
    "a.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of operation for tensors: \n",
    "- **Inplace:** Inplace operations have an underscore appended to their name and operate on the tensor's content. After this, the object itself is returned. Inplace operations are usually more efficient from a performance and memory point of view.\n",
    "\n",
    "- **Functional:** The functional equivalent creates a copy of the tensor with the performed modification, leaving the original tensor untouched.\n",
    "\n",
    "\n",
    "Another way to create a tensor by its constructor is to provide a Python iterable (for example, a list or tuple), which will be used as the contents of the newly created tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [3., 2., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert a list of lists into PyTorch tensor\n",
    "torch.FloatTensor([[1,2,3],[3,2,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **torch.tensor** method accepts the NumPy array as an argument and creates a tensor of appropriate shape from it. \n",
    "\n",
    "In the following example, we created a NumPy array initialized by zeros, which created a double (64-bit float) array by default. So, the resulting tensor has the DoubleTensor type.\n",
    "\n",
    "Usually in deep learning, double precision is not required and it adds an extra memory and performance overhead. The common practice is to use the 32-bit float type, or even the 16-bit float type, which is more than enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a zero object using NumPy\n",
    "n = np.zeros(shape = (3, 2))\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Convert the zeros object into PyTorch tensor\n",
    "b = torch.tensor(n)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a zero object using NumPy (with 32-bit float type)\n",
    "n = np.zeros(shape = (3, 2), dtype = np.float32)\n",
    "\n",
    "# Convert the zeros object into PyTorch tensor\n",
    "b = torch.tensor(n)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an option, the type of the desired tensor could be provided to the torch.tensor function in the dtype argument. Be careful to use PyTorch type and not NumPy type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 3x2 float type tensor\n",
    "n = np.zeros(shape = (3, 2))\n",
    "\n",
    "# Convert the zeros object into PyTorch tensor + Specify the type\n",
    "torch.tensor(n, dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compatibility note:** The torch.tensor() method and PyTorch type specification were added in the 0.4.0 release. In previous versions, the torch.from_numpy() function was a recommended way to convert NumPy arrays, but it had issues with handling the combination of the Python list and NumPy arrays. This from_numpy() is deprecated in favor of the more flexible torch.tensor() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 01.2. Scalar Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the 0.4.0 release, PyTorch supports zero-dimensional tensors that correspond to scalar values (on the left of Figure 1). Such tensors can be a result of some operations, such as summing all values in a tensor. These tensors can be created using torch.tensor() function. To access the actual Python value of such a tensor use item() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Creating a tensor\n",
    "a = torch.tensor([1,2,3])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum up the values inside the tensor\n",
    "s = a.sum()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the actual python values of the tensor\n",
    "s.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a zero-dimensional tensor\n",
    "torch.tensor(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 01.3. Tensor Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of operations that you can perform on tensors. You can search all of them in the PyTorch documentation at http://pytorch.org/docs.\n",
    "\n",
    "Besides the inplace and functional variants (e.g. with and without underscore, like zero() and zero_()), there are two places to look for operations: \n",
    "- **The torch package:** In this case, the function usually accepts the tensor as an argument.\n",
    "- **The tensor class:** In this case, it operates on the called tensor.\n",
    "\n",
    "Most of the time, tensor operations are trying to correspond to their NumPy equivalent, so if there is some not-very-specialized function in NumPy, then there is a good chance that PyTorch will also have it. Examples are torch.stack(), torch.transpose(), and torch.cat()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 01.4. GPU Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch supports CUDA GPUs, which means that all operations have two versions — CPU and GPU. Every tensor type that we mentioned is for CPU and has its GPU equivalent as well. Let's see the difference between these two:\n",
    "\n",
    "- **CPU:** CPU tensors reside in the _torch._ package. For example, _torch.FloatTensor_ is a 32-bit float tensor which resides in CPU memory. \n",
    "\n",
    "- **GPU:** GPU tensors reside in the _torch.cuda_ package. For example, _torch.cuda.FloatTensor_ is a 32-bit float tensor which resides in GPU counterpart. \n",
    "\n",
    "<br>\n",
    "\n",
    "There are two ways for converting from CPU to GPU:\n",
    "1. There is a tensor method **to(device)** which creates a copy of the tensor to a specified device (which could be CPU or GPU). Device type can be specified in different ways. You can pass a string name of the device, which is \"cpu\" for CPU memory or \"cuda\" for GPU. A GPU device could have an optional device index specified after the colon, for example, the second GPU card in the system could be addressed by \"cuda:1\" (index is zero-based).\n",
    "<br><br>\n",
    "2. Another slightly more efficient way to specify a device in the to() method is using the torch.device class, which accepts the device name and optional index. For accessing the device that your tensor is currently residing in, it has a device property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Create a float tensor\n",
    "a = torch.FloatTensor([2,3])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 4.])\n"
     ]
    }
   ],
   "source": [
    "print(a + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the tensor a to GPU\n",
    "c = a.cuda() \n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_tensor([ 2.,  3.], device='cuda:0')_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_tensor([ 3.,  4.], device='cuda:0')_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the device of tensor 'c'\n",
    "c.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_device(type='cuda', index=0)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following codes are taken from **PyTorch Documentation** from https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a CPU device type with index 0 (first CPU)\n",
    "device_cpu = torch.device(\"cpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor in the given device\n",
    "a = torch.FloatTensor([2,3], device = device_cpu)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check in which device tensor 'a' is\n",
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2818,  0.4418,  0.0888],\n",
       "        [-1.5435,  0.1090, -0.3801]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random tensor in the given device\n",
    "torch.randn((2,3), device = device_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 02. Gradients\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functionality of the ___automatic gradients computation___ was originally implemented in the Caffe toolkit and then became the de-facto standard in DL libraries. Computing gradients manually is extremely painful to implement and debug, even for the simplest neural network (NN). You have to calculate derivatives for all your functions, apply the chain rule, and then implement the result of the calculations, praying that everything is done right.\n",
    "\n",
    "Now defining an NN of hundreds of layers requires nothing more than assembling it from predefined building blocks. All gradients will be carefully calculated for you, backpropagated, and applied to the network. To be able to achieve this, you need to define your network architecture in terms of the DL library used, which can be different in details, but in general, must be the same: you define the order in which your network will transform inputs to outputs.\n",
    "\n",
    "<img width=\"600px\" src=\"./assets/gradient flow.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two approaches on how the gradients will be calculated:\n",
    "\n",
    "1. **Static graph:** <br> In this method, we will define our calculation in advance (we can't change this later). This graph gets processed and optimized by the DL library before any computation. This model of graph is implemented in Tensorflow and Theano.\n",
    "\n",
    "\n",
    "2. **Dynamic graph:** <br> In this method, there is no need to define the graph in advance. In here, we execute the operations for data transformation (on the actual data). Meanwhile, the library will record the order of operations. Then when it wants to calculate the gradients, it unrolls its history of operations, accumulating the gradients of network parameters. This method is also called _notebook gradients_. This method is implemented in PyTorch and Chainer.\n",
    "\n",
    "<br>\n",
    "\n",
    "Both methods have their <u>strengths and weaknesses</u>. For example:\n",
    "1. **Static graph:**\n",
    "    - Faster since all computations can be moved to the GPU, minimizing the data transfer overhead. \n",
    "    - The library has more freedom in optimizing the order that computations are performed in, or even removing parts of the graph. \n",
    "\n",
    "\n",
    "2. **Dynamic graph:**\n",
    "    - Higher computation overhead\n",
    "    - More freedom for the deveoper. For example, they can say, \"For this piece of data, I can apply this network two times, and for this piece of data, I'll use a completely different model with gradients clipped by the batch mean.\" \n",
    "    - Allows us to express our transformation more naturally and in a more \"Pythonic\" way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 02.1. Tensors and Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch tensors have a built-in gradient calculation and tracking machinery, so all you need to do is to convert the data into tensors and perform computations using the tensor's methods. Of course, if you need to access underlying low-level details, you always can, but most of the time, PyTorch does what you're expecting.\n",
    "\n",
    "There are several attributes related to gradients that every tensor has:\n",
    "- **grad:** A property which holds a tensor of the same shape containing computed gradients.\n",
    "\n",
    "\n",
    "- **is_leaf:** <br>\n",
    "    - True if this tensor is constructed by the user \n",
    "    - False if the object is a result of function transformation\n",
    "\n",
    "\n",
    "- **requires_grad:** <br>\n",
    "    - True if this tensor requires gradients to be calculated. This property is inherited from leaf tensors, which get this value from the tensor construction step (torch.zeros(), torch.tensor(), etc.). \n",
    "    - False (default) if you want gradients to be calculated for your tensor, then you need to explicitly say so.\n",
    "\n",
    "To make all of this gradient-leaf machinery clearer, let's consider this session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor that the gradients are required to be calculated\n",
    "v1 = torch.tensor([1.0, 1.0], requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor that the gradients are NOT required to be calculated\n",
    "v2 = torch.tensor([2.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add up the tensors element-wise (which is vector [3, 3])\n",
    "v_sum = v1 + v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double every element and sum the elements together\n",
    "v_res = (v_sum*2).sum()\n",
    "v_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a zero-dimension tensor with the value 12. Okay, so this is simple math so far. Now let's look at the underlying graph that our expressions created:\n",
    "\n",
    "<img width=\"500px\" src=\"assets/graph expression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check the attributes of our tensors, then we find that v1 and v2 are the only leaf nodes and every variable except v2 requires gradients to be calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is v1 constructed by user:  True\n",
      "Is v2 constructed by user:  True\n",
      "Is v_sum constructed by user:  False\n",
      "Is v_res constructed by user:  False\n"
     ]
    }
   ],
   "source": [
    "print(\"Is v1 constructed by user: \", v1.is_leaf)\n",
    "print(\"Is v2 constructed by user: \", v2.is_leaf)\n",
    "print(\"Is v_sum constructed by user: \", v_sum.is_leaf)\n",
    "print(\"Is v_res constructed by user: \", v_res.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is gradient calculation required in v1: True\n",
      "Is gradient calculation required in v2: False\n",
      "Is gradient calculation required in v_sum: True\n",
      "Is gradient calculation required in v_res: True\n"
     ]
    }
   ],
   "source": [
    "print(\"Is gradient calculation required in v1:\", v1.requires_grad)\n",
    "print(\"Is gradient calculation required in v2:\", v2.requires_grad)\n",
    "print(\"Is gradient calculation required in v_sum:\", v_sum.requires_grad)\n",
    "print(\"Is gradient calculation required in v_res:\", v_res.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradients of our graph - calculate the numerical derivative of the v_res variable, with respect to any variable that our graph has\n",
    "v_res.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following particular example, the value of 2 in v1's gradients means that by increasing every element of v1 by one, the resulting value of v_res will grow by two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the gradient value of v1\n",
    "v1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, PyTorch calculates gradients only for leaf tensors with requires_ grad=True. Indeed, if we try to check the gradients of v2 we get nothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the gradient value of v1 (there's nothing because requires_grad=False)\n",
    "v2.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason for that is efficiency in terms of computations and memory: in real\n",
    "life, our network can have millions of optimized parameters, with hundreds of intermediate operations performed on them. During gradient descent optimization, we're not interested in gradients of any intermediate matrix multiplication; the only thing we want to adjust in the model is gradients of loss with respect to model parameters (weights). Of course, if you want to calculate the gradients of input data (it could be useful if you want to generate some adversarial examples to fool the existing NN or adjust pretrained word embeddings), then you can easily do so, by passing requires_grad=True on tensor creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 03. NN Building Blocks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of predefined classes in torch.nn package. They are designed with practice in mind. For example, using minibaches, weight initialization, etc. All classes can act as a function when applied to its argument. For example, Linear class can implement a feed-forward layer with optional bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feed-forward layer with two inputs and five outputs\n",
    "l = nn.Linear(in_features = 2, out_features = 5, bias = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a float tensor\n",
    "v = torch.FloatTensor([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7293,  1.3634, -1.8153,  1.8636, -0.6975], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input the tensor into the layer\n",
    "l(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes in the torch.nn packages inherit from the nn.Module base class, which you can use to implement your own higher-level NN blocks. We'll see how you can do this in the next section.\n",
    "\n",
    "Now let's look at useful methods that all nn.Module children provide. They are as follows:\n",
    "\n",
    "- **parameters():** <br>A function that returns iterator of all variables which require gradient computation (that is, module weights) <br><br>\n",
    "- **zero_grad():** <br>This function initializes all gradients of all parameters to zero<br><br>\n",
    "- **to(device):** <br>This moves all module parameters to a given device (CPU or GPU)<br><br>\n",
    "- **state_dict():** <br>This returns the dictionary with all module parameters and is useful for model serialization<br><br>\n",
    "- **load_state_dict():** <br>This initializes the module with the state dictionary<br><br>\n",
    "\n",
    "There is a class that allows us to combine layers into the pipe: __Sequential__. Let's see an example of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a sequential and combine multiple layers\n",
    "s = nn.Sequential(nn.Linear(in_features = 2, out_features = 5), \n",
    "                  nn.ReLU(), \n",
    "                  nn.Linear(in_features = 5, out_features = 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(in_features = 20, out_features = 10),\n",
    "                  nn.Dropout(p = 0.3),\n",
    "                  nn.Softmax(dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=5, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=5, out_features=20, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (5): Dropout(p=0.3, inplace=False)\n",
       "  (6): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we defined a three-layer NN with softmax on output, applied along dimension 1 (dimension 0 is batch samples), ReLU nonlinearities and dropout. Let's push something through it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0898, 0.1059, 0.1098, 0.0902, 0.1264, 0.1270, 0.0524, 0.1016, 0.0958,\n",
       "         0.1011]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push a tensor through the layers\n",
    "tensor = torch.FloatTensor([[1, 2]]) \n",
    "s(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our minibatch is one example successfully traversed through the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight', tensor([[ 0.6234,  0.1310],\n",
       "                      [ 0.3247, -0.6904],\n",
       "                      [ 0.1651, -0.2135],\n",
       "                      [ 0.1598, -0.5944],\n",
       "                      [ 0.1943, -0.0256]])),\n",
       "             ('0.bias', tensor([-0.4926,  0.5587, -0.3992,  0.2365, -0.4030])),\n",
       "             ('2.weight',\n",
       "              tensor([[-0.2622,  0.1180, -0.0293, -0.2041,  0.1821],\n",
       "                      [-0.0837,  0.2156, -0.0450, -0.0928,  0.1783],\n",
       "                      [ 0.4273,  0.0384, -0.1828, -0.0298,  0.0754],\n",
       "                      [ 0.4340, -0.3011,  0.3925,  0.2849,  0.1206],\n",
       "                      [-0.3380, -0.2890,  0.2108, -0.0456,  0.0236],\n",
       "                      [-0.3773, -0.1504,  0.2294,  0.1127,  0.1589],\n",
       "                      [ 0.3415,  0.2268, -0.2070, -0.1196, -0.2004],\n",
       "                      [ 0.1787,  0.2802,  0.3674,  0.4372,  0.3728],\n",
       "                      [ 0.3034,  0.4025, -0.3040,  0.0568, -0.4281],\n",
       "                      [ 0.3720, -0.1388, -0.3515, -0.3248, -0.4406],\n",
       "                      [-0.2453, -0.2958,  0.0467, -0.3979, -0.4284],\n",
       "                      [ 0.2923,  0.3581,  0.0017,  0.3117, -0.2319],\n",
       "                      [ 0.0608,  0.1993, -0.2653,  0.4273, -0.0232],\n",
       "                      [-0.0778, -0.0009, -0.3848,  0.1352, -0.1309],\n",
       "                      [ 0.3908,  0.2888,  0.0044, -0.3881,  0.2804],\n",
       "                      [-0.1821, -0.3072, -0.3875, -0.1620, -0.0675],\n",
       "                      [ 0.3085, -0.2013, -0.1175, -0.0332, -0.0389],\n",
       "                      [-0.0887, -0.2230, -0.3015,  0.2359,  0.3233],\n",
       "                      [ 0.1136,  0.3848, -0.3266,  0.2993,  0.2238],\n",
       "                      [ 0.1239,  0.2774, -0.1357,  0.1715,  0.4423]])),\n",
       "             ('2.bias',\n",
       "              tensor([ 0.2609,  0.1162, -0.0064,  0.2653,  0.2537,  0.0273, -0.1657,  0.2146,\n",
       "                       0.3231,  0.2281,  0.3423,  0.0380, -0.3249,  0.0778, -0.2242,  0.2730,\n",
       "                      -0.2360,  0.2440, -0.0159,  0.3815])),\n",
       "             ('4.weight',\n",
       "              tensor([[-0.1825, -0.0635, -0.1661, -0.1470,  0.1438, -0.0544, -0.2080, -0.1732,\n",
       "                        0.2232,  0.1877, -0.2027, -0.0686, -0.0313,  0.1745,  0.0599, -0.0380,\n",
       "                        0.1429,  0.1084, -0.0462, -0.1253],\n",
       "                      [-0.0449, -0.0722, -0.0325, -0.0379, -0.1081, -0.1970,  0.0110, -0.1834,\n",
       "                        0.0395, -0.1076,  0.1253,  0.1106,  0.1614,  0.1609, -0.1290, -0.0854,\n",
       "                        0.2005,  0.1637, -0.1329, -0.2227],\n",
       "                      [-0.0375, -0.0510,  0.0277, -0.0734, -0.2101, -0.1723, -0.1036,  0.0866,\n",
       "                        0.1201, -0.1039,  0.0571, -0.0087, -0.0006,  0.0834, -0.0699, -0.1002,\n",
       "                       -0.1914, -0.1112, -0.2155,  0.2221],\n",
       "                      [ 0.1011, -0.0752, -0.0765, -0.0162, -0.0216, -0.1300, -0.1389, -0.0528,\n",
       "                        0.1706,  0.0777,  0.0611,  0.2149,  0.0968, -0.0255, -0.0743,  0.1282,\n",
       "                        0.1842,  0.0537,  0.1424, -0.1321],\n",
       "                      [-0.0231, -0.0273,  0.2191,  0.1860,  0.0198, -0.0214,  0.1808, -0.0782,\n",
       "                       -0.1998,  0.1473,  0.1564, -0.1709,  0.0192,  0.2134, -0.2158, -0.1035,\n",
       "                        0.1901, -0.1433, -0.0477,  0.2043],\n",
       "                      [ 0.0913, -0.0855,  0.1672,  0.0018,  0.1905, -0.0383,  0.0088,  0.1777,\n",
       "                       -0.2162, -0.1988, -0.1946, -0.0538, -0.2221,  0.0036,  0.1319,  0.0411,\n",
       "                        0.0757,  0.0872, -0.0398,  0.0415],\n",
       "                      [-0.2108, -0.0060, -0.1863, -0.0827, -0.0090,  0.0296, -0.0036, -0.1205,\n",
       "                        0.0971, -0.1311,  0.0798,  0.0042,  0.1933,  0.1174,  0.1802, -0.1758,\n",
       "                        0.0889, -0.1717, -0.1362, -0.1907],\n",
       "                      [ 0.0654,  0.0911, -0.1096, -0.2131,  0.1786,  0.1911,  0.0757, -0.1476,\n",
       "                       -0.0870,  0.2102,  0.1510, -0.1455, -0.0505,  0.1193, -0.1948, -0.1029,\n",
       "                        0.0270, -0.2191, -0.1112,  0.2193],\n",
       "                      [-0.0429,  0.0801, -0.0208, -0.0034,  0.0710,  0.0556,  0.1987, -0.1104,\n",
       "                       -0.0291,  0.0722,  0.0467,  0.1338,  0.2018,  0.1449,  0.0411,  0.1066,\n",
       "                        0.1918, -0.1771, -0.0962, -0.0177],\n",
       "                      [ 0.2078,  0.1368, -0.1069,  0.1352,  0.2111, -0.0573, -0.1701,  0.0336,\n",
       "                        0.1646,  0.0395,  0.1370,  0.1373, -0.0825,  0.0570,  0.0960,  0.1015,\n",
       "                        0.1188, -0.0680, -0.1460, -0.1881]])),\n",
       "             ('4.bias',\n",
       "              tensor([-0.0350, -0.0373, -0.0130, -0.2202,  0.0083,  0.2013, -0.2197, -0.0011,\n",
       "                      -0.0696, -0.2194]))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 04. Custom Layers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By subclassing the nn.Module class, you can create your own building blocks which can be stacked together, reused later, and integrated into the PyTorch framework flawlessly.\n",
    "\n",
    "<br>\n",
    "\n",
    "At its core, nn.Module provides quite rich functionality to its children. For example:\n",
    "\n",
    "1. Track all sub-modules that the current module includes.\n",
    "2. Functions for dealing with all parameters of the registered submodules. For example:\n",
    "    - Obtain the full list of the module's parameters (parameters() method)\n",
    "    - Zero the gradients (zero_grads() method)\n",
    "    - Move to CPU or GPU (to(device) method)\n",
    "    - Serialize and deserialize the module (state_dict() and load_state_dict()), \n",
    "    - Perform generic transformations using your own callable (apply() method).\n",
    "3. It establishes the convention of module application to data. Every module needs to perform its data transformation in the forward() method by overriding it\n",
    "4. There are some more functions, such as the ability to register a hook function to tweak module transformation or gradients flow, but it's more for advanced use cases.\n",
    "\n",
    "These functionalities allow us to nest our submodels into higher-level models in a unified way.\n",
    "\n",
    "<br>\n",
    "\n",
    "To create a custom module, we usually have to do two things: \n",
    "* register submodules \n",
    "* Implement the forward() method. \n",
    "\n",
    "Let's look at how this can be done for our Sequential example from the previous section, but in a more generic and reusable way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our module class that inherits nn.Module\n",
    "class OurModule(nn.Module):\n",
    "    \n",
    "    # The constructor\n",
    "    def __init__(self, num_inputs, num_classes, dropout_prob = 0.3):\n",
    "        \n",
    "        # Call the parent's constructor to initialize itself\n",
    "        super(OurModule, self).__init__()\n",
    "        \n",
    "        # Create a sequential with bunch of layers\n",
    "        self.pipe = nn.Sequential(nn.Linear(in_features = num_inputs, out_features = 5),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(in_features = 5, out_features = 20),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(in_features = 20, out_features = num_classes),\n",
    "                                  nn.Dropout(p = dropout_prob),\n",
    "                                  nn.Softmax())\n",
    "\n",
    "    # Forward function (we override the actual build-in forward function with our implementation of data transformation)\n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OurModule(\n",
      "  (pipe): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=5, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5, out_features=20, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=20, out_features=3, bias=True)\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Softmax(dim=None)\n",
      "  )\n",
      ")\n",
      "tensor([[0.3555, 0.3358, 0.3087]], grad_fn=<SoftmaxBackward>)\n",
      "Cuda's availability is False\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create ourmodule with the desired number of inputs and outputs\n",
    "    net = OurModule(num_inputs = 2, num_classes = 3)\n",
    "    print(net)\n",
    "    \n",
    "    # Create a tensor\n",
    "    v = torch.FloatTensor([[2, 3]])\n",
    "    \n",
    "    # Ask ourmodule to transform our tensor\n",
    "    out = net(v)\n",
    "    print(out)\n",
    "    \n",
    "    # Use Cuda if available\n",
    "    print(\"Cuda's availability is %s\" % torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"Data from cuda: %s\" % out.to('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 05. Final glue – loss functions and optimizers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not enough to have a network that only transforms the input into output. Our learning objective is to have a function that accepts two arguments: network's output, and desired output. This funtion should return to us a single number which tells how close the network's prediction is from the desired result. This function is called the __loss function__, and its output is the __loss value__. \n",
    "\n",
    "After getting the loss value, we will calculate the  __gradients of network parameters__ and adjust them to decrease this loss value, which pushes our model to better results in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 05.1. Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss functions reside in the nn package and are implemented as an nn.Module subclass. Usually, they accept two arguments: output from the network (prediction), and desired output (ground-truth data which is also called the label of the data sample). \n",
    "\n",
    "The most common loss functions are:\n",
    "\n",
    "- **nn.MSELoss:** <br>The Mean Square Error between arguments, which is the standard loss for regression problems.<br><br>\n",
    "\n",
    "- **nn.BCELoss and nn.BCEWithLogits:** <br>Binary Cross-Entropy loss. The first version expects a single probability value (usually it's the output of the Sigmoid layer), while the second version assumes raw scores as input and applies Sigmoid itself. The second way is usually more numerically stable and efficient. These losses (as their names suggest) are frequently used in binary classification problems.<br><br>\n",
    "\n",
    "- **nn.CrossEntropyLoss and nn.NLLLoss:** Famous \"Maximum Likelihood\" Criteria, which is used in multi-class classification problems. The first version expects raw scores for each class and applies LogSoftmax internally, while the second expects to have log probabilities as the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 05.2. Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An optimizer takes the gradients of a model parameters, and change the parameters in order to decrease the loss value. By decreasing the loss value, we're pushing our model towards desired outputs. \n",
    "\n",
    "In torch.optim package, PyTorch provides a lot of optimizer implementations. The most widely known are as follows:\n",
    "\n",
    "- **SGD:** <br>A vanilla stochastic gradient descent algorithm with optional momentum extension.<br><br>\n",
    "- **RMSprop:** <br>An optimizer, proposed by G. Hinton<br><br>\n",
    "- **Adagrad:** <br>An adaptive gradients optimizer\n",
    "\n",
    "On construction, you need to pass an iterable of Variables, which will be modified during the optimization process. The usual practice is to pass the result of the params() call of the upper-level nn.Module instance, which will return an iterable of all leaf Variables with gradients. Now, let's discuss the common blueprint of a training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through input and output batches of data\n",
    "for batch_samples, batch_labels in iterate_batches(data, batch_size = 32): \n",
    "    \n",
    "    # Convert the samples into tensor\n",
    "    batch_samples_t = torch.tensor(batch_samples)\n",
    "    \n",
    "    # Convert the labels into tensor\n",
    "    batch_labels_t = torch.tensor(batch_labels)\n",
    "    \n",
    "    # Pass the data samples to the network\n",
    "    out_t = net(batch_samples_t) \n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss_t = loss_function(out_t, batch_labels_t) \n",
    "    \n",
    "    # Calcuate the gradients for every leaf tensor with require_grad=True (Every time a gradient is calculated, it is accumulated in the tensor.grad field)\n",
    "    loss_t.backward()\n",
    "    \n",
    "    # Apply the optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Do zero gradients of parameters\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 06. Monitoring with TensorBoard\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have ever tried to train a NN on your own, then you may know how painful it is to tune the hyperparameters. Of course, with practice and experience, you'll develop a strong intuition about the possible causes of problems, but intuition needs input data about what's going on inside your network. So you need to be able to peek inside your training process somehow and observe its dynamics. Below is a list of things that you should observe during your training, which usually includes the following:\n",
    "- __Loss Value:__, This can consists of several components like base loss and regularization losses. You should monitor both total loss and individual components over time.\n",
    "- __Validation results__ on training and test sets.\n",
    "- __Gradients and weights__ statistics.\n",
    "- __Hyperparameters__ that get adjusted over time like learning rate.\n",
    "\n",
    "The list could be much longer and include domain-specific metrics, such as word embeddings' projections, audio samples, and images generated by GAN. You also may want to monitor values related to training speed, like how long an epoch takes, to see the effect of your optimizations or problems with hardware.\n",
    "\n",
    "To make a long story short, you need a generic solution to track lots of values over time and represent them for analysis, preferably developed specially for DL. Luckily, such tools exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 06.1. TensorBoard 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow included a special tool called TensorBoard, developed to observe and analyze various NN characteristics over training. TensorBoard is a powerful, generic solution with a large community and it looks quite pretty:\n",
    "\n",
    "<img width=\"600px\" src=\"assets/tensorboard.png\">\n",
    "\n",
    "TensorBoard is a Python web service which you can start on your computer, passing it the directory where your training process will save values to be analyzed. Then you point your browser to TensorBoard's port (usually 6006), and it shows you an interactive web interface with values updated in real-time. TensorBoard was deployed as a part of TensorFlow, but recently, it has been moved to a separate project and has its own package name. \n",
    "\n",
    "In theory, this is all you need to start monitoring your networks, as the tensorflow package provides you with classes to write the data that TensorBoard will be able to read. However, it's not very practical, as those classes are very low level. To overcome this, there are several third-party open-source libraries that provide\n",
    "a convenient high-level interface. One of my favorites, which is used in this book, is tensorboard-pytorch (https://github.com/lanpa/tensorboard-pytorch). You can install it using the following line:\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><code>pip install tensorboard-pytorch</code></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard-pytorch in /anaconda3/lib/python3.6/site-packages (0.7.1)\r\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (from tensorboard-pytorch) (1.16.3)\r\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from tensorboard-pytorch) (1.12.0)\r\n",
      "Requirement already satisfied: protobuf in /anaconda3/lib/python3.6/site-packages (from tensorboard-pytorch) (3.9.0)\r\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.6/site-packages (from protobuf->tensorboard-pytorch) (41.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard-pytorch\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 06.2. Plotting stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you an impression of how simple tensorboard-pytorch is, let's consider a small example that is not related to NNs, but is just about writing stuff into TensorBoard (the full example code is in Chapter03/02_tensorboard.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import math\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Begin the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Writer of data\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Functions that are going to be visualized\n",
    "    funcs = {\"sin\": math.sin, \"cos\": math.cos, \"tan\": math.tan}\n",
    "\n",
    "    # Loop over angle ranges in degrees\n",
    "    for angle in range(-360, 360):\n",
    "        \n",
    "        # Convert the angle range (in degrees) into radians\n",
    "        angle_rad = angle * math.pi / 180\n",
    "        \n",
    "        # Loop over functions and their names\n",
    "        for name, fun in funcs.items():\n",
    "            \n",
    "            # Calculate our functions' values\n",
    "            val = fun(angle_rad)\n",
    "            \n",
    "            # Add every value to the writer\n",
    "            writer.add_scalar(name, val, angle)\n",
    "\n",
    "    # Close the writer\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, SummaryWriter will create a unique directory under the runs directory for every launch, to be able to compare different launches of training. Names of the new directory include the current date and time, and hostname. To override this, you can pass the log_dir argument to SummaryWriter. You also can add a suffix to the name of the directory by passing a comment option, for example to capture different experiments' semantics, such as dropout=0.3 or strong_regularisation.\n",
    "\n",
    "Note that the writer does a periodical flush (by default, every two minutes), so even in the case of a lengthy optimization process, you still will see your values.\n",
    "\n",
    "The result of running this will be zero output on the console, but you will see a new directory created inside the runs directory with a single file. To look at the result, we need to start TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir runs --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can open http://localhost:6006 in your browser to see something like this:\n",
    "\n",
    "<img width=\"800px\" src=\"assets/plot.png\">\n",
    "\n",
    "The graphs are interactive, so you can hover over them with your mouse to see the actual values and select regions to zoom into details. To zoom out, double-click inside the graph. If you run your program several times, then you will see several items\n",
    "in the \"runs\" list on the left, which can be enabled and disabled in any combinations, allowing you to compare the dynamics of several optimizations. TensorBoard\n",
    "allows you to analyze not only scalar values but also images, audio, text data, and embeddings, and it can even show you the structure of your network. Refer to the documentation of tensorboard-pytorch and tensorboard for all those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 07. Example – GAN on Atari images\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll train a __Generative Adversarial Networks (GANs)__ to generate screenshots of various Atari games.\n",
    "\n",
    "The simplest GAN architecture is this: we have two networks and the first works as a \"cheater\" (that is ___generator___), and the other is a \"detective\" (that is ___discriminator___). Both networks compete with each other: The generator tries to generate fake data, which will be hard for the discriminator to distinguish from your dataset, and the discriminator tries to detect the generated data samples. Over time, both networks improve their skills. The generator produces more and more realistic data samples, and the discriminator invents more sophisticated ways to distinguish the fake items. \n",
    "\n",
    "Practical usage of GANs includes image quality improvement, realistic image generation, and feature learning. So, let's get started. The whole example code is in the file Chapter03/03_atari_ gan.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import random\n",
    "import argparse\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "import torchvision.utils as vutils\n",
    "import gym\n",
    "import gym.spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = gym.logger\n",
    "log.set_level(gym.logger.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LATENT_VECTOR_SIZE = 100\n",
    "DISCR_FILTERS = 64\n",
    "GENER_FILTERS = 64\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 64 # dimension input image will be rescaled\n",
    "LEARNING_RATE = 0.0001\n",
    "REPORT_EVERY_ITER = 100\n",
    "SAVE_IMAGE_EVERY_ITER = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input wrapper class that inherits gym.ObservationWrapper\n",
    "class InputWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Preprocessing of input numpy array:\n",
    "        1. resize image into predefined size\n",
    "        2. move color channel axis to a first place\n",
    "    \"\"\"\n",
    "    # The constructor\n",
    "    def __init__(self, *args):\n",
    "        \n",
    "        # Call the parent's constructor to initialize itself\n",
    "        super(InputWrapper, self).__init__(*args)\n",
    "        \n",
    "        # The type of observation space should be Box\n",
    "        assert isinstance(self.observation_space, gym.spaces.Box)\n",
    "        \n",
    "        # The old observation space\n",
    "        old_space = self.observation_space\n",
    "        \n",
    "        # The new observation space\n",
    "        self.observation_space = gym.spaces.Box(self.observation(old_space.low), self.observation(old_space.high), dtype=np.float32)\n",
    "\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        \n",
    "        # Resize image from 210×160 (standard Atari resolution) to a square size 64×64\n",
    "        new_obs = cv2.resize(observation, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        \n",
    "        # Move color plane of the image from the last position to the first (e.g. 64x64x3 => 3x64x64) - This is PyTorch convention for using convolution layers\n",
    "        new_obs = np.moveaxis(new_obs, 2, 0)\n",
    "        \n",
    "        # Cast the image from bytes to float + rescale its values to a 0..1 range\n",
    "        return new_obs.astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define two nn.Module classes: Discriminator and Generator. The first takes our scaled color image as input and, by applying five layers of convolutions, converts it into a single number, passed through a sigmoid nonlinearity. The output from Sigmoid is interpreted as the probability that Discriminator thinks our input image is from the real dataset.\n",
    "\n",
    "Generator takes as input a vector of random numbers (latent vector) and using the \"transposed convolution\" operation (it is also known as deconvolution), converts this vector into a color image of the original resolution.\n",
    "\n",
    "<img width=\"800px\" src=\"./assets/gymenv.png\">\n",
    "\n",
    "As input, we'll use screenshots from several Atari games played simultaneously by a random agent. Figure 6 is an example of what the input data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator class that inherits nn.Module\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator Network.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, input_shape):\n",
    "        \n",
    "        # Call the parent's constructor to initialize itself\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Pipe for colvolving image into the single number - output is the probability that network thinks the input image is real\n",
    "        self.conv_pipe = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape[0], out_channels=DISCR_FILTERS, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS, out_channels=DISCR_FILTERS*2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS*2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS*2, out_channels=DISCR_FILTERS*4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS*4),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS*4, out_channels=DISCR_FILTERS*8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(DISCR_FILTERS*8),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=DISCR_FILTERS*8, out_channels=1, kernel_size=4, stride=1, padding=0),\n",
    "            nn.Sigmoid())\n",
    "    \n",
    "    # Forward function \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv_pipe(x)\n",
    "        return conv_out.view(-1, 1).squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator class that inherits nn.Module\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator Network.\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(self, output_shape):\n",
    "        \n",
    "        # Call the parent's constructor to initialize itself\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Pip for deconvolving the input vector into 3x64x64 image\n",
    "        self.pipe = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=LATENT_VECTOR_SIZE, out_channels=GENER_FILTERS*8, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 8),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS*8, out_channels=GENER_FILTERS*4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 4),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS*4, out_channels=GENER_FILTERS*2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS * 2),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS*2, out_channels=GENER_FILTERS, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(GENER_FILTERS),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ConvTranspose2d(in_channels=GENER_FILTERS, out_channels=output_shape[0], kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh())\n",
    "\n",
    "    # Forward function \n",
    "    def forward(self, x):\n",
    "        return self.pipe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the batches\n",
    "def iterate_batches(envs, batch_size = BATCH_SIZE):\n",
    "    \n",
    "    # Get the environments and reset them\n",
    "    batch = [e.reset() for e in envs]\n",
    "    \n",
    "    # Environment generator (randomely)\n",
    "    env_gen = iter(lambda: random.choice(envs), None)\n",
    "    \n",
    "    # Infinite loop\n",
    "    while True:\n",
    "        \n",
    "        # Get the next environment\n",
    "        e = next(env_gen)\n",
    "        \n",
    "        # Take a random action and get the tuple of (new observation, reward, terminal state, extra information)\n",
    "        obs, reward, is_done, _ = e.step(e.action_space.sample())\n",
    "        \n",
    "        # If the mean of observations is above 0.01\n",
    "        if np.mean(obs) > 0.01:\n",
    "            \n",
    "            # Append the observations into the batch\n",
    "            batch.append(obs)\n",
    "            \n",
    "        # If length of batch is equal to batch size\n",
    "        if len(batch) == batch_size:\n",
    "            \n",
    "            # Normalising input between -1 to 1\n",
    "            batch_np = np.array(batch, dtype=np.float32) * 2.0 / 255.0 - 1.0\n",
    "            \n",
    "            # Yield the batch tensor\n",
    "            yield torch.tensor(batch_np)\n",
    "            \n",
    "            # Clear the batch\n",
    "            batch.clear()\n",
    "            \n",
    "        # If terminal state\n",
    "        if is_done:\n",
    "            \n",
    "            # Reset\n",
    "            e.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Making new env: Breakout-v0\n",
      "INFO: Making new env: AirRaid-v0\n",
      "INFO: Making new env: Pong-v0\n",
      "INFO: Iter 100: gen_loss=6.203e+00, dis_loss=2.577e-02\n",
      "INFO: Iter 200: gen_loss=7.509e+00, dis_loss=1.073e-03\n",
      "INFO: Iter 300: gen_loss=8.070e+00, dis_loss=5.415e-04\n",
      "INFO: Iter 400: gen_loss=8.376e+00, dis_loss=3.789e-04\n",
      "INFO: Iter 500: gen_loss=8.583e+00, dis_loss=2.952e-04\n",
      "INFO: Iter 600: gen_loss=8.727e+00, dis_loss=2.408e-04\n",
      "INFO: Iter 700: gen_loss=8.932e+00, dis_loss=1.859e-04\n",
      "INFO: Iter 800: gen_loss=9.104e+00, dis_loss=1.525e-04\n",
      "INFO: Iter 900: gen_loss=9.334e+00, dis_loss=1.190e-04\n",
      "INFO: Iter 1000: gen_loss=9.505e+00, dis_loss=9.962e-05\n",
      "INFO: Iter 1100: gen_loss=9.925e+00, dis_loss=6.740e-05\n",
      "INFO: Iter 1200: gen_loss=9.975e+00, dis_loss=6.361e-05\n",
      "INFO: Iter 1300: gen_loss=9.829e+00, dis_loss=7.089e-05\n",
      "INFO: Iter 1400: gen_loss=1.014e+01, dis_loss=5.115e-05\n",
      "INFO: Iter 1500: gen_loss=1.045e+01, dis_loss=3.787e-05\n",
      "INFO: Iter 1600: gen_loss=1.076e+01, dis_loss=2.826e-05\n",
      "INFO: Iter 1700: gen_loss=1.082e+01, dis_loss=2.625e-05\n",
      "INFO: Iter 1800: gen_loss=1.089e+01, dis_loss=2.427e-05\n",
      "INFO: Iter 1900: gen_loss=1.099e+01, dis_loss=2.182e-05\n",
      "INFO: Iter 2000: gen_loss=1.101e+01, dis_loss=2.101e-05\n",
      "INFO: Iter 2100: gen_loss=1.114e+01, dis_loss=1.870e-05\n",
      "INFO: Iter 2200: gen_loss=1.138e+01, dis_loss=1.498e-05\n",
      "INFO: Iter 2300: gen_loss=1.130e+01, dis_loss=1.583e-05\n",
      "INFO: Iter 2400: gen_loss=1.113e+01, dis_loss=1.819e-05\n",
      "INFO: Iter 2500: gen_loss=1.115e+01, dis_loss=1.777e-05\n",
      "INFO: Iter 2600: gen_loss=1.114e+01, dis_loss=1.766e-05\n",
      "INFO: Iter 2700: gen_loss=1.131e+01, dis_loss=1.504e-05\n",
      "INFO: Iter 2800: gen_loss=1.148e+01, dis_loss=1.274e-05\n",
      "INFO: Iter 2900: gen_loss=1.155e+01, dis_loss=1.187e-05\n",
      "INFO: Iter 3000: gen_loss=1.160e+01, dis_loss=1.121e-05\n",
      "INFO: Iter 3100: gen_loss=1.174e+01, dis_loss=9.852e-06\n",
      "INFO: Iter 3200: gen_loss=1.186e+01, dis_loss=8.785e-06\n",
      "INFO: Iter 3300: gen_loss=1.205e+01, dis_loss=7.468e-06\n",
      "INFO: Iter 3400: gen_loss=1.220e+01, dis_loss=6.504e-06\n",
      "INFO: Iter 3500: gen_loss=1.252e+01, dis_loss=4.979e-06\n",
      "INFO: Iter 3600: gen_loss=1.275e+01, dis_loss=4.101e-06\n",
      "INFO: Iter 3700: gen_loss=1.293e+01, dis_loss=3.522e-06\n",
      "INFO: Iter 3800: gen_loss=1.311e+01, dis_loss=3.021e-06\n",
      "INFO: Iter 3900: gen_loss=1.333e+01, dis_loss=2.528e-06\n",
      "INFO: Iter 4000: gen_loss=1.345e+01, dis_loss=2.308e-06\n",
      "INFO: Iter 4100: gen_loss=1.354e+01, dis_loss=2.077e-06\n",
      "INFO: Iter 4200: gen_loss=1.371e+01, dis_loss=1.849e-06\n",
      "INFO: Iter 4300: gen_loss=1.378e+01, dis_loss=1.716e-06\n",
      "INFO: Iter 4400: gen_loss=1.362e+01, dis_loss=1.841e-06\n",
      "INFO: Iter 4500: gen_loss=1.346e+01, dis_loss=2.064e-06\n",
      "INFO: Iter 4600: gen_loss=1.333e+01, dis_loss=2.273e-06\n",
      "INFO: Iter 4700: gen_loss=1.310e+01, dis_loss=2.663e-06\n",
      "INFO: Iter 4800: gen_loss=1.299e+01, dis_loss=2.880e-06\n",
      "INFO: Iter 4900: gen_loss=1.307e+01, dis_loss=2.651e-06\n",
      "INFO: Iter 5000: gen_loss=1.310e+01, dis_loss=2.592e-06\n",
      "INFO: Iter 5100: gen_loss=1.308e+01, dis_loss=2.625e-06\n",
      "INFO: Iter 5200: gen_loss=1.297e+01, dis_loss=2.884e-06\n",
      "INFO: Iter 5300: gen_loss=1.309e+01, dis_loss=2.601e-06\n",
      "INFO: Iter 5400: gen_loss=1.308e+01, dis_loss=2.622e-06\n",
      "INFO: Iter 5500: gen_loss=1.308e+01, dis_loss=2.533e-06\n",
      "INFO: Iter 5600: gen_loss=1.309e+01, dis_loss=2.481e-06\n",
      "INFO: Iter 5700: gen_loss=1.338e+01, dis_loss=1.946e-06\n",
      "INFO: Iter 5800: gen_loss=1.362e+01, dis_loss=1.597e-06\n",
      "INFO: Iter 5900: gen_loss=1.366e+01, dis_loss=1.543e-06\n",
      "INFO: Iter 6000: gen_loss=1.368e+01, dis_loss=1.447e-06\n",
      "INFO: Iter 6100: gen_loss=1.378e+01, dis_loss=1.296e-06\n",
      "INFO: Iter 6200: gen_loss=1.393e+01, dis_loss=1.143e-06\n",
      "INFO: Iter 6300: gen_loss=1.409e+01, dis_loss=1.005e-06\n",
      "INFO: Iter 6400: gen_loss=1.422e+01, dis_loss=9.120e-07\n",
      "INFO: Iter 6500: gen_loss=1.435e+01, dis_loss=8.322e-07\n",
      "INFO: Iter 6600: gen_loss=1.447e+01, dis_loss=7.651e-07\n",
      "INFO: Iter 6700: gen_loss=1.452e+01, dis_loss=7.388e-07\n",
      "INFO: Iter 6800: gen_loss=1.465e+01, dis_loss=5.760e-07\n",
      "INFO: Iter 6900: gen_loss=1.474e+01, dis_loss=5.204e-07\n",
      "INFO: Iter 7000: gen_loss=1.482e+01, dis_loss=4.888e-07\n",
      "INFO: Iter 7100: gen_loss=1.493e+01, dis_loss=4.512e-07\n",
      "INFO: Iter 7200: gen_loss=1.508e+01, dis_loss=4.048e-07\n",
      "INFO: Iter 7300: gen_loss=1.516e+01, dis_loss=3.835e-07\n",
      "INFO: Iter 7400: gen_loss=1.517e+01, dis_loss=3.835e-07\n",
      "INFO: Iter 7500: gen_loss=1.513e+01, dis_loss=3.938e-07\n",
      "INFO: Iter 7600: gen_loss=1.516e+01, dis_loss=3.854e-07\n",
      "INFO: Iter 7700: gen_loss=1.521e+01, dis_loss=3.655e-07\n",
      "INFO: Iter 7800: gen_loss=1.534e+01, dis_loss=3.440e-07\n",
      "INFO: Iter 7900: gen_loss=1.545e+01, dis_loss=3.086e-07\n",
      "INFO: Iter 8000: gen_loss=1.557e+01, dis_loss=2.955e-07\n",
      "INFO: Iter 8100: gen_loss=1.570e+01, dis_loss=2.794e-07\n",
      "INFO: Iter 8200: gen_loss=1.581e+01, dis_loss=2.417e-07\n",
      "INFO: Iter 8300: gen_loss=1.593e+01, dis_loss=2.381e-07\n",
      "INFO: Iter 8400: gen_loss=1.600e+01, dis_loss=2.048e-07\n",
      "INFO: Iter 8500: gen_loss=1.609e+01, dis_loss=1.311e-07\n",
      "INFO: Iter 8600: gen_loss=1.612e+01, dis_loss=1.296e-07\n",
      "INFO: Iter 8700: gen_loss=1.612e+01, dis_loss=1.322e-07\n",
      "INFO: Iter 8800: gen_loss=1.610e+01, dis_loss=1.293e-07\n",
      "INFO: Iter 8900: gen_loss=1.608e+01, dis_loss=1.297e-07\n",
      "INFO: Iter 9000: gen_loss=1.608e+01, dis_loss=1.293e-07\n",
      "INFO: Iter 9100: gen_loss=1.608e+01, dis_loss=1.313e-07\n",
      "INFO: Iter 9200: gen_loss=1.610e+01, dis_loss=1.292e-07\n",
      "INFO: Iter 9300: gen_loss=1.617e+01, dis_loss=1.222e-07\n",
      "INFO: Iter 9400: gen_loss=1.627e+01, dis_loss=8.896e-08\n",
      "INFO: Iter 9500: gen_loss=1.640e+01, dis_loss=6.881e-08\n",
      "INFO: Iter 9600: gen_loss=1.654e+01, dis_loss=6.780e-08\n",
      "INFO: Iter 9700: gen_loss=1.667e+01, dis_loss=6.501e-08\n",
      "INFO: Iter 9800: gen_loss=1.678e+01, dis_loss=6.609e-08\n",
      "INFO: Iter 9900: gen_loss=1.687e+01, dis_loss=6.508e-08\n",
      "INFO: Iter 10000: gen_loss=1.696e+01, dis_loss=6.489e-08\n",
      "INFO: Iter 10100: gen_loss=1.704e+01, dis_loss=6.419e-08\n",
      "INFO: Iter 10200: gen_loss=1.707e+01, dis_loss=6.348e-08\n",
      "INFO: Iter 10300: gen_loss=1.711e+01, dis_loss=6.352e-08\n",
      "INFO: Iter 10400: gen_loss=1.716e+01, dis_loss=6.236e-08\n",
      "INFO: Iter 10500: gen_loss=1.720e+01, dis_loss=6.076e-08\n",
      "INFO: Iter 10600: gen_loss=1.725e+01, dis_loss=5.797e-08\n",
      "INFO: Iter 10700: gen_loss=1.731e+01, dis_loss=4.604e-08\n",
      "INFO: Iter 10800: gen_loss=1.726e+01, dis_loss=4.519e-08\n",
      "INFO: Iter 10900: gen_loss=1.705e+01, dis_loss=6.277e-08\n",
      "INFO: Iter 11000: gen_loss=1.703e+01, dis_loss=6.404e-08\n",
      "INFO: Iter 11100: gen_loss=1.706e+01, dis_loss=6.348e-08\n",
      "INFO: Iter 11200: gen_loss=1.714e+01, dis_loss=6.277e-08\n",
      "INFO: Iter 11300: gen_loss=1.722e+01, dis_loss=6.065e-08\n",
      "INFO: Iter 11400: gen_loss=1.727e+01, dis_loss=5.376e-08\n",
      "INFO: Iter 11500: gen_loss=1.738e+01, dis_loss=1.810e-08\n",
      "INFO: Iter 11600: gen_loss=1.746e+01, dis_loss=2.310e-09\n",
      "INFO: Iter 11700: gen_loss=1.756e+01, dis_loss=1.937e-09\n",
      "INFO: Iter 11800: gen_loss=1.770e+01, dis_loss=2.533e-09\n",
      "INFO: Iter 11900: gen_loss=1.784e+01, dis_loss=1.043e-09\n",
      "INFO: Iter 12000: gen_loss=1.798e+01, dis_loss=2.012e-09\n",
      "INFO: Iter 12100: gen_loss=1.811e+01, dis_loss=1.714e-09\n",
      "INFO: Iter 12200: gen_loss=1.821e+01, dis_loss=1.118e-09\n",
      "INFO: Iter 12300: gen_loss=1.830e+01, dis_loss=1.341e-09\n",
      "INFO: Iter 12400: gen_loss=1.838e+01, dis_loss=1.267e-09\n",
      "INFO: Iter 12500: gen_loss=1.845e+01, dis_loss=8.196e-10\n",
      "INFO: Iter 12600: gen_loss=1.851e+01, dis_loss=7.451e-10\n",
      "INFO: Iter 12700: gen_loss=1.855e+01, dis_loss=1.043e-09\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Select CPU or GPU - OPTION I\n",
    "    #parser = argparse.ArgumentParser()\n",
    "    #parser.add_argument(\"--cuda\", default=False, action='store_true')\n",
    "    #args = parser.parse_args()\n",
    "    #device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "    # Select CPU or GPU - OPTION II\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # Environment names\n",
    "    env_names = ('Breakout-v0', 'AirRaid-v0', 'Pong-v0')\n",
    "    \n",
    "    # Get the environments\n",
    "    envs = [InputWrapper(gym.make(name)) for name in env_names]\n",
    "    \n",
    "    # Get the input shape\n",
    "    input_shape = envs[0].observation_space.shape\n",
    "\n",
    "    # Generator network\n",
    "    net_discr = Discriminator(input_shape=input_shape).to(device)\n",
    "\n",
    "    # Discriminator network\n",
    "    net_gener = Generator(output_shape=input_shape).to(device)\n",
    "\n",
    "    # Loss function of Binary Cross Entropy (BCL)\n",
    "    objective = nn.BCELoss()\n",
    "\n",
    "    # Optimizer for generator\n",
    "    gen_optimizer = optim.Adam(params=net_gener.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Optimizer for discriminator\n",
    "    dis_optimizer = optim.Adam(params=net_discr.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Writer of data\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Initinialize a list for accumulated loss for generator\n",
    "    gen_losses = []\n",
    "\n",
    "    # Initinialize a list for accumulated loss for discriminator\n",
    "    dis_losses = []\n",
    "\n",
    "    # Initialize the iterator counter\n",
    "    iter_no = 0\n",
    "\n",
    "    # Initialize variables for true labels\n",
    "    true_labels_v = torch.ones(BATCH_SIZE, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Initialize variables for fake labels\n",
    "    fake_labels_v = torch.zeros(BATCH_SIZE, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Generate a random vector and iterate through it\n",
    "    for batch_v in iterate_batches(envs):\n",
    "\n",
    "        # Create a input tensor for genertor in 4D (batch, filters, x, y) + Normalize + Make it in the given device\n",
    "        gen_input_v = torch.FloatTensor(BATCH_SIZE, LATENT_VECTOR_SIZE, 1, 1).normal_(0, 1).to(device)\n",
    "\n",
    "        # Make the batch in the given device\n",
    "        batch_v = batch_v.to(device)\n",
    "\n",
    "        # Pass the batch into the Generator network\n",
    "        gen_output_v = net_gener(gen_input_v)\n",
    "\n",
    "        ###### Train the discriminator: ######\n",
    "\n",
    "        # Zero out the optimizer for discriminator\n",
    "        dis_optimizer.zero_grad()\n",
    "\n",
    "        # Pass the batch into the discriminator's network\n",
    "        dis_output_true_v = net_discr(batch_v)\n",
    "\n",
    "        # Pass the generated batch from generator into the discriminator's network\n",
    "        dis_output_fake_v = net_discr(gen_output_v.detach())       # We call the detach() function on the generator's output to prevent gradients of this training pass from flowing into the generator (detach() is a method of tensor, which makes a copy of it without connection to the parent's operation).\n",
    "\n",
    "        # Get the loss for the TRUE data samples + Get the loss for the FAKE data samples\n",
    "        dis_loss = objective(dis_output_true_v, true_labels_v) + objective(dis_output_fake_v, fake_labels_v)\n",
    "\n",
    "        # Calcuate the gradients for every leaf tensor with require_grad=True \n",
    "        dis_loss.backward()\n",
    "\n",
    "        # Apply the optimizer\n",
    "        dis_optimizer.step()\n",
    "\n",
    "        # Append the discriminator loss into dis_losses list\n",
    "        dis_losses.append(dis_loss.item())\n",
    "        \n",
    "        ###### Train the generator: ######\n",
    "\n",
    "        # Zero out the optimizer for discriminator\n",
    "        gen_optimizer.zero_grad()\n",
    "\n",
    "        # Pass the generated batch from generator into the discriminator's network\n",
    "        dis_output_v = net_discr(gen_output_v)\n",
    "\n",
    "        # Get the loss\n",
    "        gen_loss_v = objective(dis_output_v, true_labels_v)\n",
    "\n",
    "        # Calcuate the gradients for every leaf tensor with require_grad=True \n",
    "        gen_loss_v.backward()\n",
    "\n",
    "        # Apply the optimizer\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        # Append the generator loss into gen_losses list\n",
    "        gen_losses.append(gen_loss_v.item())\n",
    "        \n",
    "        ######\n",
    "\n",
    "        # Increment the iteration counter\n",
    "        iter_no += 1\n",
    "\n",
    "        # Every REPORT_EVERY_ITER times\n",
    "        if iter_no % REPORT_EVERY_ITER == 0:\n",
    "\n",
    "            # Print the losses\n",
    "            log.info(\"Iter %d: gen_loss=%.3e, dis_loss=%.3e\", iter_no, np.mean(gen_losses), np.mean(dis_losses))\n",
    "            writer.add_scalar(\"gen_loss\", np.mean(gen_losses), iter_no)\n",
    "            writer.add_scalar(\"dis_loss\", np.mean(dis_losses), iter_no)\n",
    "\n",
    "            # Make the generator losses empty\n",
    "            gen_losses = []\n",
    "\n",
    "            # Make the generator discriminator empty\n",
    "            dis_losses = []\n",
    "\n",
    "        # Every SAVE_IMAGE_EVERY_ITER times\n",
    "        if iter_no % SAVE_IMAGE_EVERY_ITER == 0:\n",
    "\n",
    "            # Feed image samples to TensorBoard\n",
    "            writer.add_image(\"fake\", vutils.make_grid(gen_output_v.data[:64]), iter_no)\n",
    "            writer.add_image(\"real\", vutils.make_grid(batch_v.data[:64]), iter_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of this example is quite a lengthy process. On a GTX 1080 GPU, 100 iterations take about 40 seconds. At the beginning, the generated images are completely random noise, but after 10k-20k iterations, the generator becomes more and more proficient at its job and the generated images become more and more similar to the real game screenshots.\n",
    "\n",
    "My experiments gave the following images after 40k-50k of training iterations (several hours on a GPU):\n",
    "\n",
    "<img width=\"700px\" src=\"./assets/generator sample.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 08. Summary\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter, we saw a quick overview of PyTorch functionality and features. We talked about basic fundamental pieces such as tensor and gradients, saw how an NN can be made from the basic building blocks, and learned how to implement those blocks ourselves. We discussed loss functions and optimizers, as well as the monitoring of training dynamics. The goal of the chapter was to give a very quick introduction to PyTorch, which will be used later in the book. For the next chapter, we're ready to start dealing with the main subject of this book: RL methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***THE END***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
