{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Rainbow Algorithm (DQN Extensions)</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "Since DeepMind published its paper on the [deep Q-network (DQN) model](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning) in 2015, many improvements have been proposed, along with tweaks to the basic architecture, which, significantly, have improved the convergence, stability, and sample efficiency of DeepMind's basic DQN. In this chapter, we will take a deeper look at some of those ideas.\n",
    "\n",
    "In October 2017, DeepMind published a paper called [Rainbow: Combining Improvements in Deep Reinforcement Learning](https://deepmind.com/research/publications/rainbow-combining-improvements-deep-reinforcement-learning), which presented the seven most important improvements to DQN. \n",
    "\n",
    "\n",
    "This chapter will go through all those methods. Then, we will check the combined system with all the methods.\n",
    "The DQN extensions that we will become familiar with are the following:\n",
    "\n",
    "* __N-step DQN:__ Improve convergence speed and stability with a simple unrolling of the Bellman equation. \n",
    "\n",
    "\n",
    "* __Double DQN:__ Deal with DQN overestimation of the values of the actions. \n",
    "\n",
    "\n",
    "* __Noisy networks:__ Make exploration more efficient by adding noise to the network weights. \n",
    "\n",
    "\n",
    "* __Prioritized replay buffer:__ why uniform sampling of our experience is not the best way to train. \n",
    "\n",
    "\n",
    "* __Dueling DQN:__ Improve convergence speed by making our network's architecture represent more closely the problem that we are solving. \n",
    "\n",
    "\n",
    "* __Categorical DQN:__ Go beyond the single expected value of the action and work with full distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 0. Content\n",
    "\n",
    "---\n",
    "\n",
    "Content of this jupyter notebook:\n",
    "\n",
    "- <a href=\"#01.-Basic-DQN\">01. Basic DQN</a>\n",
    "- <a href=\"#02.-N-Step-DQN\">02. N-Step DQN</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 01. Basic DQN\n",
    "\n",
    "---\n",
    "\n",
    "Let's start by implementing DQN using a higher level RL library (e.g. PTAN). This will make our code much more compact, which is good, as non-relevant details won't distract us from the method's logic.\n",
    "\n",
    "In the basic DQN implementation, we have three modules:\n",
    "\n",
    "- __01_dqn_basic.py:__ Implementation of basic DQN method.\n",
    "\n",
    "- __lib/dqn_model.py:__ The DQN neural network.\n",
    "\n",
    "- __lib/common.py:__ Common functions and declarations.\n",
    "\n",
    "<img src=\"assets/dir1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.1. common.py\n",
    "\n",
    "---\n",
    "\n",
    "Let's start by exploring common.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "from datetime import timedelta, datetime\n",
    "from types import SimpleNamespace\n",
    "from typing import Iterable, Tuple, List\n",
    "\n",
    "import ptan\n",
    "import ptan.ignite as ptan_ignite\n",
    "from ignite.engine import Engine\n",
    "from ignite.metrics import RunningAverage\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed \n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In common.py file, we have stored hyperparameters for different environments. For each environment, the hyperparameters are stored in the SimpleNamespace object (for having a simple access to a variable set of keys and values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyerparameters for different environments\n",
    "HYPERPARAMS = {\n",
    "    \n",
    "    'pong': SimpleNamespace(**{'env_name':         \"PongNoFrameskip-v4\",\n",
    "                               'stop_reward':      18.0,\n",
    "                               'run_name':         'pong',\n",
    "                               'replay_size':      100000,\n",
    "                               'replay_initial':   10000,\n",
    "                               'target_net_sync':  1000,\n",
    "                               'epsilon_frames':   10**5,\n",
    "                               'epsilon_start':    1.0,\n",
    "                               'epsilon_final':    0.02,\n",
    "                               'learning_rate':    0.0001,\n",
    "                               'gamma':            0.99,\n",
    "                               'batch_size':       32                    \n",
    "                              }),\n",
    "    \n",
    "    'breakout-small': SimpleNamespace(**{'env_name':         \"BreakoutNoFrameskip-v4\",\n",
    "                                         'stop_reward':      500.0,\n",
    "                                         'run_name':         'breakout-small',\n",
    "                                         'replay_size':      3*10 ** 5,\n",
    "                                         'replay_initial':   20000,\n",
    "                                         'target_net_sync':  1000,\n",
    "                                         'epsilon_frames':   10 ** 6,\n",
    "                                         'epsilon_start':    1.0,\n",
    "                                         'epsilon_final':    0.1,\n",
    "                                         'learning_rate':    0.0001,\n",
    "                                         'gamma':            0.99,\n",
    "                                         'batch_size':       64\n",
    "                                        }),\n",
    "    \n",
    "    'breakout': SimpleNamespace(**{'env_name':         \"BreakoutNoFrameskip-v4\",\n",
    "                                   'stop_reward':      500.0,\n",
    "                                   'run_name':         'breakout',\n",
    "                                   'replay_size':      10 ** 6,\n",
    "                                   'replay_initial':   50000,\n",
    "                                   'target_net_sync':  10000,\n",
    "                                   'epsilon_frames':   10 ** 6,\n",
    "                                   'epsilon_start':    1.0,\n",
    "                                   'epsilon_final':    0.1,\n",
    "                                   'learning_rate':    0.00025,\n",
    "                                   'gamma':            0.99,\n",
    "                                   'batch_size':       32\n",
    "                                  }),\n",
    "    \n",
    "    'invaders': SimpleNamespace(**{'env_name': \"SpaceInvadersNoFrameskip-v4\",\n",
    "                                   'stop_reward': 500.0,\n",
    "                                   'run_name': 'breakout',\n",
    "                                   'replay_size': 10 ** 6,\n",
    "                                   'replay_initial': 50000,\n",
    "                                   'target_net_sync': 10000,\n",
    "                                   'epsilon_frames': 10 ** 6,\n",
    "                                   'epsilon_start': 1.0,\n",
    "                                   'epsilon_final': 0.1,\n",
    "                                   'learning_rate': 0.00025,\n",
    "                                   'gamma': 0.99,\n",
    "                                   'batch_size': 32\n",
    "                                  }),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instance of the SimpleNamespace class provides a generic container for values;\n",
    "for example, with the preceding hyperparameters, you can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment:  PongNoFrameskip-v4\n",
      "Gamma:  0.99\n",
      "Stop Reward:  18.0\n"
     ]
    }
   ],
   "source": [
    "# Get parameters for PONG game\n",
    "params = HYPERPARAMS['pong']                        # params = common.HYPERPARAMS['pong']\n",
    "\n",
    "# Report\n",
    "print(\"Environment: \", params.env_name)\n",
    "print(\"Gamma: \", params.gamma)\n",
    "print(\"Stop Reward: \", params.stop_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Next, we will write a function that takes a batch of transitions and converts it into the set of NumPy arrays suitable for training. Every transition from ExperienceSourceFirstLast has a type of ExperienceFirstLast, which is a namedtuple with the following fields:\n",
    "- state: observation from the environment.\n",
    "- action: integer action taken by the agent.\n",
    "- reward: if we have created ExperienceSourceFirstLast with the attribute steps_count=1, it's just the immediate reward. For larger step counts, it contains the discounted sum of rewards for this number of steps.\n",
    "- last_state: if the transition corresponds to the final step in the environment, then this field is None; otherwise, it contains the last observation in the experience chain.\n",
    "\n",
    "The code of unpack_batch is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for unpacking a batch (into S, A, R, is_done, Last S)\n",
    "def unpack_batch(batch: List[ptan.experience.ExperienceFirstLast]):\n",
    "    \n",
    "    # Initialize empty list for S, A, R, is_done, Last S\n",
    "    states, actions, rewards, dones, last_states = [],[],[],[],[]\n",
    "    \n",
    "    # Loop over each batch\n",
    "    for exp in batch:\n",
    "        \n",
    "        # Append arrays of S, A, R, is_done, Last S into our list\n",
    "        state = np.array(exp.state)\n",
    "        states.append(state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        dones.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            lstate = state                         # the result will be masked anyway\n",
    "        else:\n",
    "            lstate = np.array(exp.last_state)\n",
    "        last_states.append(lstate)\n",
    "        \n",
    "    # Return the lists\n",
    "    return np.array(states, copy = False), \\\n",
    "           np.array(actions), \\\n",
    "           np.array(rewards, dtype = np.float32), \\\n",
    "           np.array(dones, dtype = np.uint8), \\\n",
    "           np.array(last_states, copy = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Note how we handle the final transitions in the batch. To avoid the special handling of such cases, for terminal transitions, we store the initial state in the last_states array. To make our calculations of the Bellman update correct, we can mask such batch entries during the loss calculation using the dones array. \n",
    "\n",
    "Another solution would be to calculate the value of the last states only for non-terminal transitions, but it would make our loss function logic a bit more complicated.\n",
    "\n",
    "Calculation of the DQN loss function is provided by the function calc_loss_dqn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function for DQN\n",
    "def calc_loss_dqn(batch, net, tgt_net, gamma, device = \"cpu\"):\n",
    "    \n",
    "    # Unpack the batch\n",
    "    states, actions, rewards, dones, next_states = unpack_batch(batch)\n",
    "\n",
    "    # Convert S, A, R, is_done, S' into a torch tensor + Convert to given device\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    # Insert a dimension of 1 to the last dimension (see the example below)\n",
    "    actions_v = actions_v.unsqueeze(dim = -1)\n",
    "    \n",
    "    # Feedforward + Gathers values along an axis specified by \"dim\" (see the example below)\n",
    "    state_action_vals = net(states_v).gather(dim = 1, index = actions_v)\n",
    "    \n",
    "    # If the last dimension is 1 then remove it  (see the example below)\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    \n",
    "    # Disabled gradient calculation\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Feedforward next state into the target network + Get the maximum along dimension 1 + Choose the first tensor\n",
    "        next_state_vals = tgt_net(next_states_v).max(dim = 1)[0]\n",
    "        \n",
    "        # Mask done_mask in next_state_vals with zeros\n",
    "        next_state_vals[done_mask] = 0.0\n",
    "\n",
    "    # Get the Bellman values\n",
    "    bellman_vals = next_state_vals.detach() * gamma + rewards_v\n",
    "    \n",
    "    # Calculate the MSE Loss\n",
    "    loss = nn.MSELoss()(state_action_vals, bellman_vals)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSOR:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "\n",
      "UNSQUEEZE (LAST DIMENSION): \n",
      "tensor([[[1],\n",
      "         [2],\n",
      "         [3]],\n",
      "\n",
      "        [[4],\n",
      "         [5],\n",
      "         [6]],\n",
      "\n",
      "        [[7],\n",
      "         [8],\n",
      "         [9]]])\n"
     ]
    }
   ],
   "source": [
    "### Unsequeeze: Returns a new tensor with a dimension of size one inserted at the specified position.\n",
    "tensor = torch.tensor([[1, 2, 3], \n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "\n",
    "print(\"TENSOR:\")\n",
    "print(tensor)\n",
    "\n",
    "print(\"\\nUNSQUEEZE (LAST DIMENSION): \")\n",
    "print(tensor.unsqueeze(dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Size:  \t\t\t\t  torch.Size([10, 1, 10, 1, 10]) \n",
      "\n",
      "Size after squeezing all dimensions: \t  torch.Size([10, 10, 10])\n",
      "Size after squeezing dimension 0:  \t  torch.Size([10, 1, 10, 1, 10])\n",
      "Size after squeezing dimension 1:  \t  torch.Size([10, 10, 1, 10])\n",
      "Size after squeezing dimension 2:  \t  torch.Size([10, 1, 10, 1, 10])\n",
      "Size after squeezing dimension 3:  \t  torch.Size([10, 1, 10, 10])\n",
      "Size after squeezing dimension 4:  \t  torch.Size([10, 1, 10, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "### Squeeze: Returns a tensor with all the dimensions of input of size 1 removed.\n",
    "\n",
    "x = torch.zeros(size = (10, 1, 10, 1, 10))\n",
    "print(\"Initial Size:  \\t\\t\\t\\t \", x.size(), \"\\n\")\n",
    "\n",
    "y = torch.squeeze(input = x)\n",
    "print(\"Size after squeezing all dimensions: \\t \", y.size())\n",
    "\n",
    "y = torch.squeeze(input = x, dim = 0)\n",
    "print(\"Size after squeezing dimension 0:  \\t \", y.size())\n",
    "\n",
    "y = torch.squeeze(input = x, dim = 1)\n",
    "print(\"Size after squeezing dimension 1:  \\t \", y.size())\n",
    "\n",
    "y = torch.squeeze(input = x, dim = 2)\n",
    "print(\"Size after squeezing dimension 2:  \\t \", y.size())\n",
    "\n",
    "y = torch.squeeze(input = x, dim = 3)\n",
    "print(\"Size after squeezing dimension 3:  \\t \", y.size())\n",
    "\n",
    "y = torch.squeeze(input = x, dim = -1)\n",
    "print(\"Size after squeezing dimension 4:  \\t \", y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSOR:\n",
      "tensor([[ 1, 10],\n",
      "        [ 5, 50]])\n",
      "\n",
      "GATHER (OPTION 1): \n",
      "tensor([[ 1,  1],\n",
      "        [50,  5]])\n",
      "\n",
      "GATHER (LAST DIMENSION): \n",
      "tensor([[ 1,  1],\n",
      "        [50,  5]])\n"
     ]
    }
   ],
   "source": [
    "### Gather: Gathers values along an axis specified by `dim`.\n",
    "\n",
    "tensor = torch.tensor([[1, 10],\n",
    "                       [5, 50]])\n",
    "\n",
    "print(\"TENSOR:\")\n",
    "print(tensor)\n",
    "\n",
    "print(\"\\nGATHER (OPTION 1): \")\n",
    "print(torch.gather(input = tensor, dim = 1, index = torch.tensor([[0,0], [1,0]])))\n",
    "\n",
    "print(\"\\nGATHER (LAST DIMENSION): \")\n",
    "print(tensor.gather(dim = 1, index = torch.tensor([[0,0], [1,0]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Besides those core DQN functions, common.py provides several utilities. The first such utility is a small class that implements epsilon decay during the training. \n",
    "\n",
    "__Epsilon__ defines the probability of taking the random action by the agent. It should be decayed from 1.0 in the beginning (fully random agent) to some small number, like 0.02 or 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for tracking the epsilon\n",
    "class EpsilonTracker:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, selector: ptan.actions.EpsilonGreedyActionSelector, params: SimpleNamespace):\n",
    "        \n",
    "        # Initialize the selector\n",
    "        self.selector = selector\n",
    "        \n",
    "        # Initialize the parameters\n",
    "        self.params = params\n",
    "        \n",
    "        # Initialize frame 0\n",
    "        self.frame(0)\n",
    "\n",
    "    # Frame funcion\n",
    "    def frame(self, frame_idx: int):\n",
    "        \n",
    "        # Get the epsilon\n",
    "        eps = self.params.epsilon_start - frame_idx / self.params.epsilon_frames\n",
    "        \n",
    "        # Set the epsilon\n",
    "        self.selector.epsilon = max(self.params.epsilon_final, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Another small function is batch_generator, which takes ExperienceReplayBuffer and infinitely generates training batches sampled from the buffer. In the beginning, the function ensures that the buffer contains the required amount of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate batches\n",
    "def batch_generator(buffer: ptan.experience.ExperienceReplayBuffer, initial: int, batch_size: int):\n",
    "    \n",
    "    # Populate the buffer with initial\n",
    "    buffer.populate(initial)\n",
    "    \n",
    "    # Infinite loop\n",
    "    while True:\n",
    "        \n",
    "        # Populate the buffer\n",
    "        buffer.populate(1)\n",
    "        \n",
    "        # Yield sample of a batch\n",
    "        yield buffer.sample(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Finally, a lengthy and useful function is called setup_ignite which attaches the needed Ignite handlers, showing the training progress and writing metrics to TensorBoard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the ignite (for tensorboard)\n",
    "def setup_ignite(engine: Engine, params: SimpleNamespace, exp_source, run_name: str, extra_metrics: Iterable[str] = ()):\n",
    "    \n",
    "    # Ignore warning regarding missing metrics\n",
    "    warnings.simplefilter(\"ignore\", category = UserWarning)\n",
    "\n",
    "    # End of episode handler\n",
    "    handler = ptan_ignite.EndOfEpisodeHandler(exp_source, bound_avg_reward = params.stop_reward)\n",
    "    \n",
    "    # Attach engine to end of episode handler\n",
    "    handler.attach(engine)\n",
    "    \n",
    "    # Frames per second (FPS) handler + attach engine\n",
    "    ptan_ignite.EpisodeFPSHandler().attach(engine)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Install event handler (when episode completed)\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.EPISODE_COMPLETED)\n",
    "    \n",
    "    # Function for when an episode completed\n",
    "    def episode_completed(trainer: Engine):\n",
    "        \n",
    "        # See if time passed 0\n",
    "        passed = trainer.state.metrics.get('time_passed', 0)\n",
    "        \n",
    "        # Report\n",
    "        print(\"Episode %d: reward=%.0f, steps=%s, \"\n",
    "              \"speed=%.1f f/s, elapsed=%s\" % (\n",
    "                  trainer.state.episode, trainer.state.episode_reward,\n",
    "                  trainer.state.episode_steps,\n",
    "                  trainer.state.metrics.get('avg_fps', 0),\n",
    "                  timedelta(seconds = int(passed))))\n",
    "\n",
    "     \n",
    "    \n",
    "    # Install event handler (when reach to bound reward)\n",
    "    @engine.on(ptan_ignite.EpisodeEvents.BOUND_REWARD_REACHED)\n",
    "    \n",
    "    # Function for when a game got solved\n",
    "    def game_solved(trainer: Engine):\n",
    "        \n",
    "        # Get the passed time\n",
    "        passed = trainer.state.metrics['time_passed']\n",
    "        \n",
    "        # Report\n",
    "        print(\"Game solved in %s, after %d episodes \"\n",
    "              \"and %d iterations!\" % (\n",
    "                  timedelta(seconds=int(passed)),\n",
    "                  trainer.state.episode, trainer.state.iteration))\n",
    "        \n",
    "        # Set .should_terminate to True\n",
    "        trainer.should_terminate = True\n",
    "\n",
    "        \n",
    "    ### TensorBoard\n",
    "    \n",
    "    # Get the current time\n",
    "    now = datetime.now().isoformat(timespec='minutes')\n",
    "    \n",
    "    # logdir name (for tensorboard)\n",
    "    logdir = f\"runs/{now}-{params.run_name}-{run_name}\"\n",
    "    \n",
    "    # Tensorboard logger (a class provided by Ignite to write into TensorBoard)\n",
    "    tb = tb_logger.TensorboardLogger(log_dir = logdir)\n",
    "    \n",
    "    # Get the running average (for loss values)\n",
    "    run_avg = RunningAverage(output_transform = lambda v: v['loss'])\n",
    "    \n",
    "    # Attach engine to RunningAverage\n",
    "    run_avg.attach(engine, \"avg_loss\")\n",
    "\n",
    "    # Metrics name (reward, steps, average reward)\n",
    "    metrics = ['reward', 'steps', 'avg_reward']\n",
    "    \n",
    "    # Output handler\n",
    "    handler = tb_logger.OutputHandler(tag = \"episodes\", metric_names = metrics)\n",
    "    \n",
    "    # Get the event (that episode has completed or not)\n",
    "    event = ptan_ignite.EpisodeEvents.EPISODE_COMPLETED\n",
    "    \n",
    "    # Attach to tensorboard\n",
    "    tb.attach(engine, log_handler = handler, event_name = event)\n",
    "\n",
    "    # write to tensorboard every 100 iterations\n",
    "    ptan_ignite.PeriodicEvents().attach(engine)\n",
    "    \n",
    "    # Metrics name (average loss, average frames per second)\n",
    "    metrics = ['avg_loss', 'avg_fps']\n",
    "    \n",
    "    # Add extra_metrics to metrics list\n",
    "    metrics.extend(extra_metrics)\n",
    "    \n",
    "    # Output handler\n",
    "    handler = tb_logger.OutputHandler(tag = \"train\", metric_names = metrics, output_transform = lambda a: a)\n",
    "    \n",
    "    # Get the event (if 100 iterations completed)  - For storing values in TensorBoard every 100 training iterations\n",
    "    event = ptan_ignite.PeriodEvents.ITERS_100_COMPLETED\n",
    "    \n",
    "    # Attach to tensorboard\n",
    "    tb.attach(engine, log_handler=handler, event_name=event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.2. dqn_model.py\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Network\n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        \n",
    "        # Inherite the parent's constructor\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # Model architect (CNN)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = input_shape[0], out_channels = 32, kernel_size = 8, stride = 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 4, stride = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = 3, stride = 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Get the output size\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        # # Model architect (FCL)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "        \n",
    "    # Get the output size of convolution\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    \n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.3. dqn_basic.py\n",
    "\n",
    "---\n",
    "\n",
    "Now, let's take a look at 01_dqn_basic.py, which creates the needed classes and starts the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import ptan\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine\n",
    "#from lib import dqn_model, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run name for ignite\n",
    "NAME = \"01_baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-20, steps=867, speed=0.0 f/s, elapsed=0:00:43\n",
      "Episode 2: reward=-21, steps=881, speed=0.0 f/s, elapsed=0:00:43\n",
      "Episode 3: reward=-21, steps=849, speed=0.0 f/s, elapsed=0:00:43\n",
      "Episode 4: reward=-19, steps=1002, speed=0.0 f/s, elapsed=0:00:43\n",
      "Episode 5: reward=-20, steps=1086, speed=0.0 f/s, elapsed=0:00:43\n",
      "Episode 6: reward=-21, steps=775, speed=0.0 f/s, elapsed=0:00:43\n",
      "Episode 7: reward=-21, steps=896, speed=0.0 f/s, elapsed=0:00:43\n",
      "Episode 8: reward=-21, steps=815, speed=0.0 f/s, elapsed=0:00:43\n",
      "Episode 9: reward=-20, steps=910, speed=0.0 f/s, elapsed=0:00:43\n",
      "Episode 10: reward=-20, steps=1025, speed=0.0 f/s, elapsed=0:00:43\n",
      "Episode 11: reward=-19, steps=1080, speed=7.7 f/s, elapsed=0:01:07\n",
      "Episode 12: reward=-18, steps=1134, speed=7.7 f/s, elapsed=0:03:33\n",
      "Episode 13: reward=-18, steps=1127, speed=7.7 f/s, elapsed=0:06:05\n",
      "Episode 14: reward=-21, steps=863, speed=7.7 f/s, elapsed=0:08:04\n",
      "Episode 15: reward=-21, steps=820, speed=7.7 f/s, elapsed=0:09:59\n",
      "Episode 16: reward=-20, steps=936, speed=7.7 f/s, elapsed=0:12:17\n",
      "Episode 17: reward=-20, steps=1020, speed=7.7 f/s, elapsed=0:14:46\n",
      "Episode 18: reward=-17, steps=1209, speed=7.7 f/s, elapsed=0:17:37\n",
      "Episode 19: reward=-20, steps=950, speed=7.6 f/s, elapsed=0:19:57\n",
      "Episode 20: reward=-20, steps=875, speed=7.6 f/s, elapsed=0:22:04\n",
      "Episode 21: reward=-19, steps=1145, speed=7.6 f/s, elapsed=0:24:47\n",
      "Episode 22: reward=-21, steps=963, speed=7.6 f/s, elapsed=0:27:04\n",
      "Episode 23: reward=-20, steps=962, speed=7.6 f/s, elapsed=0:29:27\n",
      "Episode 24: reward=-19, steps=1010, speed=7.6 f/s, elapsed=0:31:56\n",
      "Episode 25: reward=-20, steps=889, speed=7.6 f/s, elapsed=0:34:02\n",
      "Episode 26: reward=-21, steps=845, speed=7.5 f/s, elapsed=0:36:01\n",
      "Episode 27: reward=-19, steps=948, speed=7.5 f/s, elapsed=0:38:22\n",
      "Episode 28: reward=-21, steps=808, speed=7.5 f/s, elapsed=0:40:20\n",
      "Episode 29: reward=-21, steps=936, speed=7.5 f/s, elapsed=0:42:31\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Set the random seed\n",
    "    random.seed(SEED)      # random.seed(common.SEED)\n",
    "    \n",
    "    # Set the random seed for pytorch\n",
    "    torch.manual_seed(SEED)      # torch.manual_seed(common.SEED)\n",
    "    \n",
    "    # Set the hyperparameters for \"pong\" game\n",
    "    params = HYPERPARAMS['pong']        # params = common.HYPERPARAMS['pong']\n",
    "    \n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Get the environment\n",
    "    env = gym.make(params.env_name)\n",
    "    \n",
    "    # Apply a common set of wrappers for our environment\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    # Set the random seed for environment\n",
    "    env.seed(SEED)    # env.seed(common.SEED)\n",
    "\n",
    "    # Instantiat the source network (DQN) + Set the device\n",
    "    net = DQN(env.observation_space.shape, env.action_space.n).to(device)       # net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "    # Instantiat the source network (DQN)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    # Instantiate the action selector (epsilon greedy)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon = params.epsilon_start)\n",
    "    \n",
    "    # Instantiate the epsilon tracker\n",
    "    epsilon_tracker = EpsilonTracker(selector, params)     # epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    \n",
    "    # Instantiate the DQN agent\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device = device)\n",
    "\n",
    "    # Instantiate the experience source (first-last)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params.gamma)\n",
    "    \n",
    "    # Instantiate the experience reaply buffer\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size = params.replay_size)\n",
    "    \n",
    "    # Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr = params.learning_rate)\n",
    "\n",
    "    \n",
    "    # Function for processing bathes\n",
    "    def process_batch(engine, batch):\n",
    "        \n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss_v = calc_loss_dqn(batch, net, tgt_net.target_model, gamma = params.gamma, device = device)         # loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, gamma = params.gamma, device = device)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss_v.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track the epsilon\n",
    "        epsilon_tracker.frame(engine.state.iteration)\n",
    "        \n",
    "        # Every target_net_sync time\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            \n",
    "            # Sync the source weight with target weight\n",
    "            tgt_net.sync()\n",
    "            \n",
    "        return {\"loss\": loss_v.item(), \"epsilon\": selector.epsilon}\n",
    "\n",
    "    # Instantiate the engine\n",
    "    engine = Engine(process_batch)\n",
    "    \n",
    "    # Setup the engine\n",
    "    setup_ignite(engine, params, exp_source, NAME)     # common.setup_ignite(engine, params, exp_source, NAME)\n",
    "    \n",
    "    # Run engine\n",
    "    engine.run(data = batch_generator(buffer, params.replay_initial, params.batch_size))     # engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every line in the output is written at the end of the next episode, showing the episode reward, a count of steps, speed, and the total training time. For the basic DQN version, it usually takes about 1M frames to reach the mean reward of 18, so be patient. During the training, we can check the dynamics of the training process in TensorBoard, which shows charts for epsilon, raw reward values, average reward, and speed. The following charts show the reward and the number of steps for episodes. (The bottom x axis shows wall clock time, and the top axis is the episode number.)\n",
    "\n",
    "<img src=\"assets/outputdqn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 02. N-Step DQN\n",
    "\n",
    "---\n",
    "\n",
    "The first improvement that we will implement and evaluate is quite an old one. It was first introduced in the paper _Learning to Predict by the Methods of Temporal Differences, by Richard Sutton ([2] Sutton, 1988)_. To get the idea, let's look at the Bellman update used in Q-learning once again:\n",
    "\n",
    "<img width=\"400\" src=\"assets/formu1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This equation is recursive, which means that we can express ${Q(s_{t+1}, a_{t+1})}$ in terms of itself, which gives us this result:\n",
    "\n",
    "<img width=\"500\" src=\"assets/formu2.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value ${r_{a,t+1}}$ means local reward at time ${t+1}$, after issuing action a. However, if we assume that action a at the step ${t+1}$ was chosen optimally, or close to optimally, we can omit the ${max_a}$ operation and obtain this:\n",
    "\n",
    "<img width=\"450\" src=\"assets/formu3.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value can be unrolled again and again any number of times. As you may guess, this unrolling can be easily applied to our DQN update by replacing one-step transition sampling with longer transition sequences of n-steps. To understand why this unrolling will help us to speed up training, let's consider the example illustrated in Figure 8.3. Here, we have a simple environment of four states ${(s_1, s_2, s_3, s_4)}$ and the only action available at every state, except $s_4$, which is a terminal state.\n",
    "\n",
    "<img width=\"650\" src=\"assets/transitiondiag.png\">\n",
    "\n",
    "So, what happens in a one-step case? We have three total updates possible (we don't use max, as there is only one action available):\n",
    "\n",
    "<img width=\"300\" src=\"assets/formu4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine that, at the beginning of the training, we complete the preceding updates in this order. The first two updates will be useless, as our current ${Q(s_2, a)}$ and ${Q(s_3, a)}$ are incorrect and contain initial random data. The only useful update will be update 3, which will correctly assign reward $r_3$ to the state $s_3$ prior to the terminal state.\n",
    "\n",
    "Now let's complete the updates over and over again. On the second iteration, the correct value will be assigned to ${Q(s_2, a)}$, but the update of ${Q(s_1, a)}$ will still be noisy. Only on the third iteration will we get the valid values for every $Q$. So, even in a one-step case, it takes three steps to propagate the correct values to all the states.\n",
    "\n",
    "Now let's consider a two-step case. This situation again has three updates:\n",
    "\n",
    "<img width=\"350\" src=\"assets/formu5.png\">\n",
    "\n",
    "In this case, on the first loop over the updates, the correct values will be assigned to both $Q(s_2, a)$ and $Q(s_3, a)$. On the second iteration, the value of $Q(s_1, a)$ also will be properly updated. So, multiple steps improve the propagation speed of values, which improves convergence. You may be thinking, \"If it's so helpful, let's unroll the Bellman equation, say, 100 steps ahead. Will it speed up our convergence 100 times?\" Unfortunately, the answer is no.\n",
    "\n",
    "Despite our expectations, our DQN will fail to converge at all. To understand why, let's again return to our unrolling process, especially where we dropped the $max_a$. Was it correct? Strictly speaking, no. We omitted the max operation at the intermediate step, assuming that our action selection during experience gathering (or our policy) was optimal. What if it wasn't, for example, at the beginning of the training, when our agent acted randomly? In that case, our calculated value for $Q(s_t, a_t)$ may be smaller than the optimal value of the state (as some steps have been taken randomly, but not following the most promising paths by maximizing the Q-value). The more steps on which we unroll the Bellman equation, the more incorrect our update could be.\n",
    "\n",
    "Our large experience replay buffer will make the situation even worse, as it will increase the chance of getting transitions obtained from the old bad policy (dictated by old bad approximations of $Q$). This will lead to a wrong update of the current Q approximation, so it can easily break our training progress. This problem is a fundamental characteristic of RL methods, as was briefly mentioned in Chapter 4, The Cross-Entropy Method, when we talked about RL methods' taxonomy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two large classes: the off-policy and on-policy methods. The first class of off-policy methods doesn't depend on \"freshness of data.\" For example, a simple DQN is off-policy, which means that we can use very old data sampled from the environment several million steps ago, and this data will still be useful for learning. That's because we are just updating the value of the action, $Q(s_t, a_t)$, with the immediate reward, plus the discounted current approximation of the best action's value. Even if action at was sampled randomly, it doesn't matter because for this particular action at, in the state st, our update will be correct. That's why in off-policy methods, we can use a very large experience buffer to make our data closer to being independent and identically distributed (i.i.d.).\n",
    "\n",
    "On the other hand, on-policy methods heavily depend on the training data to be sampled according to the current policy that we are updating. That happens because on-policy methods are trying to improve the current policy indirectly (as in the previous n-step DQN) or directly (all of part three of the book is devoted to such methods).\n",
    "\n",
    "So, which class of methods is better? Well, it depends. Off-policy methods allow you to train on the previous large history of data or even on human demonstrations, but they usually are slower to converge. On-policy methods are typically faster, but require much more fresh data from the environment, which can be costly. Just imagine a self-driving car trained with the on-policy method. It will cost you a lot of crashed cars before the system learns that walls and trees are things that it should avoid!\n",
    "\n",
    "You may have a question: why are we talking about an n-step DQN if this \"n-stepness\" turns it into an on-policy method, which will make our large experience buffer useless? In practice, this is usually not black and white. You may still use an n-step DQN if it will help to speed up DQNs, but you need to be modest with the selection of n. Small values of two or three usually work well, because our trajectories in the experience buffer are not that different from one-step transitions. In such cases, convergence speed usually improves proportionally, but large values of n can break the training process. So, the number of steps should be tuned, but convergence speeding up usually makes it worth doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementations\n",
    "\n",
    "As the ExperienceSourceFirstLast class already supports the multi-step\n",
    "Bellman unroll, our n-step version of a DQN is extremely simple. There are only two modifications that we need to make to the basic DQN to turn it into an n-step version:\n",
    "- Pass the count of steps that we want to unroll on ExperienceSourceFirstLast creation in the steps_count parameter.\n",
    "- Pass the correct gamma to the calc_loss_dqn function. This modification is really easy to overlook, which could be harmful to convergence. As our Bellman is now n-steps, the discount coefficient for the last state in the experience chain will no longer be just $\\gamma$, but $\\gamma^n$ .\n",
    "\n",
    "You can find the whole example in Chapter08/02_dqn_n_steps.py, but here are the modified lines:\n",
    "\n",
    "```python\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params.gamma, steps_count=args.n)\n",
    "```\n",
    "\n",
    "The args.n value is a count of steps passed in command-line arguments; the default is to use four steps. Another modification is in gamma passed to the calc_loss_dqn function:\n",
    "\n",
    "```python\n",
    "loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, gamma=params.gamma**args.n, device=device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine\n",
    "\n",
    "from lib import dqn_model, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run name for ignite\n",
    "NAME = \"02_n_steps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of N-Steps\n",
    "DEFAULT_N_STEPS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-21, steps=821, speed=0.0 f/s, elapsed=0:00:56\n",
      "Episode 2: reward=-21, steps=881, speed=0.0 f/s, elapsed=0:00:56\n",
      "Episode 3: reward=-21, steps=911, speed=0.0 f/s, elapsed=0:00:56\n",
      "Episode 4: reward=-20, steps=868, speed=0.0 f/s, elapsed=0:00:56\n",
      "Episode 5: reward=-21, steps=910, speed=0.0 f/s, elapsed=0:00:56\n",
      "Episode 6: reward=-20, steps=984, speed=0.0 f/s, elapsed=0:00:56\n",
      "Episode 7: reward=-20, steps=928, speed=0.0 f/s, elapsed=0:00:56\n",
      "Episode 8: reward=-20, steps=1036, speed=0.0 f/s, elapsed=0:00:56\n",
      "Episode 9: reward=-21, steps=779, speed=0.0 f/s, elapsed=0:00:56\n",
      "Episode 10: reward=-20, steps=924, speed=0.0 f/s, elapsed=0:00:56\n",
      "Episode 11: reward=-21, steps=989, speed=7.4 f/s, elapsed=0:01:00\n",
      "Episode 12: reward=-20, steps=918, speed=7.4 f/s, elapsed=0:03:03\n",
      "Episode 13: reward=-19, steps=997, speed=7.4 f/s, elapsed=0:05:24\n",
      "Episode 14: reward=-20, steps=971, speed=7.4 f/s, elapsed=0:07:43\n",
      "Episode 15: reward=-20, steps=1049, speed=7.4 f/s, elapsed=0:10:13\n",
      "Episode 16: reward=-21, steps=868, speed=7.4 f/s, elapsed=0:12:15\n",
      "Episode 17: reward=-20, steps=894, speed=7.4 f/s, elapsed=0:14:31\n",
      "Episode 18: reward=-20, steps=1043, speed=7.4 f/s, elapsed=0:17:13\n",
      "Episode 19: reward=-21, steps=817, speed=7.4 f/s, elapsed=0:19:17\n",
      "Episode 20: reward=-19, steps=1038, speed=7.4 f/s, elapsed=0:21:54\n",
      "Episode 21: reward=-20, steps=943, speed=7.3 f/s, elapsed=0:24:16\n",
      "Episode 22: reward=-20, steps=902, speed=7.3 f/s, elapsed=0:26:35\n",
      "Episode 23: reward=-20, steps=922, speed=7.3 f/s, elapsed=0:28:46\n",
      "Episode 24: reward=-20, steps=935, speed=7.3 f/s, elapsed=0:31:00\n",
      "Episode 25: reward=-20, steps=900, speed=7.3 f/s, elapsed=0:33:03\n",
      "Episode 26: reward=-21, steps=777, speed=7.3 f/s, elapsed=0:34:51\n",
      "Episode 27: reward=-21, steps=972, speed=7.3 f/s, elapsed=0:37:08\n",
      "Episode 28: reward=-21, steps=823, speed=7.3 f/s, elapsed=0:39:07\n",
      "Episode 29: reward=-21, steps=834, speed=7.3 f/s, elapsed=0:41:05\n",
      "Episode 30: reward=-20, steps=923, speed=7.3 f/s, elapsed=0:43:17\n",
      "Episode 31: reward=-19, steps=1190, speed=7.3 f/s, elapsed=0:46:02\n",
      "Episode 32: reward=-18, steps=1147, speed=7.3 f/s, elapsed=0:48:33\n",
      "Episode 33: reward=-21, steps=1083, speed=7.3 f/s, elapsed=0:50:56\n",
      "Episode 34: reward=-17, steps=1186, speed=7.3 f/s, elapsed=0:53:39\n",
      "Episode 35: reward=-21, steps=951, speed=7.3 f/s, elapsed=0:55:50\n",
      "Episode 36: reward=-20, steps=1043, speed=7.3 f/s, elapsed=0:58:17\n",
      "Episode 37: reward=-20, steps=1201, speed=7.3 f/s, elapsed=1:01:06\n",
      "Episode 38: reward=-18, steps=1327, speed=7.3 f/s, elapsed=1:04:14\n",
      "Episode 39: reward=-21, steps=1078, speed=7.3 f/s, elapsed=1:06:46\n",
      "Episode 40: reward=-19, steps=1108, speed=7.3 f/s, elapsed=1:09:23\n",
      "Episode 41: reward=-18, steps=1235, speed=7.3 f/s, elapsed=1:12:19\n",
      "Episode 42: reward=-17, steps=1505, speed=7.3 f/s, elapsed=1:15:52\n",
      "Episode 43: reward=-20, steps=1223, speed=7.3 f/s, elapsed=1:18:38\n",
      "Episode 44: reward=-20, steps=1257, speed=7.3 f/s, elapsed=1:21:44\n",
      "Episode 45: reward=-20, steps=1009, speed=7.3 f/s, elapsed=1:24:03\n",
      "Episode 46: reward=-19, steps=1256, speed=7.2 f/s, elapsed=1:27:19\n",
      "Episode 47: reward=-19, steps=1073, speed=7.2 f/s, elapsed=1:29:53\n",
      "Episode 48: reward=-18, steps=1459, speed=7.2 f/s, elapsed=1:33:20\n",
      "Episode 49: reward=-21, steps=1338, speed=7.2 f/s, elapsed=1:36:31\n",
      "Episode 50: reward=-16, steps=1506, speed=7.2 f/s, elapsed=1:40:04\n",
      "Episode 51: reward=-16, steps=1588, speed=7.2 f/s, elapsed=1:43:46\n",
      "Episode 52: reward=-18, steps=1428, speed=7.2 f/s, elapsed=1:47:09\n",
      "Episode 53: reward=-19, steps=1201, speed=7.2 f/s, elapsed=1:49:59\n",
      "Episode 54: reward=-19, steps=1424, speed=7.2 f/s, elapsed=1:53:23\n",
      "Episode 55: reward=-18, steps=1252, speed=7.2 f/s, elapsed=1:56:23\n",
      "Episode 56: reward=-16, steps=1478, speed=7.2 f/s, elapsed=1:59:58\n",
      "Episode 57: reward=-17, steps=1947, speed=7.2 f/s, elapsed=2:04:25\n",
      "Episode 58: reward=-15, steps=1477, speed=7.2 f/s, elapsed=2:07:45\n",
      "Episode 59: reward=-17, steps=1590, speed=7.2 f/s, elapsed=2:11:20\n",
      "Episode 60: reward=-18, steps=1472, speed=7.2 f/s, elapsed=2:14:54\n",
      "Episode 61: reward=-18, steps=1538, speed=7.2 f/s, elapsed=2:52:09\n",
      "Episode 62: reward=-17, steps=1698, speed=7.2 f/s, elapsed=2:56:02\n",
      "Episode 63: reward=-19, steps=1264, speed=7.2 f/s, elapsed=2:58:57\n",
      "Episode 64: reward=-13, steps=1649, speed=7.2 f/s, elapsed=3:23:56\n",
      "Episode 65: reward=-17, steps=1622, speed=7.2 f/s, elapsed=4:19:25\n",
      "Episode 66: reward=-18, steps=2222, speed=7.2 f/s, elapsed=4:25:22\n",
      "Episode 67: reward=-15, steps=1784, speed=7.1 f/s, elapsed=4:30:05\n",
      "Episode 68: reward=-11, steps=2256, speed=7.1 f/s, elapsed=4:35:55\n",
      "Episode 69: reward=-18, steps=1630, speed=7.1 f/s, elapsed=4:40:05\n",
      "Episode 70: reward=-17, steps=1822, speed=7.1 f/s, elapsed=4:44:44\n",
      "Episode 71: reward=-19, steps=1614, speed=7.1 f/s, elapsed=4:48:57\n",
      "Episode 72: reward=-16, steps=1874, speed=7.1 f/s, elapsed=4:54:03\n",
      "Episode 73: reward=-7, steps=2479, speed=7.1 f/s, elapsed=5:00:58\n",
      "Episode 74: reward=-18, steps=1526, speed=7.0 f/s, elapsed=5:04:58\n",
      "Episode 75: reward=-9, steps=2371, speed=7.0 f/s, elapsed=5:11:08\n",
      "Episode 76: reward=-7, steps=2956, speed=7.0 f/s, elapsed=5:18:52\n",
      "Episode 77: reward=-8, steps=2543, speed=7.0 f/s, elapsed=5:25:02\n",
      "Episode 78: reward=-15, steps=2198, speed=7.0 f/s, elapsed=6:18:04\n",
      "Episode 79: reward=-14, steps=3112, speed=7.0 f/s, elapsed=6:56:20\n",
      "Episode 80: reward=-16, steps=2321, speed=6.9 f/s, elapsed=7:03:46\n",
      "Episode 81: reward=-9, steps=3294, speed=6.9 f/s, elapsed=7:13:06\n",
      "Episode 82: reward=-9, steps=3150, speed=6.9 f/s, elapsed=7:22:38\n",
      "Episode 83: reward=-10, steps=2976, speed=6.9 f/s, elapsed=7:32:33\n",
      "Episode 84: reward=6, steps=3685, speed=6.8 f/s, elapsed=7:44:09\n",
      "Episode 85: reward=-7, steps=3074, speed=6.8 f/s, elapsed=7:52:22\n",
      "Episode 86: reward=-3, steps=4059, speed=6.8 f/s, elapsed=8:19:19\n",
      "Episode 87: reward=7, steps=3922, speed=6.8 f/s, elapsed=8:28:41\n",
      "Episode 88: reward=8, steps=3699, speed=6.8 f/s, elapsed=11:12:12\n",
      "Episode 89: reward=16, steps=2418, speed=6.8 f/s, elapsed=13:08:53\n",
      "Episode 90: reward=12, steps=2608, speed=6.8 f/s, elapsed=15:20:44\n",
      "Episode 91: reward=6, steps=2613, speed=6.8 f/s, elapsed=17:51:36\n",
      "Episode 92: reward=13, steps=2285, speed=6.8 f/s, elapsed=19:05:55\n",
      "Episode 93: reward=12, steps=2657, speed=6.8 f/s, elapsed=19:14:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/soheilmohammadpour/opt/anaconda3/envs/rl/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/soheilmohammadpour/opt/anaconda3/envs/rl/lib/python3.6/site-packages/tensorboardX/event_file_writer.py\", line 220, in run\n",
      "    self._record_writer.flush()\n",
      "  File \"/Users/soheilmohammadpour/opt/anaconda3/envs/rl/lib/python3.6/site-packages/tensorboardX/event_file_writer.py\", line 69, in flush\n",
      "    self._py_recordio_writer.flush()\n",
      "  File \"/Users/soheilmohammadpour/opt/anaconda3/envs/rl/lib/python3.6/site-packages/tensorboardX/record_writer.py\", line 124, in flush\n",
      "    self._writer.flush()\n",
      "OSError: [Errno 5] Input/output error\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Set the random seed\n",
    "    random.seed(common.SEED)\n",
    "    \n",
    "    # Set the random seed for pytorch\n",
    "    torch.manual_seed(common.SEED)\n",
    "    \n",
    "    # Set the hyperparameters for \"pong\" game\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "    \n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Get the environment\n",
    "    env = gym.make(params.env_name)\n",
    "    \n",
    "    # Apply a common set of wrappers for our environment\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    # Set the random seed for environment\n",
    "    env.seed(common.SEED)\n",
    "\n",
    "    # Instantiat the source network (DQN) + Set the device\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "    # Instantiat the source network (DQN)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    # Instantiate the action selector (epsilon greedy)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "    \n",
    "    # Instantiate the epsilon tracker\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    \n",
    "    # Instantiate the DQN agent\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "    # Instantiate the experience source (first-last)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma = params.gamma, steps_count = DEFAULT_N_STEPS)\n",
    "    \n",
    "    # Instantiate the experience reaply buffer\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size = params.replay_size)\n",
    "    \n",
    "    # Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)\n",
    "\n",
    "    # Function for processing bathes\n",
    "    def process_batch(engine, batch):\n",
    "\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, gamma=params.gamma**DEFAULT_N_STEPS, device = device)         \n",
    "\n",
    "        # Backpropagation\n",
    "        loss_v.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the epsilon\n",
    "        epsilon_tracker.frame(engine.state.iteration)\n",
    "\n",
    "        # Every target_net_sync time\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "\n",
    "            # Sync the source weight with target weight\n",
    "            tgt_net.sync()\n",
    "\n",
    "        return {\"loss\": loss_v.item(), \"epsilon\": selector.epsilon}\n",
    "\n",
    "    # Instantiate the engine\n",
    "    engine = Engine(process_batch)\n",
    "\n",
    "    # Setup the engine\n",
    "    common.setup_ignite(engine, params, exp_source, f\"{NAME}={DEFAULT_N_STEPS}\")\n",
    "\n",
    "    # Run engine\n",
    "    engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The training module Chapter08/02_dqn_n_steps.py can be started as before, with the additional command-line option -n, which gives a count of steps to unroll the Bellman equation.\n",
    "\n",
    "These are charts for our baseline and n-step DQN, with n being equal to 2 and 3. As you can see, the Bellman unroll has given a significant boost in convergence.\n",
    "\n",
    "<img width=\"700\" src=\"assets/figu2.png\">\n",
    "\n",
    "As you can see in the diagram, the three-step DQN converges more than twice as fast as the simple DQN, which is a nice improvement. So, what about a larger n? The following chart shows the reward dynamics for n = 3...6:\n",
    "\n",
    "<img width=\"700\" src=\"assets/figu3.png\">\n",
    "\n",
    "As you can see, going from three steps to four has given some improvement, but it is much less than before. The variant with n = 5 is almost the same as n = 4, but n = 6 behaves worse. So, in our case, n = 4 looks optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 03. Double DQN\n",
    "\n",
    "---\n",
    "\n",
    "The next fruitful idea on how to improve a basic DQN came from DeepMind researchers in the paper titled _Deep Reinforcement Learning with Double Q-Learning ([3] van Hasselt, Guez, and Silver, 2015)_. In the paper, the authors demonstrated that the basic DQN tends to overestimate values for $Q$, which may be harmful to training performance and sometimes can lead to suboptimal policies. The root cause of this is the max operation in the Bellman equation, but the strict proof is too complicated to write down here. As a solution to this problem, the authors proposed modifying the Bellman update a bit.\n",
    "\n",
    "In the basic DQN, our target value for $Q$ looked lik′e this:\n",
    "\n",
    "<img width=\"400\" src=\"assets/formula1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q'(s_{t+1}, a)$ was Q-values calculated using our target network, so we update with the trained network every n steps. The authors of the paper proposed choosing actions for the next state using the trained network, but taking values of $Q$ from the target network. So, the new expression for target′ Q-values will look like this:\n",
    "\n",
    "<img width=\"500\" src=\"assets/formula2.png\">\n",
    "\n",
    "The authors proved that this simple tweak fixes overestimation completely, and they called this new architecture double DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "The core implementation is very simple. What we need to do is slightly modify our loss function. Let's go a step further and compare action values produced by the basic DQN and double DQN. To do this, we store a random held-out set of states and periodically calculate the mean value of the best action for every state in the evaluation set.\n",
    "\n",
    "The complete example is in Chapter08/03_dqn_double.py. Let's first take a look at the loss function:\n",
    "\n",
    "```python\n",
    "def calc_loss_double_dqn(batch, net, tgt_net, gamma, device=\"cpu\", double=True):\n",
    "        states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The double extra argument turns on and off the double DQN way of calculating\n",
    "actions to take.\n",
    "```python\n",
    "        states_v = torch.tensor(states).to(device)\n",
    "        actions_v = torch.tensor(actions).to(device)\n",
    "        rewards_v = torch.tensor(rewards).to(device)\n",
    "        done_mask = torch.BoolTensor(dones).to(device)\n",
    "```        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding section is the same as before.\n",
    "\n",
    "```python\n",
    "        actions_v = actions_v.unsqueeze(-1)\n",
    "        state_action_vals = net(states_v).gather(1, actions_v)\n",
    "        state_action_vals = state_action_vals.squeeze(-1)\n",
    "        with torch.no_grad():\n",
    "                next_states_v = torch.tensor(next_states).to(device)\n",
    "                if double:\n",
    "                        next_state_acts = net(next_states_v).max(1)[1]\n",
    "                        next_state_acts = next_state_acts.unsqueeze(-1)\n",
    "                        next_state_vals = tgt_net(next_states_v).gather(1, next_state_acts).squeeze(-1)\n",
    "                else:\n",
    "                        next_state_vals = tgt_net(next_states_v).max(1)[0]\n",
    "                next_state_vals[done_mask] = 0.0\n",
    "                exp_sa_vals = next_state_vals.detach()*gamma+rewards_v\n",
    "        return nn.MSELoss()(state_action_vals, exp_sa_vals)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the difference compared to the basic DQN loss function. If double DQN is enabled, we calculate the best action to take in the next state using our main trained network, but values corresponding to this action come from the target network. Of course, this part could be implemented in a faster way, by combining next_ states_v with states_v and calling our main network only once, but it will make the code less clear.\n",
    "\n",
    "\n",
    "The rest of the function is the same: we mask completed episodes and compute the mean squared error (MSE) loss between Q-values predicted by the network and approximated Q-values. The last function that we consider calculates the values\n",
    "of our held-out state:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing too complicated here: we just split our held-out states array into equal chunks and pass every chunk to the network to obtain action values. From those values, we choose the action with the largest value (for every state) and calculate the mean of such values. As our array with states is fixed for the whole training process, and this array is large enough (in the code we store 1,000 states), we can compare the dynamics of this mean value in both DQN variants.\n",
    "\n",
    "The rest of the 03_dqn_double.py file is almost the same; the two differences are usage of our tweaked loss function and keep randomly sampled 1,000 states for periodical evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source activate base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from ignite.engine import Engine\n",
    "\n",
    "from lib import dqn_model, common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run name for ignite\n",
    "NAME = \"03_double\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of states to do evaluation\n",
    "STATES_TO_EVALUATE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of frames to do evaluation\n",
    "EVAL_EVERY_FRAME = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_double_dqn(batch, net, tgt_net, gamma, device=\"cpu\", double=True):\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    state_action_vals = net(states_v).gather(1, actions_v)\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        if double:\n",
    "            next_state_acts = net(next_states_v).max(1)[1]\n",
    "            next_state_acts = next_state_acts.unsqueeze(-1)\n",
    "            next_state_vals = tgt_net(next_states_v).gather(1, next_state_acts).squeeze(-1)\n",
    "        else:\n",
    "            next_state_vals = tgt_net(next_states_v).max(1)[0]\n",
    "        next_state_vals[done_mask] = 0.0\n",
    "        exp_sa_vals = next_state_vals.detach() * gamma + rewards_v\n",
    "    return nn.MSELoss()(state_action_vals, exp_sa_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss calculation for double DQN\n",
    "def calc_loss_double_dqn(batch, net, tgt_net, gamma, device = \"cpu\", double = True):\n",
    "    \n",
    "    # Unpack the batch\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "\n",
    "    # Get the S, A, R, done_mask + Convert it to tensor + Convert it to device\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "\n",
    "    # Unsqeeze the actions (add extra dimension to last dimension)\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    \n",
    "    # Feedforward in source network + Gathers values along an axis specified by \"dim\" \n",
    "    state_action_vals = net(states_v).gather(1, actions_v)\n",
    "    \n",
    "    # If the last dimension is 1 then remove it\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    \n",
    "    # Disabled gradient calculation\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Get S' + Convert it to tensro + Convert it to device\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        \n",
    "        # If \"double\" is true\n",
    "        if double:\n",
    "            \n",
    "            # Feedforward in source network  + Get the max in dimension 1 + Get the 2nd value\n",
    "            next_state_acts = net(next_states_v).max(1)[1]\n",
    "            \n",
    "            # Unsqeeze the next actions (add extra dimension to last dimension)\n",
    "            next_state_acts = next_state_acts.unsqueeze(-1)\n",
    "            \n",
    "            # Feedforward in target network + Gathers values along an axis specified by \"dim\" + If the last dimension is 1 then remove it\n",
    "            next_state_vals = tgt_net(next_states_v).gather(1, next_state_acts).squeeze(-1)\n",
    "        \n",
    "        # If \"double\" is false\n",
    "        else:\n",
    "            \n",
    "            # Feedforward in target network  + Get the max in dimension 1 + Get the 1st value\n",
    "            next_state_vals = tgt_net(next_states_v).max(1)[0]\n",
    "            \n",
    "        # Mask S'\n",
    "        next_state_vals[done_mask] = 0.0\n",
    "        \n",
    "        # Modifued Bellman equation\n",
    "        exp_sa_vals = next_state_vals.detach() * gamma + rewards_v\n",
    "        \n",
    "    # Calculate loss\n",
    "    loss = nn.MSELoss()(state_action_vals, exp_sa_vals) \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-21, steps=816, speed=0.0 f/s, elapsed=0:00:41\n",
      "Episode 2: reward=-20, steps=933, speed=0.0 f/s, elapsed=0:00:41\n",
      "Episode 3: reward=-20, steps=949, speed=0.0 f/s, elapsed=0:00:41\n",
      "Episode 4: reward=-20, steps=921, speed=0.0 f/s, elapsed=0:00:41\n",
      "Episode 5: reward=-20, steps=927, speed=0.0 f/s, elapsed=0:00:41\n",
      "Episode 6: reward=-20, steps=955, speed=0.0 f/s, elapsed=0:00:41\n",
      "Episode 7: reward=-21, steps=911, speed=0.0 f/s, elapsed=0:00:41\n",
      "Episode 8: reward=-19, steps=1100, speed=0.0 f/s, elapsed=0:00:41\n",
      "Episode 9: reward=-21, steps=778, speed=0.0 f/s, elapsed=0:00:41\n",
      "Episode 10: reward=-21, steps=905, speed=0.0 f/s, elapsed=0:00:41\n",
      "Episode 11: reward=-21, steps=881, speed=6.9 f/s, elapsed=0:00:52\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Set the random seed\n",
    "    random.seed(common.SEED)\n",
    "    \n",
    "    # Set the random seed for pytorch\n",
    "    torch.manual_seed(common.SEED)\n",
    "    \n",
    "    # Get the environment parameters for \"pong\"\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "    \n",
    "    # Set the device type\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Set double to true\n",
    "    double = True\n",
    "\n",
    "    # Make the environment\n",
    "    env = gym.make(params.env_name)\n",
    "    \n",
    "    # Apply a common set of wrappers for Atari games\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    # Set the random seed for environment\n",
    "    env.seed(common.SEED)\n",
    "\n",
    "    # Instantiate the source network\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "    # Instantiate the target network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    # Instantiate the action  selector (epsilon-greedy)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon = params.epsilon_start)\n",
    "    \n",
    "    # Instantiate the epsilon tracker\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    \n",
    "    # Instantiate the DQN agent\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "    # Instantiate the experience source (first-last)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma = params.gamma)\n",
    "    \n",
    "    # Instantiate the buffer\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size = params.replay_size)\n",
    "    \n",
    "    # Instantiate the optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr = params.learning_rate)\n",
    "\n",
    "    # Function for processing batches\n",
    "    def process_batch(engine, batch):\n",
    "        \n",
    "        # Reset the optimizer's weight to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calcualte the loss\n",
    "        loss_v = calc_loss_double_dqn(batch, net, tgt_net.target_model, gamma = params.gamma, device = device, double = double)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss_v.backward()\n",
    "        \n",
    "        # Run optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track the epsilon\n",
    "        epsilon_tracker.frame(engine.state.iteration)\n",
    "        \n",
    "        # Every target_net_sync times\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            \n",
    "            # Sync the source network to target network\n",
    "            tgt_net.sync()\n",
    "            \n",
    "        # Every EVAL_EVERY_FRAME times \n",
    "        if engine.state.iteration % EVAL_EVERY_FRAME == 0:\n",
    "            \n",
    "            # Create a named attribute from an object\n",
    "            eval_states = getattr(engine.state, \"eval_states\", None)\n",
    "            \n",
    "            # If eval_states is None\n",
    "            if eval_states is None:\n",
    "                \n",
    "                # Sample from buffer\n",
    "                eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
    "                \n",
    "                # Extract the states\n",
    "                eval_states = [np.array(transition.state, copy = False) for transition in eval_states]\n",
    "                \n",
    "                # Convert to numpy array\n",
    "                eval_states = np.array(eval_states, copy = False)\n",
    "                \n",
    "                # Update the eval_states in engine\n",
    "                engine.state.eval_states = eval_states\n",
    "                \n",
    "            # Calculate the state values + Update the values in engine\n",
    "            engine.state.metrics[\"values\"] = common.calc_values_of_states(states = eval_states, net = net, device = device)\n",
    "            \n",
    "        return {\"loss\": loss_v.item(), \"epsilon\": selector.epsilon,}\n",
    "\n",
    "    # Instantiate the engine\n",
    "    engine = Engine(process_batch)\n",
    "    \n",
    "    # Setup the engine\n",
    "    common.setup_ignite(engine, params, exp_source, f\"{NAME}={double}\", extra_metrics = ('values',))\n",
    "    \n",
    "    # Run the engine\n",
    "    engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "To train a double DQN, with the extension enabled, pass the --double command- line argument:\n",
    "\n",
    "```\n",
    "Chapter08$ ./03_dqn_double.py --cuda --double\n",
    "```\n",
    "\n",
    "To compare the Q-values with those for a basic DQN, train it again without the --double option. Training will take some time, depending on your computing power. On GTX 1080 Ti, 1M frames take about two hours. In addition, I've noticed that double extension convergence is less frequent than with the basic version.\n",
    "\n",
    "In basic DQN, about one in 10 runs fails to converge, but with double extension, it is about one in three. Very likely, hyperparameters need to be tuned, but our comparison is supposed to check the effect of extensions without touching hyperparameters.\n",
    "\n",
    "Anyway, as you can see on the following chart, double DQN shows better reward dynamics (the average reward for episodes starts to grow earlier), but the final time to solve the game is almost the same as in our baseline.\n",
    "\n",
    "<img width=\"700\" src=\"assets/figure8.6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the standard metrics, the example also outputs the mean value for the held-out set of states. They are shown on the next chart; indeed, basic DQN does overestimation of values, so values are decreasing after some level. In contrast, double DQN grows more consistently. In my experiments, double DQN has only a small effect on the training time, but this doesn't necessary mean that double DQN is useless, as Pong is a simple environment. In more complicated games, double DQN could give better results.\n",
    "\n",
    "<img width=\"700\" src=\"assets/figure8.7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 04. Noisy networks\n",
    "\n",
    "---\n",
    "\n",
    "The next improvement is to addresses the \"environment exploration\" problem. The paper that we will draw from is called _Noisy Networks for Exploration ([4] Fortunato and others, 2017)_ and it has a very simple idea for learning exploration characteristics during training instead of having a separate schedule related to exploration.\n",
    "\n",
    "Classical DQN achieves exploration by choosing random actions with a specially defined hyperparameter epsilon, which is slowly decreased over time from 1.0 (fully random actions) to some small ratio of 0.1 or 0.02. This process works well for simple environments with short episodes, without much non-stationarity during the game; but even in such simple cases, it requires tuning to make the training processes efficient.\n",
    "\n",
    "In the Noisy Networks paper, the authors proposed a quite simple solution that, nevertheless, works well. They add noise to the weights of fully connected layers of the network and adjust the parameters of this noise during training using backpropagation. Of course, this method shouldn't be confused with \"the network decides where to explore more,\" which is a much more complex approach that also has widespread support. We will discuss advanced exploration techniques in Chapter 21, Advanced Exploration.\n",
    "\n",
    "The authors proposed two ways of adding the noise, both of which work according to their experiments, but they have different computational overheads:\n",
    "\n",
    "1. __Independent Gaussian noise:__ for every weight in a fully connected layer, we have a random value that we draw from the normal distribution. Parameters of the noise, 𝜇 and 𝜎, are stored inside the layer and get trained using backpropagation in the same way that we train weights of the standard linear layer. The output of such a \"noisy layer\" is calculated in the same way as in a linear layer.\n",
    "\n",
    "\n",
    "2. __Factorized Gaussian noise:__ to minimize the number of random values to be sampled, the authors proposed keeping only two random vectors: one with the size of the input and another with the size of the output of the layer. Then, a random matrix for the layer is created by calculating the outer product of the vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "---\n",
    "\n",
    "In PyTorch, both methods can be easily implemented in a very straightforward way. What we need to do is create our own nn.Linear layer equivalent with the additional random values sampled every time forward() gets called. I've implemented both noisy layers, and their implementations are in Chapter08/ lib/dqn_extra.py in classes NoisyLinear (for independent Gaussian noise) and NoisyFactorizedLinear (for the factorized noise variant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class NoisyLinear(nn.Linear):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, sigma_init=0.017, bias=True):\n",
    "        super(NoisyLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "        w = torch.full((out_features, in_features), sigma_init)\n",
    "        self.sigma_weight = nn.Parameter(w)\n",
    "        z = torch.zeros(out_features, in_features)\n",
    "        self.register_buffer(\"epsilon_weight\", z)\n",
    "        if bias:\n",
    "            w = torch.full((out_features,), sigma_init)\n",
    "            self.sigma_bias = nn.Parameter(w)\n",
    "            z = torch.zeros(out_features)\n",
    "            self.register_buffer(\"epsilon_bias\", z)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        std = math.sqrt(3 / self.in_features)\n",
    "        self.weight.data.uniform_(-std, std)\n",
    "        self.bias.data.uniform_(-std, std)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.epsilon_weight.normal_()\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            self.epsilon_bias.normal_()\n",
    "            bias = bias + self.sigma_bias * self.epsilon_bias.data\n",
    "        v = self.sigma_weight * self.epsilon_weight.data + self.weight\n",
    "        return F.linear(input, v, bias)        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the constructor, we create a matrix for 𝜎. (Values of 𝜇 will be stored in a matrix inherited from nn.Linear.) To make sigmas trainable, we need to wrap the tensor in an nn.Parameter. The register_buffer method creates a tensor in the network that won't be updated during backpropagation, but will be handled by the nn.Module machinery (for example, it will be copied to GPU with the cuda() call). An extra parameter and buffer are created for the bias of the layer. The initial value for sigmas (0.017) was taken from the Noisy Networks article cited in the beginning of this section. At the end, we call the reset_parameters() method, which was overridden from nn.Linear and is supposed to perform the initialization of the layer. In the reset_parameters method, we perform initialization of the nn.Linear weight and bias according to the recommendations in the article. In the forward() method, we sample random noise in both weight and bias buffers, and perform linear transformation of the input data in the same way that nn.Linear does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factorized Gaussian noise works in a similar way and I haven't found much difference in the results, so I'll just include its code here for completeness. If you are curious, you can find the details and equations in the article [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class NoisyFactorizedLinear(nn.Linear):\n",
    "    \n",
    "    def __init__(self, in_features, out_features, sigma_zero=0.4, bias=True):\n",
    "        super(NoisyFactorizedLinear, self).__init__(in_features, out_features, bias=bias)\n",
    "        sigma_init = sigma_zero / math.sqrt(in_features)\n",
    "        w = torch.full((out_features, in_features), sigma_init)\n",
    "        self.sigma_weight = nn.Parameter(w)\n",
    "        z1 = torch.zeros(1, in_features)\n",
    "        self.register_buffer(\"epsilon_input\", z1)\n",
    "        z2 = torch.zeros(out_features, 1)\n",
    "        self.register_buffer(\"epsilon_output\", z2)\n",
    "        if bias:\n",
    "            w = torch.full((out_features,), sigma_init)\n",
    "            self.sigma_bias = nn.Parameter(w)\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.epsilon_input.normal_()\n",
    "        self.epsilon_output.normal_()\n",
    "        func = lambda x: torch.sign(x) * \\\n",
    "        torch.sqrt(torch.abs(x))\n",
    "        eps_in = func(self.epsilon_input.data)\n",
    "        eps_out = func(self.epsilon_output.data)\n",
    "        bias = self.bias\n",
    "        if bias is not None:\n",
    "            bias = bias + self.sigma_bias * eps_out.t()\n",
    "        noise_v = torch.mul(eps_in, eps_out)\n",
    "        v = self.weight + self.sigma_weight * noise_v\n",
    "        return F.linear(input, v, bias)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the implementation point of view, that's it. What we now need to do to turn the classic DQN into a noisy network variant is just replace nn.Linear (which are the two last layers in our DQN network) with the NoisyLinear layer (or NoisyFactorizedLinear if you wish). Of course, you have to remove all the code related to the epsilon-greedy strategy.\n",
    "\n",
    "To check the internal noise level during training, we can monitor the signal-to-noise ratio (SNR) of our noisy layers, which is a ratio of RMS(𝜇) / RMS(𝜎), where RMS is the root mean square of the corresponding weights. In our case, SNR shows how many times the stationary component of the noisy layer is larger than the injected noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine\n",
    "from lib import common, dqn_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run name for ignite\n",
    "NAME = \"04_noisy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N iteration to Noisy signal-to-noise ratio (SNR)\n",
    "NOISY_SNR_EVERY_ITERS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-20, steps=841, speed=0.0 f/s, elapsed=0:04:21\n",
      "Episode 2: reward=-21, steps=755, speed=0.0 f/s, elapsed=0:04:21\n",
      "Episode 3: reward=-21, steps=757, speed=0.0 f/s, elapsed=0:04:21\n",
      "Episode 4: reward=-21, steps=761, speed=0.0 f/s, elapsed=0:04:21\n",
      "Episode 5: reward=-21, steps=759, speed=0.0 f/s, elapsed=0:04:21\n",
      "Episode 6: reward=-21, steps=757, speed=0.0 f/s, elapsed=0:04:21\n",
      "Episode 7: reward=-21, steps=822, speed=0.0 f/s, elapsed=0:04:21\n",
      "Episode 8: reward=-21, steps=759, speed=0.0 f/s, elapsed=0:04:21\n",
      "Episode 9: reward=-21, steps=759, speed=0.0 f/s, elapsed=0:04:21\n",
      "Episode 10: reward=-21, steps=761, speed=0.0 f/s, elapsed=0:04:21\n",
      "Episode 11: reward=-21, steps=757, speed=0.0 f/s, elapsed=0:04:21\n",
      "Episode 12: reward=-21, steps=757, speed=0.0 f/s, elapsed=0:04:21\n",
      "Episode 13: reward=-21, steps=757, speed=3.9 f/s, elapsed=0:04:22\n",
      "Episode 14: reward=-21, steps=816, speed=3.9 f/s, elapsed=0:07:26\n",
      "Episode 15: reward=-20, steps=840, speed=3.9 f/s, elapsed=0:11:01\n",
      "Episode 16: reward=-21, steps=906, speed=3.9 f/s, elapsed=0:15:02\n",
      "Episode 17: reward=-20, steps=867, speed=3.9 f/s, elapsed=0:18:50\n",
      "Episode 18: reward=-21, steps=1012, speed=3.9 f/s, elapsed=0:23:14\n",
      "Episode 19: reward=-20, steps=862, speed=3.9 f/s, elapsed=0:27:05\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Set the random seed\n",
    "    random.seed(common.SEED)\n",
    "    \n",
    "    # Set the random seed for pytorch\n",
    "    torch.manual_seed(common.SEED)\n",
    "    \n",
    "    # Get the environment parameters for \"pong\"\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "\n",
    "    # Set the device type\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Make the environment\n",
    "    env = gym.make(params.env_name)\n",
    "    \n",
    "    # Apply a common set of wrappers for Atari games\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    # Set the random seed for environment\n",
    "    env.seed(common.SEED)\n",
    "\n",
    "    # Source network (noisy DQN) + Convert it into \"device\" type\n",
    "    net = dqn_extra.NoisyDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "    # Target network (noisy DQN)\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    # Instantiate the action selector (argmax)\n",
    "    selector = ptan.actions.ArgmaxActionSelector()\n",
    "    \n",
    "    # Instantiate the agent\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "    # Instantiate the experience source\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=params.gamma)\n",
    "    \n",
    "    # Instantiate the buffer\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size = params.replay_size)\n",
    "    \n",
    "    # Instantiate the Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)\n",
    "\n",
    "    # Function for processing each batch\n",
    "    def process_batch(engine, batch):\n",
    "        \n",
    "        # Reset the optimizer's weight to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calcualte the loss\n",
    "        loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, gamma = params.gamma, device = device)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss_v.backward()\n",
    "        \n",
    "        # Do optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Every target_net_sync time\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            \n",
    "            # Sync the source network to target network\n",
    "            tgt_net.sync()\n",
    "            \n",
    "        # Every NOISY_SNR_EVERY_ITERS time\n",
    "        if engine.state.iteration % NOISY_SNR_EVERY_ITERS == 0:\n",
    "            \n",
    "            # Iterate over sigmas\n",
    "            for layer_idx, sigma_l2 in enumerate(net.noisy_layers_sigma_snr()):\n",
    "                \n",
    "                # Update the meterics in engine\n",
    "                engine.state.metrics[f'snr_{layer_idx+1}'] = sigma_l2\n",
    "                \n",
    "        return {\"loss\": loss_v.item(),}\n",
    "\n",
    "    # Instantiate the engine\n",
    "    engine = Engine(process_batch)\n",
    "    \n",
    "    # Setup the ignite\n",
    "    common.setup_ignite(engine, params, exp_source, NAME, extra_metrics=('snr_1', 'snr_2'))\n",
    "    \n",
    "    # Run engine\n",
    "    engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig8.8.png\">\n",
    "\n",
    "After checking the SNR chart, you may notice that both layers' noise levels have decreased very quickly. The first layer has gone from 1 to almost 1/2.5 ratio of noise. The second layer is even more interesting, as its noise level decreased from 1/3 in the beginning to 1/15, but after 250k frames (roughly the same time as when raw rewards climbed close to the 20 score), the level of noise in the last layer started to increase back, pushing the agent to explore the environment more. This makes a lot of sense, as after reaching high score levels, the agent basically knows how to play at a good level, but still needs to \"polish\" its actions to improve the results even more.\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig8.9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 05. Prioritized replay buffer\n",
    "\n",
    "---\n",
    "\n",
    "The next useful idea on how to improve DQN training was proposed in 2015 in the paper _Prioritized Experience Replay ([7] Schaul and others, 2015)_. This method tries to improve the efficiency of samples in the replay buffer by prioritizing those samples according to the training loss.\n",
    "\n",
    "The basic DQN used the replay buffer to break the correlation between immediate transitions in our episodes. As we discussed in Chapter 6, Deep Q-Networks, the examples we experience during the episode will be highly correlated, as most of the time, the environment is \"smooth\" and doesn't change much according to our actions. However, the stochastic gradient descent (SGD) method assumes that the data we use for training has an i.i.d. property. To solve this problem, the classic DQN method uses a large buffer of transitions, randomly sampled to get the next training batch.\n",
    "\n",
    "The authors of the paper questioned this uniform random sample policy and proved that by assigning priorities to buffer samples, according to training loss and sampling the buffer proportional to those priorities, we can significantly improve convergence and the policy quality of the DQN. This method's basic idea could be explained as \"train more on data that surprises you.\" The tricky point here is to keep the balance of training on an \"unusual\" sample and training on the rest of the buffer. If we focus only on a small subset of the buffer, we can lose our i.i.d. property and simply overfit on this subset.\n",
    "\n",
    "From the mathematical point of view, the priority of every sample in the buffer is calculated as ${P(i) = \\dfrac{p_i^\\alpha}{\\sum_k p_k^\\alpha}}$, where $p_i$ is the priority of the $i$ th sample in the buffer and $\\alpha$ is the number that shows how much emphasis we give to the priority. If ${\\alpha=0}$, our sampling will become uniform as in the classic DQN method. Larger values for $\\alpha$ put more stress on samples with higher priority. So, it's another hyperparameter to tune, and the starting value of $\\alpha$ proposed by the paper is 0.6.\n",
    "\n",
    "There were several options proposed in the paper for how to define the priority, and the most popular is to make it proportional to the loss for this particular example in the Bellman update. New samples added to the buffer need to be assigned a maximum value of priority to be sure that they will be sampled soon.\n",
    "\n",
    "By adjusting the priorities for the samples, we are introducing bias into our data distribution (we sample some transitions much more frequently than others), which we need to compensate for if SGD is to work. To get this result, the authors of the study used sample weights, which needed to be multiplied to the individual sample loss. The value of the weight for each sample is defined as ${w_i = (N . P(i))^{-\\beta}}$, where $\\beta$ is another hyperparameter that should be between 0 and 1.\n",
    "\n",
    "With ${\\beta = 1}$, the bias introduced by the sampling is fully compensated, but the authors showed that it's good for convergence to start with some $\\beta$ between 0 and 1 and slowly increase it to 1 during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "To implement this method, we have to introduce several changes in our code. First of all, we need a new replay buffer that will track priorities, sample a batch according to them, calculate weights, and let us update priorities after the loss has become known. The second change will be the loss function itself. Now we not only need to incorporate weights for every sample, but we need to pass loss values back to the replay buffer to adjust the priorities of the sampled transitions.\n",
    "\n",
    "In the example file Chapter08/05_dqn_prio_replay.py, we have all those changes implemented. For the sake of simplicity, the new priority replay buffer class uses a very similar storage scheme to our previous replay buffer. Unfortunately, new requirements for prioritization make it impossible to implement sampling in O(1) time to buffer size. If we are using simple lists, every time that we sample a new batch, we need to process all the priorities, which makes our sampling have O(N) time complexity in proportion to the buffer size. It's not a big deal if our buffer is small, such as 100k samples, but may become an issue for real-life large buffers of millions of transitions. There are other storage schemes that support efficient sampling in O(log N) time, for example, using the segment tree data structure. You can find such implementation in the OpenAI Baselines project: https://github.com/openai/baselines. The PTAN library also provides an efficient prioritized replay buffer in the class ptan.experience.PrioritizedReplayBuffer. You can update the example to use the more efficient version and check the effect on training performance.\n",
    "\n",
    "But, for now, let's take a look at the naïve version, whose source code is in lib/dqn_ extra.py.\n",
    "\n",
    "```python\n",
    "BETA_START = 0.4\n",
    "BETA_FRAMES = 100000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning, we define parameters for the $\\beta$ increase rate. Our beta will be changed from 0.4 to 1.0 during the first 100k frames. Now the prioritized replay buffer class:\n",
    "\n",
    "```python\n",
    "class PrioReplayBuffer:\n",
    "    \n",
    "    def __init__(self, exp_source, buf_size, prob_alpha=0.6):\n",
    "        self.exp_source_iter = iter(exp_source)\n",
    "        self.prob_alpha = prob_alpha\n",
    "        self.capacity = buf_size\n",
    "        self.pos = 0\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((buf_size, ), dtype=np.float32)\n",
    "        self.beta = BETA_START\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class for the priority replay buffer stores samples in a circular buffer (it allows us to keep a fixed amount of entries without reallocating the list) and a NumPy array to keep priorities. We also store the iterator to the experience source object to pull the samples from the environment.\n",
    "\n",
    "```python\n",
    "        def update_beta(self, idx):\n",
    "            v = BETA_START + idx * (1.0 - BETA_START) / BETA_FRAMES\n",
    "            self.beta = min(1.0, v)\n",
    "            return self.beta\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.buffer)\n",
    "\n",
    "        def populate(self, count):\n",
    "            max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "            for _ in range(count):\n",
    "                sample = next(self.exp_source_iter)\n",
    "                if len(self.buffer) < self.capacity:\n",
    "                    self.buffer.append(sample)\n",
    "                else:\n",
    "                    self.buffer[self.pos] = sample\n",
    "                self.priorities[self.pos] = max_prio\n",
    "                self.pos = (self.pos + 1) % self.capacity\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The populate() method needs to pull the given number of transitions from the ExperienceSource object and store them in the buffer. As our storage for the transitions is implemented as a circular buffer, we have two different situations with this buffer:\n",
    "\n",
    "- When our buffer hasn't reached the maximum capacity, we just need to append a new transition to the buffer.\n",
    "- If the buffer is already full, we need to overwrite the oldest transition, which is tracked by the pos class field, and adjust this position modulo buffer's size.\n",
    "\n",
    "The method update_beta needs to be called periodically to increase beta according to schedule.\n",
    "\n",
    "```python\n",
    "        def sample(self, batch_size):\n",
    "            if len(self.buffer) == self.capacity:\n",
    "                prios = self.priorities\n",
    "            else:\n",
    "                prios = self.priorities[:self.pos]\n",
    "            probs = prios ** self.prob_alpha\n",
    "            probs /= probs.sum()\n",
    "```            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample method, we need to convert priorities to probabilities using our $\\alpha$ hyperparameter.\n",
    "\n",
    "```python\n",
    "            indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "            samples = [self.buffer[idx] for idx in indices]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, using those probabilities, we sample our buffer to obtain a batch of samples.\n",
    "\n",
    "```python\n",
    "            total = len(self.buffer)\n",
    "            weights = (total * probs[indices]) ** (-self.beta)\n",
    "            weights /= weights.max()\n",
    "            return samples, indices, np.array(weights, dtype=np.float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step, we calculate weights for samples in the batch and return three objects: the batch, indices, and weights. Indices for batch samples are required to update priorities for sampled items.\n",
    "\n",
    "```python\n",
    "        def update_priorities(self, batch_indices, batch_priorities):\n",
    "            for idx, prio in zip(batch_indices, batch_priorities):\n",
    "                self.priorities[idx] = prio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function of the priority replay buffer allows us to update new priorities for the processed batch. It's the responsibility of the caller to use this function with the calculated losses for the batch.\n",
    "\n",
    "The next custom function that we have in our example is the loss calculation. As the MSELoss class in PyTorch doesn't support weights (which is understandable, as MSE is loss used in regression problems, but weighting of the samples is commonly utilized in classification losses), we need to calculate the MSE and explicitly multiply the result on the weights:\n",
    "\n",
    "```python\n",
    "def calc_loss(batch, batch_weights, net, tgt_net, gamma, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    batch_weights_v = torch.tensor(batch_weights).to(device)\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    state_action_vals = net(states_v).gather(1, actions_v)\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        next_s_vals = tgt_net(next_states_v).max(1)[0]\n",
    "        next_s_vals[done_mask] = 0.0\n",
    "        exp_sa_vals = next_s_vals.detach() * gamma + rewards_v\n",
    "    l = (state_action_vals - exp_sa_vals) ** 2\n",
    "    losses_v = batch_weights_v * l\n",
    "    return losses_v.mean(), (losses_v + 1e-5).data.cpu().numpy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last part of the loss calculation, we implement the same MSE loss but write our expression explicitly, rather than using the library. This allows us to take into account the weights of samples and keep individual loss values for every sample. Those values will be passed to the priority replay buffer to update priorities. A small value is added to every loss to handle the situation of zero loss value, which will lead to zero priority for an entry in the replay buffer.\n",
    "\n",
    "In the main section of the utility, we have only two updates: the creation of the replay buffer and our processing function. Buffer creation is straightforward, so we will take a look at only a new processing function:\n",
    "\n",
    "```python\n",
    "def process_batch(engine, batch_data):\n",
    "  batch, batch_indices, batch_weights = batch_data\n",
    "  optimizer.zero_grad()\n",
    "  loss_v, sample_prios = calc_loss(batch, batch_weights, net, tgt_net.target_model, gamma=params.gamma, device=device)\n",
    "  loss_v.backward()\n",
    "  optimizer.step()\n",
    "  buffer.update_priorities(batch_indices, sample_prios)\n",
    "  epsilon_tracker.frame(engine.state.iteration)\n",
    "  if engine.state.iteration % params.target_net_sync == 0:\n",
    "      tgt_net.sync()\n",
    "  return {\"loss\": loss_v.item(), \"epsilon\": selector.epsilon, \"beta\": buffer.update_beta(engine.state.iteration),}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several changes here:\n",
    "- Our batch now contains three entities: the batch of data, indices of sampled items, and samples' weights.\n",
    "- We call our new loss function, which accepts weights and returns the additional items' priorities. They are passed to the buffer.update_ priorities function to reprioritize items that we have sampled.\n",
    "- We call the update_beta method of the buffer to change the beta parameter according to schedule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine\n",
    "\n",
    "from lib import dqn_model, common, dqn_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run name of ignite\n",
    "NAME = \"05_prio_replay\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha (the number that shows how much emphasis we give to the priority)\n",
    "PRIO_REPLAY_ALPHA = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating loss\n",
    "def calc_loss(batch, batch_weights, net, tgt_net, gamma, device = \"cpu\"):\n",
    "    \n",
    "    # Unpacking the batch\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "\n",
    "    # Converting S, A, R, done_mask, batch_wights to tensor + Convert them to device\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    batch_weights_v = torch.tensor(batch_weights).to(device)\n",
    "\n",
    "    # Unsqueeze the actions (add extra dimension to last dimension)\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    \n",
    "    # Feedforward in source network + Gathers values along an axis specified by \"dim\" \n",
    "    state_action_vals = net(states_v).gather(1, actions_v)\n",
    "    \n",
    "    # If the last dimension is 1 then remove it\n",
    "    state_action_vals = state_action_vals.squeeze(-1)\n",
    "    \n",
    "    # Avoid gradient calculation\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Convert S' to tensor + Convert them to device\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        \n",
    "        # Feedforward in the target network + Get the maximum value + Choose the first value\n",
    "        next_s_vals = tgt_net(next_states_v).max(1)[0]\n",
    "        \n",
    "        # Mask the S'\n",
    "        next_s_vals[done_mask] = 0.0\n",
    "        \n",
    "        # Calculate the Bellman equation\n",
    "        exp_sa_vals = next_s_vals.detach() * gamma + rewards_v\n",
    "        \n",
    "    # Calculate the losses\n",
    "    l = (state_action_vals - exp_sa_vals) ** 2\n",
    "    losses_v = batch_weights_v * l\n",
    "    \n",
    "    return losses_v.mean(), (losses_v + 1e-5).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-19, steps=1038, speed=0.0 f/s, elapsed=0:00:39\n",
      "Episode 2: reward=-21, steps=1012, speed=0.0 f/s, elapsed=0:00:39\n",
      "Episode 3: reward=-21, steps=876, speed=0.0 f/s, elapsed=0:00:39\n",
      "Episode 4: reward=-18, steps=1190, speed=0.0 f/s, elapsed=0:00:39\n",
      "Episode 5: reward=-21, steps=939, speed=0.0 f/s, elapsed=0:00:39\n",
      "Episode 6: reward=-21, steps=897, speed=0.0 f/s, elapsed=0:00:39\n",
      "Episode 7: reward=-20, steps=947, speed=0.0 f/s, elapsed=0:00:39\n",
      "Episode 8: reward=-19, steps=1157, speed=0.0 f/s, elapsed=0:00:39\n",
      "Episode 9: reward=-21, steps=905, speed=0.0 f/s, elapsed=0:00:39\n",
      "Episode 10: reward=-20, steps=840, speed=0.0 f/s, elapsed=0:00:39\n",
      "Episode 11: reward=-21, steps=780, speed=7.9 f/s, elapsed=0:01:53\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Set the random seed\n",
    "    random.seed(common.SEED)\n",
    "    \n",
    "    # Set the random seed for pytorch\n",
    "    torch.manual_seed(common.SEED)\n",
    "    \n",
    "    # Get the hyperparameters for 'pong' environment\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "    \n",
    "    # Set the device type\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Make the environment\n",
    "    env = gym.make(params.env_name)\n",
    "    \n",
    "    # Apply a common set of wrappers for Atari games\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    # Set the random seed for environment\n",
    "    env.seed(common.SEED)\n",
    "\n",
    "    # Instantiate the source network + Convert it to device\n",
    "    net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "    # Instantiate the target network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    # Instantaite the action selector (epsilon-greedy)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon = params.epsilon_start)\n",
    "    \n",
    "    # Instantiate the epsilon tracker\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    \n",
    "    # Instantiate the DQN agent\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "    # Instantiate the experience source\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma = params.gamma)\n",
    "    \n",
    "    # Instantiate the prioritized replay buffer\n",
    "    buffer = dqn_extra.PrioReplayBuffer(exp_source, params.replay_size, PRIO_REPLAY_ALPHA)\n",
    "    \n",
    "    # Instantiate hte Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr = params.learning_rate)\n",
    "\n",
    "    # Function for processing batches\n",
    "    def process_batch(engine, batch_data):\n",
    "        \n",
    "        # Get the batch data, indices, and weights\n",
    "        batch, batch_indices, batch_weights = batch_data\n",
    "        \n",
    "        # Reset the optimizer's weight to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss_v, sample_prios = calc_loss(batch, batch_weights, net, tgt_net.target_model, gamma = params.gamma, device = device)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss_v.backward()\n",
    "        \n",
    "        # Do optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the priorities in buffer\n",
    "        buffer.update_priorities(batch_indices, sample_prios)\n",
    "        \n",
    "        # Track the epsilon\n",
    "        epsilon_tracker.frame(engine.state.iteration)\n",
    "        \n",
    "        # Every target_net_sync times\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            \n",
    "            # Sync the weight's of source network with target network\n",
    "            tgt_net.sync()\n",
    "            \n",
    "        return {\"loss\": loss_v.item(), \"epsilon\": selector.epsilon, \"beta\": buffer.update_beta(engine.state.iteration),}\n",
    "\n",
    "    # Instantiate the engine\n",
    "    engine = Engine(process_batch)\n",
    "    \n",
    "    # Setup ignite\n",
    "    common.setup_ignite(engine, params, exp_source, NAME)\n",
    "    \n",
    "    # Run the engine\n",
    "    engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "This example can be trained as usual. According to my experiments, the prioritized replay buffer took almost the same absolute time to solve the environment: almost two hours. But it took fewer training iterations and fewer episodes. So, wall clock time is the same mostly due to the less efficient replay buffer, which, of course, could be resolved by proper O(log N) implementation of the buffer.\n",
    "\n",
    "Here are the charts of reward dynamics of the baseline (left) and prioritized replay buffer (right). The top axes show the number of game episodes.\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig8.10.png\">\n",
    "\n",
    "Another difference to note on the TensorBoard charts is a much lower loss for the prioritized replay buffer. The following chart shows the comparison.\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig8.11.png\">\n",
    "\n",
    "Lower loss value is also expected and is a good sign that our implementation works. The idea of prioritization is to train more on samples with high loss value, so training becomes more efficient. But there is a danger here: loss value during the training is not the primary objective to optimize; we can have very low loss, but due to a lack\n",
    "of exploration, the final policy learned could be far from being optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 06. Dueling DQN\n",
    "\n",
    "---\n",
    "\n",
    "This improvement to DQN was proposed in 2015, in the paper called _Dueling Network Architectures for Deep Reinforcement Learning ([8] Wang et al., 2015)_. The core observation of this paper is that the Q-values, $Q(s, a)$, that our network is trying to approximate can be divided into quantities: the value of the state, $V(s)$, and the advantage of actions in this state, $A(s, a)$.\n",
    "\n",
    "You have seen the quantity $V(s)$ before, as it was the core of the value iteration method from Chapter 5, Tabular Learning and the Bellman Equation. It is just equal to the discounted expected reward achievable from this state. The advantage $A(s, a)$ is supposed to bridge the gap from $A(s)$ to $Q(s, a)$, as, by definition, $Q(s, a) = V(s) + A(s, a)$. In other words, the advantage $A(s, a)$ is just the delta, saying how much extra reward some particular action from the state brings us. The advantage could be positive or negative and, in general, can have any magnitude. For example, at some tipping point, the choice of one action over another can cost us a lot of the total reward.\n",
    "\n",
    "The Dueling paper's contribution was an explicit separation of the value and the advantage in the network's architecture, which brought better training stability, faster convergence, and better results on the Atari benchmark. The architecture difference from the classic DQN network is shown in the following illustration. The classic DQN network (top) takes features from the convolution layer and, using fully connected layers, transforms them into a vector of Q-values, one for each action. On the other hand, dueling DQN (bottom) takes convolution features and processes them using two independent paths: one path is responsible for $V(s)$ prediction, which is just a single number, and another path predicts individual advantage values, having the same dimension as Q-values in the classic case. After that, we add $V(s)$ to every value of $A(s, a)$ to obtain $Q(s, a)$, which is used and trained as normal.\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig8.12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These changes in the architecture are not enough to make sure that the network will learn $V(s)$ and $A(s, a)$ as we want it to. Nothing prevents the network, for example, from predicting some state, $V(s) = 0$, and $A(s) = [1, 2, 3, 4]$, which is completely wrong, as the predicted $V(s)$ is not the expected value of the state. We have yet another constraint to set: we want the mean value of the advantage of any state to be zero. In that case, the correct prediction for the preceding example will be $V(s) = 2.5$ and $A(s) = [–1.5, –0.5, 0.5, 1.5]$.\n",
    "\n",
    "This constraint could be enforced in various ways, for example, via the loss function; but in the Dueling paper, the authors proposed a very elegant solution of subtracting the mean value of the advantage from the Q expression in the network, which effectively pulls the mean for the advantage to zero:\n",
    "\n",
    "${Q(s, a) = V(s) + A(s, a) - \\frac{1}{N} \\sum_k A(s, k)}$\n",
    "\n",
    "This keeps the changes that need to be made in the classic DQN very simple: to convert it to the double DQN, you need to change only the network architecture, without affecting other pieces of the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "---\n",
    "\n",
    "A complete example is available in Chapter08/06_dqn_dueling.py. All the changes sit in the network architecture, so here I'll show only the network class (which is in the lib/dqn_extra.py module).\n",
    "\n",
    "```python\n",
    "class DuelingDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution layers are completely the same as before.\n",
    "\n",
    "```python\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "    \n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "        \n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of defining a single path of fully connected layers, we create two different transformations: one for advantages and one for value prediction. Also, to keep the number of parameters in the model comparable to the original network, the inner dimension in both paths is decreased from 512 to 256.\n",
    "\n",
    "```python\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        adv, val = self.adv_val(x)\n",
    "        return val + (adv - adv.mean(dim=1, keepdim=True))\n",
    "    \n",
    "    def adv_val(self, x):\n",
    "        fx = x.float() / 256\n",
    "        conv_out = self.conv(fx).view(fx.size()[0], -1)\n",
    "        return self.fc_adv(conv_out), self.fc_val(conv_out)\n",
    "```\n",
    "\n",
    "The changes in the forward() function are also very simple, thanks to PyTorch's expressiveness: we calculate the value and advantage for our batch of samples and add them together, subtracting the mean of the advantage to obtain the final Q-values. A subtle, but important, difference lies in calculating the mean along the second dimension of the tensor, which produces a vector of the mean advantage for every sample in our batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine\n",
    "\n",
    "from lib import common, dqn_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run name of ignite\n",
    "NAME = \"06_dueling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of states to evaluate\n",
    "STATES_TO_EVALUATE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every 100 frame do evaluation\n",
    "EVAL_EVERY_FRAME = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid gradient calculation\n",
    "@torch.no_grad()\n",
    "\n",
    "# Function for evaluating states\n",
    "def evaluate_states(states, net, device, engine):\n",
    "    \n",
    "    # Convert states into tensor + Convert it to device\n",
    "    s_v = torch.tensor(states).to(device)\n",
    "    \n",
    "    # Get the advantage and Q-values\n",
    "    adv, val = net.adv_val(s_v)\n",
    "    \n",
    "    # Update the advantage metric in engine\n",
    "    engine.state.metrics['adv'] = adv.mean().item()\n",
    "    \n",
    "    # Update the Q-value metric in engine\n",
    "    engine.state.metrics['val'] = val.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-21, steps=882, speed=0.0 f/s, elapsed=0:00:46\n",
      "Episode 2: reward=-19, steps=1220, speed=0.0 f/s, elapsed=0:00:46\n",
      "Episode 3: reward=-20, steps=912, speed=0.0 f/s, elapsed=0:00:46\n",
      "Episode 4: reward=-21, steps=846, speed=0.0 f/s, elapsed=0:00:46\n",
      "Episode 5: reward=-20, steps=918, speed=0.0 f/s, elapsed=0:00:46\n",
      "Episode 6: reward=-20, steps=946, speed=0.0 f/s, elapsed=0:00:46\n",
      "Episode 7: reward=-21, steps=880, speed=0.0 f/s, elapsed=0:00:46\n",
      "Episode 8: reward=-21, steps=790, speed=0.0 f/s, elapsed=0:00:46\n",
      "Episode 9: reward=-21, steps=942, speed=0.0 f/s, elapsed=0:00:46\n",
      "Episode 10: reward=-21, steps=817, speed=0.0 f/s, elapsed=0:00:46\n",
      "Episode 11: reward=-21, steps=822, speed=0.0 f/s, elapsed=0:00:46\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Set the random seed\n",
    "    random.seed(common.SEED)\n",
    "    \n",
    "    # Set the random seed for pytorch\n",
    "    torch.manual_seed(common.SEED)\n",
    "    \n",
    "    # Get the hyperparameter for \"pong\" environment\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "    \n",
    "    # Set the device type\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Make the environment\n",
    "    env = gym.make(params.env_name)\n",
    "    \n",
    "    # Apply a common set of wrappers for Atari games\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    # Set the random seed for environment\n",
    "    env.seed(common.SEED)\n",
    "\n",
    "    # Instantaite the dueling DQN\n",
    "    net = dqn_extra.DuelingDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "    # Instantiate the target network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    # Instantiate the action selector (epsilon-greedy)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "    \n",
    "    # Instantiate the epsilon tracker\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    \n",
    "    # Instantiate the DQN agent\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "    # Instantiate the experience source (first-last)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma = params.gamma)\n",
    "    \n",
    "    # Instantiate the buffer\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size = params.replay_size)\n",
    "    \n",
    "    # Instantiate the adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)\n",
    "\n",
    "    # Function for processing batches\n",
    "    def process_batch(engine, batch):\n",
    "        \n",
    "        # Reset the weight's of optimizer to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss_v = common.calc_loss_dqn(batch, net, tgt_net.target_model, gamma = params.gamma, device = device)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss_v.backward()\n",
    "        \n",
    "        # Do optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track the epsilon\n",
    "        epsilon_tracker.frame(engine.state.iteration)\n",
    "        \n",
    "        # Every target_net_sync times\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            \n",
    "            # Sync the source network to target network\n",
    "            tgt_net.sync()\n",
    "            \n",
    "        # Every EVAL_EVERY_FRAME times\n",
    "        if engine.state.iteration % EVAL_EVERY_FRAME == 0:\n",
    "            \n",
    "            # Create a named attribute from an object\n",
    "            eval_states = getattr(engine.state, \"eval_states\", None)\n",
    "            \n",
    "            # If eval_states is None\n",
    "            if eval_states is None:\n",
    "                \n",
    "                # Sample states from buffer\n",
    "                eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
    "                \n",
    "                # Convert each states into the array\n",
    "                eval_states = [np.array(transition.state, copy=False) for transition in eval_states]\n",
    "                \n",
    "                # Convert the eval_states into array\n",
    "                eval_states = np.array(eval_states, copy=False)\n",
    "                \n",
    "                # Update the eval_states in engine\n",
    "                engine.state.eval_states = eval_states\n",
    "                \n",
    "            # Evaluate states\n",
    "            evaluate_states(eval_states, net, device, engine)\n",
    "            \n",
    "        return {\"loss\": loss_v.item(), \"epsilon\": selector.epsilon,}\n",
    "\n",
    "    # Instantiate the engine\n",
    "    engine = Engine(process_batch)\n",
    "    \n",
    "    # Setup the ignite\n",
    "    common.setup_ignite(engine, params, exp_source, NAME, extra_metrics=('adv', 'val'))\n",
    "    \n",
    "    # Run the engine\n",
    "    engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "After training a dueling DQN, we can compare it to the classic DQN convergence on our Pong benchmark, as shown here.\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig8.13.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example also outputs the advantage and value for a fixed set of states, shown in the following charts. They meet our expectations: the advantage is not very different from zero, but the value improves over time (and resembles the value from the Double DQN section).\n",
    "\n",
    "<img width=\"750\" src=\"assets/fig8.14.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 07. Categorical DQN\n",
    "\n",
    "---\n",
    "\n",
    "The last, and the most complicated, method in our DQN improvements toolbox is from a very recent paper, published by DeepMind in June 2017, called _A Distributional Perspective on Reinforcement Learning ([9] Bellemare, Dabney, and Munos, 2017)_.\n",
    "\n",
    "In the paper, the authors questioned the fundamental piece of Q-learning — Q-values — and tried to replace them with a more generic Q-value probability distribution. Let's try to understand the idea. Both the Q-learning and value iteration methods work with the values of the actions or states represented as simple numbers and showing how much total reward we can achieve from a state, or an action and a state. However, is it practical to squeeze all future possible rewards into one number? In complicated environments, the future could be stochastic, giving us different values with different probabilities.\n",
    "\n",
    "For example, imagine the commuter scenario when you regularly drive from home to work. Most of the time, the traffic isn't that heavy, and it takes you around 30 minutes to reach your destination. It's not exactly 30 minutes, but on average it's 30. From time to time, something happens, like road repairs or an accident, and due to traffic jams, it takes you three times longer to get to work. The probability of your commute time can be represented as a distribution of the \"commute time\" random variable and it is shown in the following chart.\n",
    "\n",
    "<img src=\"assets/fig8.15.png\">\n",
    "\n",
    "Now imagine that you have an alternative way to get to work: the train. It takes a bit longer, as you need to get from home to the train station and from the station to the office, but they are much more reliable. Say, for example, that the train commute time is 40 minutes on average, with a small chance of train disruption, which adds 20 minutes of extra time to the journey. The distribution of the train commute is shown in the following graph.\n",
    "\n",
    "<img src=\"assets/fig8.16.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that now we want to make the decision on how to commute. If we know only the mean time for both car and train, a car looks more attractive, as on average it takes 35.43 minutes to travel, which is better than 40.54 minutes for the train.\n",
    "\n",
    "However, if we look at full distributions, we may decide to go by train, as even in the worst-case scenario, it will be one hour of commuting versus one hour and 30 minutes. Switching to statistical language, the car distribution has much higher variance, so in situations when you really have to be at the office in 60 minutes max, the train is better.\n",
    "\n",
    "The situation becomes even more complicated in the Markov decision process (MDP) scenario, when the sequence of decisions needs to be made and every decision might influence the future situation. In the commute example, it might be the time of an important meeting that you need to arrange given the way that you are going to commute. In that case, working with mean reward values might mean losing lots of information about the underlying dynamics.\n",
    "\n",
    "Exactly the same idea was proposed by the authors of _Distributional Perspective on Reinforcement Learning [9]_. Why do we limit ourselves by trying to predict an average value for an action, when the underlying value may have a complicated underlying distribution? Maybe it will help us to work with distributions directly.\n",
    " \n",
    "The results presented in the paper show that, in fact, this idea could be helpful, but at the cost of introducing a more complicated method. I'm not going to put a strict mathematical definition here, but the overall idea is to predict the distribution of value for every action, similar to the distributions for our car/train example. As the next step, the authors showed that the Bellman equation can be generalized for a distribution case, and it will have the form ${Z(x, a) \\stackrel{D}{=} R(x, a) + \\gamma Z(x', a')}$, which is very similar to the familiar Bellman equation, but now $Z(x, a)$ and $R(x, a)$ are the probability distributions and not numbers.\n",
    "\n",
    "The resulting distribution can be used to train our network to give better predictions of value distribution for every action of the given state, exactly in the same way as with Q-learning. The only difference will be in the loss function, which now has to be replaced with something suitable for distribution comparison. There are several alternatives available, for example, Kullback-Leibler (KL) divergence (or cross- entropy loss), which is used in classification problems, or the Wasserstein metric. In the paper, the authors gave theoretical justification for the Wasserstein metric, but when they tried to apply it in practice, they faced limitations. So, in the end, the paper used KL divergence. The paper is very recent, so it's quite probable that improvements to the methods will follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "As mentioned, the method is quite complex, so it took me a while to implement it and make sure it was working. The complete code is in Chapter08/07_dqn_ distrib.py, which uses a function in lib/dqn_extra.py that we haven't discussed before to perform distribution projection. Before we start it, I need to say several words about the implementation logic.\n",
    "\n",
    "The central part of the method is the probability distribution, which we are approximating. There are lots of ways to represent the distribution, but the authors of the paper chose a quite generic parametric distribution, which is basically a fixed number of values placed regularly on a values range. The range of values should cover the range of possible accumulated discounted reward. In the paper, the authors did experiments with various numbers of atoms, but the best results were obtained with the range split on N_ATOMS=51 intervals in the range of values from Vmin=-10 to Vmax=10.\n",
    "\n",
    "For every atom (we have 51 of them), our network predicts the probability that the future discounted value will fall into this atom's range. The central part of the method is the code, which performs the contraction of distribution of the next state's best action using gamma, adds local reward to the distribution, and projects the results back into our original atoms. The following is the function that does exactly this:\n",
    "\n",
    "```python\n",
    "def distr_projection(next_distr, rewards, dones, gamma):\n",
    "    batch_size = len(rewards)\n",
    "    proj_distr = np.zeros((batch_size, N_ATOMS), dtype=np.float32)\n",
    "    delta_z = (Vmax - Vmin) / (N_ATOMS - 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the beginning, we allocate the array that will keep the result of the projection. This function expects the batch of distributions with a shape (batch_size, N_ATOMS), the array of rewards, flags for completed episodes, and our hyperparameters: Vmin, Vmax, N_ATOMS, and gamma. The delta_z variable is the width of every atom in our value range.\n",
    "\n",
    "```python\n",
    "    for atom in range(N_ATOMS):\n",
    "        v = rewards + (Vmin + atom * delta_z) * gamma\n",
    "        tz_j = np.minimum(Vmax, np.maximum(Vmin, v))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding code, we iterate over every atom in the original distribution that we have and calculate the place that this atom will be projected to by the Bellman operator, taking into account our value bounds.\n",
    "\n",
    "For example, the very first atom, with index 0, corresponds with the value Vmin=-10, but for the sample with reward +1 will be projected into the value –10 * 0.99 + 1 = –8.9. In other words, it will be shifted to the right (assume gamma=0.99). If the value falls beyond our value range given by Vmin and Vmax, we clip it to the bounds.\n",
    "\n",
    "```python\n",
    "        b_j = (tz_j - Vmin) / delta_z\n",
    "```\n",
    "\n",
    "In the next line, we calculate the atom numbers that our samples have projected. Of course, samples can be projected between atoms. In such situations, we spread the value in the original distribution at the source atom between the two atoms that it falls between. This spreading should be carefully handled, as our target atom can land exactly at some atom's position. In that case, we just need to add the source distribution value to the target atom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "        l = np.floor(b_j).astype(np.int64)\n",
    "        u = np.ceil(b_j).astype(np.int64)\n",
    "        eq_mask = u == l\n",
    "        proj_distr[eq_mask, l[eq_mask]] += next_distr[eq_mask, atom]\n",
    "```\n",
    "\n",
    "The preceding code handles the situation when the projected atom lands exactly on the target atom. Otherwise, b_j won't be the integer value and variables l and u (which correspond to the indices of atoms below and above the projected point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "        ne_mask = u != l\n",
    "        proj_distr[ne_mask, l[ne_mask]] += next_distr[ne_mask, atom] * (u - b_j)[ne_mask]\n",
    "        proj_distr[ne_mask, u[ne_mask]] += next_distr[ne_mask, atom] * (b_j - l)[ne_mask]\n",
    "```\n",
    "\n",
    "When the projected point lands between atoms, we need to spread the probability of the source atom between the atoms below and above. This is carried out by two lines in the preceding code, and, of course, we need to properly handle the final transitions of episodes. In that case, our projection shouldn't take into account the next distribution and should just have a 1 probability corresponding to the reward obtained. However, we again need to take into account our atoms and properly distribute this probability if the reward value falls between atoms. This case is handled by the following code branch, which zeroes the resulting distribution for samples with the done flag set and then calculates the resulting projection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    if dones.any():\n",
    "        proj_distr[dones] = 0.0\n",
    "        tz_j = np.minimum(Vmax, np.maximum(Vmin, rewards[dones]))\n",
    "        b_j = (tz_j - Vmin) / delta_z\n",
    "        l = np.floor(b_j).astype(np.int64)\n",
    "        u = np.ceil(b_j).astype(np.int64)\n",
    "        eq_mask = u == l\n",
    "        eq_dones = dones.copy()\n",
    "        eq_dones[dones] = eq_mask\n",
    "        if eq_dones.any():\n",
    "            proj_distr[eq_dones, l[eq_mask]] = 1.0\n",
    "        ne_mask = u != l\n",
    "        ne_dones = dones.copy()\n",
    "        ne_dones[dones] = ne_mask\n",
    "        if ne_dones.any():\n",
    "            proj_distr[ne_dones, l[ne_mask]] = (u - b_j)[ne_mask]\n",
    "            proj_distr[ne_dones, u[ne_mask]] = (b_j - l)[ne_mask]\n",
    "    return proj_distr\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/fig8.17.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top chart of Figure 8.17 (named \"Source\") is a normal distribution with zero mean and scale=3. The chart following that (named \"Projected\") is obtained from distribution projection with gamma=0.9 and is shifted to the right with reward=2. In the situation when we pass done=True with the same data, the result will be different and is shown on Figure 8.18. In such cases, the source distribution will be ignored completely, and the result will have only the reward projected.\n",
    "\n",
    "<img src=\"assets/fig18.18.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two versions of this method: in Chapter08/07_dqn_distrib.py there is only the essential code; I put the same code in source Chapter08/07_dqn_distrib_ plots.py, but it additionally saved images of underlying probability distributions for a fixed set of states. Those images were essential during the development of the code, and I believe they could be useful if you want to understand and visualize the convergence dynamics. Sample images are shown in Figure 8.21 and Figure 8.22.\n",
    "\n",
    "I'm going to show only essential pieces of the implementation here. The core of the method, the function distr_projection, was already covered, and it is the most complicated piece. What is still missing is the network architecture and modified loss function.\n",
    "\n",
    "Let's start with the network, which is in lib/dqn_extra.py, in the class DistributionalDQN.\n",
    "\n",
    "```python\n",
    "Vmax = 10\n",
    "Vmin = -10\n",
    "N_ATOMS = 51\n",
    "\n",
    "DELTA_Z = (Vmax - Vmin) / (N_ATOMS - 1)\n",
    "\n",
    "class DistributionalDQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "\n",
    "        super(DistributionalDQN, self).__init__()    \n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions * N_ATOMS)\n",
    "        )\n",
    "        \n",
    "        sups = torch.arange(Vmin, Vmax + DELTA_Z, DELTA_Z)\n",
    "        self.register_buffer(\"supports\", sups)\n",
    "        self.softmax = nn.Softmax(dim=1)    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference is the output of the fully connected layer. Now it outputs the vector of n_actions * N_ATOMS values, which is 6 × 51 = 306 for Pong. For every action, it needs to predict the probability distribution on 51 atoms. Every atom (called \"support\") has a value, which corresponds to a particular reward. Those atoms' rewards are evenly distributed from –10 to 10, which gives a grid with step 0.4. Those supports are stored in the network's buffer.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    batch_size = x.size()[0]\n",
    "    fx = x.float() / 256\n",
    "    conv_out = self.conv(fx).view(batch_size, -1)\n",
    "    fc_out = self.fc(conv_out)\n",
    "    return fc_out.view(batch_size, -1, N_ATOMS)\n",
    "\n",
    "def both(self, x):\n",
    "    cat_out = self(x)\n",
    "    probs = self.apply_softmax(cat_out)\n",
    "    weights = probs * self.supports\n",
    "    res = weights.sum(dim=2)\n",
    "    return cat_out, res\n",
    "\n",
    "def qvals(self, x):\n",
    "    return self.both(x)[1]\n",
    "\n",
    "def apply_softmax(self, t):\n",
    "    return self.softmax(t.view(-1, N_ATOMS)).view(t.size())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method forward() returns the predicted probability distribution as a 3D tensor (batch, actions, and supports). The network also defines several helper functions to simplify the calculation of Q-values and apply softmax on the probability distribution.\n",
    "\n",
    "The final change is the new loss function that has to apply distribution projection instead of the Bellman equation, and calculate KL divergence between predicted and projected distributions.\n",
    "\n",
    "```python\n",
    "def calc_loss(batch, net, tgt_net, gamma, device=\"cpu\"):\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "    batch_size = len(batch)\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    next_distr_v, next_qvals_v = tgt_net.both(next_states_v)\n",
    "    next_acts = next_qvals_v.max(1)[1].data.cpu().numpy()\n",
    "    next_distr = tgt_net.apply_softmax(next_distr_v)\n",
    "    next_distr = next_distr.data.cpu().numpy()\n",
    "    next_best_distr = next_distr[range(batch_size), next_acts]\n",
    "    dones = dones.astype(np.bool)\n",
    "    proj_distr = dqn_extra.distr_projection(\n",
    "    next_best_distr, rewards, dones, gamma)\n",
    "    distr_v = net(states_v)\n",
    "    sa_vals = distr_v[range(batch_size), actions_v.data]\n",
    "    state_log_sm_v = F.log_softmax(sa_vals, dim=1)\n",
    "    proj_distr_v = torch.tensor(proj_distr).to(device)\n",
    "    loss_v = -state_log_sm_v * proj_distr_v\n",
    "    return loss_v.sum(dim=1).mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding code is not very complicated; it is just preparing to call distr_projection and KL divergence, which is defined as ${D_{KL}(P || Q) = - \\sum_i p_i log q_i}$ \n",
    "\n",
    "To calculate the logarithm of probability, we use the PyTorch function log_softmax, which performs both log and softmax in a numerical and stable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from ignite.engine import Engine\n",
    "from lib import common, dqn_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run name for ignite\n",
    "NAME = \"07_distrib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating loss\n",
    "def calc_loss(batch, net, tgt_net, gamma, device = \"cpu\"):\n",
    "    \n",
    "    # Unpack the batch\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "    \n",
    "    # Get the batch size\n",
    "    batch_size = len(batch)\n",
    "\n",
    "    # Convert S, A, S' to tensor + Convert them to device\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "\n",
    "    # Get the next state distribution + next Q-values\n",
    "    next_distr_v, next_qvals_v = tgt_net.both(next_states_v)\n",
    "    \n",
    "    # Get the next actions by:\n",
    "    # Maximum Q-Value + Convert it into data + Convert it to CPU device + Convert it into array\n",
    "    next_acts = next_qvals_v.max(1)[1].data.cpu().numpy()\n",
    "    \n",
    "    # Get the next distrubution by:\n",
    "    # Apply softmax + Convert it into data + Convert it into CPU device + Convert it into array\n",
    "    next_distr = tgt_net.apply_softmax(next_distr_v).data.cpu().numpy()\n",
    "\n",
    "    # Get the best next distribution \n",
    "    next_best_distr = next_distr[range(batch_size), next_acts]\n",
    "    \n",
    "    # Get dones\n",
    "    dones = dones.astype(np.bool)\n",
    "\n",
    "    # Get the distribution projection\n",
    "    proj_distr = dqn_extra.distr_projection(next_best_distr, rewards, dones, gamma)\n",
    "\n",
    "    # Feedforward\n",
    "    distr_v = net(states_v)\n",
    "    \n",
    "    # \n",
    "    sa_vals = distr_v[range(batch_size), actions_v.data]\n",
    "    \n",
    "    # Apply log softmax\n",
    "    state_log_sm_v = F.log_softmax(sa_vals, dim=1)\n",
    "    \n",
    "    # Convert proj_distr to tensor + Convert it into device\n",
    "    proj_distr_v = torch.tensor(proj_distr).to(device)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss_v = -state_log_sm_v * proj_distr_v\n",
    "    \n",
    "    # Return a loss value\n",
    "    return loss_v.sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-21, steps=965, speed=0.0 f/s, elapsed=0:00:51\n",
      "Episode 2: reward=-21, steps=759, speed=0.0 f/s, elapsed=0:00:51\n",
      "Episode 3: reward=-21, steps=1027, speed=0.0 f/s, elapsed=0:00:51\n",
      "Episode 4: reward=-20, steps=951, speed=0.0 f/s, elapsed=0:00:51\n",
      "Episode 5: reward=-19, steps=949, speed=0.0 f/s, elapsed=0:00:51\n",
      "Episode 6: reward=-20, steps=838, speed=0.0 f/s, elapsed=0:00:51\n",
      "Episode 7: reward=-21, steps=845, speed=0.0 f/s, elapsed=0:00:51\n",
      "Episode 8: reward=-20, steps=924, speed=0.0 f/s, elapsed=0:00:51\n",
      "Episode 9: reward=-16, steps=1352, speed=0.0 f/s, elapsed=0:00:51\n",
      "Episode 10: reward=-19, steps=1038, speed=0.0 f/s, elapsed=0:00:51\n",
      "Episode 11: reward=-21, steps=777, speed=5.6 f/s, elapsed=0:02:08\n",
      "Episode 12: reward=-21, steps=755, speed=5.6 f/s, elapsed=0:04:22\n",
      "Episode 13: reward=-21, steps=867, speed=5.6 f/s, elapsed=0:06:55\n",
      "Episode 14: reward=-20, steps=951, speed=5.6 f/s, elapsed=0:09:37\n",
      "Episode 15: reward=-21, steps=787, speed=5.6 f/s, elapsed=0:12:00\n",
      "Episode 16: reward=-21, steps=816, speed=5.6 f/s, elapsed=0:14:17\n",
      "Episode 17: reward=-21, steps=874, speed=5.6 f/s, elapsed=0:17:06\n",
      "Episode 18: reward=-21, steps=759, speed=5.6 f/s, elapsed=0:19:19\n",
      "Episode 19: reward=-21, steps=759, speed=5.6 f/s, elapsed=0:21:20\n",
      "Episode 20: reward=-21, steps=865, speed=5.6 f/s, elapsed=0:23:36\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Set the random seed\n",
    "    random.seed(common.SEED)\n",
    "    \n",
    "    # Set the random seed for pytorch\n",
    "    torch.manual_seed(common.SEED)\n",
    "    \n",
    "    # Get the hyperparameters for \"pong\" hyperparameters\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "    \n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Make the environment\n",
    "    env = gym.make(params.env_name)\n",
    "    \n",
    "    # Apply a common set of wrappers for Atari games\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    # Set the random seed for envuronment\n",
    "    env.seed(common.SEED)\n",
    "\n",
    "    # Instantiate the source network (Distributional DQN) + Convert it into the device\n",
    "    net = dqn_extra.DistributionalDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    \n",
    "    # Instantiate the target network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    # Instantiate the action selector (epsilon-greedy)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "    \n",
    "    # Instantiate the epsilon tracker\n",
    "    epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "    \n",
    "    # Instantiate the DQN agent\n",
    "    agent = ptan.agent.DQNAgent(lambda x: net.qvals(x), selector, device=device)\n",
    "\n",
    "    # Instantiate the experience sourec\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma = params.gamma)\n",
    "    \n",
    "    # Instantiate the experience replay buffer\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size = params.replay_size)\n",
    "    \n",
    "    # Instantiate the Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=params.learning_rate)\n",
    "\n",
    "    # Function for processing each bach\n",
    "    def process_batch(engine, batch):\n",
    "        \n",
    "        # Reset the optimizer's weight to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calcualte the loss\n",
    "        loss_v = calc_loss(batch, net, tgt_net.target_model, gamma=params.gamma, device=device)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss_v.backward()\n",
    "        \n",
    "        # Do optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track epsilon\n",
    "        epsilon_tracker.frame(engine.state.iteration)\n",
    "        \n",
    "        # Every target_net_sync times\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            \n",
    "            # Sync the source network with target network\n",
    "            tgt_net.sync()\n",
    "\n",
    "        return {\"loss\": loss_v.item(), \"epsilon\": selector.epsilon,}\n",
    "\n",
    "    # Instantiate the engine\n",
    "    engine = Engine(process_batch)\n",
    "    \n",
    "    # Setup the ignite\n",
    "    common.setup_ignite(engine, params, exp_source, NAME)\n",
    "    \n",
    "    # Run engine\n",
    "    engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "From my experiments, the distributional version of DQN converged a bit slower and less stably than the original DQN, which is not surprising, as the network output is now 51 times larger and the loss function has changed. So, hyperparameter tuning is required. I've tried to avoid this, as it might take another chapter to describe\n",
    "the process and not allow us to compare the effect of the methods in a meaningful way. The following charts show the reward dynamics and loss value over time. Obviously, the learning rate could be increased.\n",
    "\n",
    "<img src=\"assets/fig8.19.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"assets/fig8.20.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the categorical DQN is the only method that we have compared, and it behaves worse than the classic DQN. However, there is one factor that protects this new method: Pong is too simple a game to draw conclusions. In the 2017 A Distributional Perspective paper, the authors reported state-of-the-art scores for more than half of the games from the Atari benchmark (Pong was not among them).\n",
    "\n",
    "It might be interesting to look into the dynamics of the probability distribution during the training. The extended version of the code in Chapter08/07_dqn_ distrib_plots.py has two flags—SAVE_STATES_IMG and SAVE_TRANSITIONS_ IMG—that enable the saving of probability distribution images during the training. For example, the following image shows the probability distribution for all six actions for one state at the beginning of the training (after 30k frames).\n",
    "\n",
    "<img src=\"assets/fig8.21.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the distributions are very wide (as the network hasn't converged yet), and the peak in the middle corresponds to the negative reward that the network expects to get from its actions. The same state after 500k frames of training is shown in the following figure:\n",
    "\n",
    "<img src=\"assets/fig8.22.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that different actions have different distributions. The first action (which corresponds to the NOOP, the do nothing action) has its distribution shifted to the left, so doing nothing in this state usually leads to losing. The fifth action, which is RIGHTFIRE, has the mean value shifted to the right, so this action leads to a better score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 08. Combining everything (Rainbow Algorithm)\n",
    "\n",
    "---\n",
    "\n",
    "You have now seen all the DQN improvements mentioned in the paper Rainbow: Combining Improvements in Deep Reinforcement Learning, but it was done in an incremental way, which helped you to understand the idea and implementation of every improvement. The main point of the paper was to combine those improvements and check the results. In the final example, I've decided to exclude categorical DQN and double DQN from the final system, as they haven't shown too much improvement on our guinea pig environment. If you want, you can add them and try using a different game. The complete example is available in Chapter08/08_ dqn_rainbow.py.\n",
    "\n",
    "First of all, we need to define our network architecture and the methods that have contributed to it:\n",
    "\n",
    "- __Dueling DQN:__ Our network will have two separate paths for the value of the state distribution and advantage distribution. On the output, both paths will be summed together, providing the final value probability distributions for actions. To force the advantage distribution to have a zero mean, we will subtract the distribution with the mean advantage in every atom.\n",
    "\n",
    "- __Noisy networks:__ Our linear layers in the value and advantage paths will be noisy variants of nn.Linear.\n",
    "\n",
    "In addition to network architecture changes, we will use the prioritized replay buffer to keep environment transitions and sample them proportionally to the MSE loss. Finally, we will unroll the Bellman equation to n-steps.\n",
    "I'm not going to repeat all the code, as individual methods have already been given in the preceding sections, and it should be obvious what the final result of combining the methods will look like. If you have any trouble, you can find the code on GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import ptan\n",
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine\n",
    "from lib import common, dqn_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run name for ignite\n",
    "NAME = \"08_rainbow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of N-Steps\n",
    "N_STEPS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha (the number that shows how much emphasis we give to the priority)\n",
    "PRIO_REPLAY_ALPHA = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss for rainbow algorithm\n",
    "def calc_loss_rainbow(batch, batch_weights, net, tgt_net, gamma, device = \"cpu\", double = True):\n",
    "    \n",
    "    # Unpack the bach\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "\n",
    "    # Convert S, A, R, done_mask, batch_weights to tensors + Convert it into device\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    batch_weights_v = torch.tensor(batch_weights).to(device)\n",
    "\n",
    "    # Unsqueeze the actions (add extra dimension to last dimension)\n",
    "    actions_v = actions_v.unsqueeze(-1)\n",
    "    \n",
    "    # Feedforward in source network + Gathers values along an axis specified by \"dim\" \n",
    "    state_action_values = net(states_v).gather(1, actions_v)\n",
    "    \n",
    "    # If the last dimension is 1 then remove it\n",
    "    state_action_values = state_action_values.squeeze(-1)\n",
    "    \n",
    "    # Avoid gradient calculation\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Convert S' into tensor + Convert it into device\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        \n",
    "        # If double is true\n",
    "        if double:\n",
    "            \n",
    "            # Feedforward the next states in source network + Get the maximum value + Select the 2nd value\n",
    "            next_state_actions = net(next_states_v).max(1)[1]\n",
    "            \n",
    "            # Unsqueeze the actions (add extra dimension to last dimension)\n",
    "            next_state_actions = next_state_actions.unsqueeze(-1)\n",
    "            \n",
    "            # Feedforward the next states in target network + Gathers values along an axis specified by \"dim\"\n",
    "            # + If the last dimension is 1 then remove it\n",
    "            next_state_values = tgt_net(next_states_v).gather(1, next_state_actions).squeeze(-1)\n",
    "            \n",
    "        # If double is false\n",
    "        else:\n",
    "            \n",
    "            # Feedforward the next states in target network + Get the maximum value + Select the qst value\n",
    "            next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "            \n",
    "        # Mask the next_state_values\n",
    "        next_state_values[done_mask] = 0.0\n",
    "        \n",
    "        # Calculate the expected state-action values\n",
    "        expected_state_action_values = next_state_values.detach() * gamma + rewards_v\n",
    "        \n",
    "    # Calculate the loss\n",
    "    losses_v = (state_action_values - expected_state_action_values) ** 2\n",
    "    losses_v *= batch_weights_v\n",
    "    \n",
    "    return losses_v.mean(), (losses_v + 1e-5).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte the loss for priority\n",
    "def calc_loss_prio(batch, batch_weights, net, tgt_net, gamma, device = \"cpu\"):\n",
    "    \n",
    "    # Unpack the batch\n",
    "    states, actions, rewards, dones, next_states = common.unpack_batch(batch)\n",
    "\n",
    "    # Convert the S, A, R, done_mask, batch_weights to tensor + Convert it to device\n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    batch_weights_v = torch.tensor(batch_weights).to(device)\n",
    "\n",
    "    # Feedforward in source network + Gathers values along an axis specified by \"dim\" + If the last dimension is 1 then remove it\n",
    "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # Ignore gradient calculation\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Convert S' to tensor + Convert it to device\n",
    "        next_states_v = torch.tensor(next_states).to(device)\n",
    "        \n",
    "        # Feedforward in target network + Find the maximum value + Select the 1st value\n",
    "        next_state_values = tgt_net(next_states_v).max(1)[0]\n",
    "        \n",
    "        # Mask the next_state_values\n",
    "        next_state_values[done_mask] = 0.0\n",
    "        \n",
    "        # Calcualte the exected state-action values\n",
    "        expected_state_action_values = next_state_values.detach() * gamma + rewards_v\n",
    "        \n",
    "    # Calculate the loss\n",
    "    losses_v = batch_weights_v * (state_action_values - expected_state_action_values) ** 2\n",
    "    \n",
    "    return losses_v.mean(), (losses_v + 1e-5).data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward=-21, steps=761, speed=0.0 f/s, elapsed=0:02:26\n",
      "Episode 2: reward=-21, steps=760, speed=0.0 f/s, elapsed=0:02:26\n",
      "Episode 3: reward=-21, steps=758, speed=0.0 f/s, elapsed=0:02:26\n",
      "Episode 4: reward=-21, steps=756, speed=0.0 f/s, elapsed=0:02:26\n",
      "Episode 5: reward=-21, steps=762, speed=0.0 f/s, elapsed=0:02:26\n",
      "Episode 6: reward=-21, steps=758, speed=0.0 f/s, elapsed=0:02:26\n",
      "Episode 7: reward=-21, steps=761, speed=0.0 f/s, elapsed=0:02:26\n",
      "Episode 8: reward=-21, steps=758, speed=0.0 f/s, elapsed=0:02:26\n",
      "Episode 9: reward=-21, steps=756, speed=0.0 f/s, elapsed=0:02:26\n",
      "Episode 10: reward=-21, steps=762, speed=0.0 f/s, elapsed=0:02:26\n",
      "Episode 11: reward=-21, steps=762, speed=0.0 f/s, elapsed=0:02:26\n",
      "Episode 12: reward=-21, steps=762, speed=0.0 f/s, elapsed=0:02:26\n",
      "Episode 13: reward=-21, steps=758, speed=0.0 f/s, elapsed=0:02:26\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Set the random seed\n",
    "    random.seed(common.SEED)\n",
    "    \n",
    "    # Set the random seed for pytorch\n",
    "    torch.manual_seed(common.SEED)\n",
    "    \n",
    "    # Get the hyperparameters for \"pong\" environment\n",
    "    params = common.HYPERPARAMS['pong']\n",
    "    \n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Make the environment\n",
    "    env = gym.make(params.env_name)\n",
    "    \n",
    "    # Apply a common set of wrappers for Atari games\n",
    "    env = ptan.common.wrappers.wrap_dqn(env)\n",
    "    \n",
    "    # Set the random seed for environment\n",
    "    env.seed(common.SEED)\n",
    "\n",
    "    # Instantiate the Rainbow DQN + Convert it to device\n",
    "    net = dqn_extra.RainbowDQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "\n",
    "    # Instantiate the target network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    # Instanatiate the action selector (argmax)\n",
    "    selector = ptan.actions.ArgmaxActionSelector()\n",
    "    \n",
    "    # Instantiate the DQN agent\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "    # Instantiate the experience source (first-last)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma = params.gamma, steps_count = N_STEPS)\n",
    "    \n",
    "    # Instantiate the priority replay buffer\n",
    "    buffer = dqn_extra.PrioReplayBuffer(exp_source, params.replay_size, PRIO_REPLAY_ALPHA)\n",
    "    \n",
    "    # Instantiate the Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr = params.learning_rate)\n",
    "\n",
    "    # Function for processing each batch\n",
    "    def process_batch(engine, batch_data):\n",
    "        \n",
    "        # Get the batch data, indices and weights\n",
    "        batch, batch_indices, batch_weights = batch_data\n",
    "        \n",
    "        # Reset the optimizer's weight to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calcualte the loss priority\n",
    "        loss_v, sample_prios = calc_loss_prio(batch, \n",
    "                                              batch_weights, \n",
    "                                              net, \n",
    "                                              tgt_net.target_model, \n",
    "                                              gamma = params.gamma**N_STEPS, \n",
    "                                              device = device)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss_v.backward()\n",
    "        \n",
    "        # Do optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the buffer\n",
    "        buffer.update_priorities(batch_indices, sample_prios)\n",
    "        \n",
    "        # Each target_net_sync times\n",
    "        if engine.state.iteration % params.target_net_sync == 0:\n",
    "            \n",
    "            # Sync the source network's weight with target network\n",
    "            tgt_net.sync()\n",
    "            \n",
    "        return {\"loss\": loss_v.item(), \"beta\": buffer.update_beta(engine.state.iteration),}\n",
    "\n",
    "    # Instantiate the engine\n",
    "    engine = Engine(process_batch)\n",
    "    \n",
    "    # Setup the ignite\n",
    "    common.setup_ignite(engine, params, exp_source, NAME)\n",
    "    \n",
    "    # Run the engine\n",
    "    engine.run(common.batch_generator(buffer, params.replay_initial, params.batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The following are reward charts. On the left is the combined system alone, and on the right, it is compared to our baseline code.\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig8.23.png\">\n",
    "\n",
    "As you can see, we got an improvement both in terms of wall clock time (50 minutes versus two hours) and the number of game episodes required (180 versus 520). The wall clock time speed-up is much more modest than sample efficiency, due to lower system performance in terms of FPS. The following chart compares the performance:\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig8.24.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the chart, you can see that at the beginning of the training, the combined system showed 110 FPS, but the performance dropped to 95 due to inefficiencies in the replay buffer. The drop from 160 to 110 FPS was caused by the more complicated network architecture (noisy networks, dueling). Probably the code could be optimized in some way. Anyway, the final result is still positive.\n",
    "\n",
    "The final chart I'd like to discuss is the number of steps in episodes (Figure 8.25).\n",
    "As you can see, the combined system discovered very quickly how to win very efficiently. The first peak to 3k steps corresponds to the initial phase of training when the system found ways to stand against the opponent for longer. The later drop in the number of steps corresponds to the \"policy polishing\" phase, when our agent optimized the actions to win faster (as discounted factor gamma pushed it toward shorter episodes).\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig8.25.png\">\n",
    "\n",
    "Of course, you are free to experiment with other combinations of improvements, or you can even combine them with the material of the next chapter, which discusses ways to speed up RL methods from the technical angle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this chapter, we have walked through and implemented a lot of DQN improvements that have been discovered by researchers since the first DQN\n",
    "paper was published in 2015. This list is far from complete. First of all, for the list\n",
    "of methods, I used the paper Rainbow: Combining Improvements in Deep Reinforcement Learning, which was published by DeepMind, so the list of methods is definitely biased to DeepMind papers. Secondly, RL is so active nowadays that new papers come out almost every day, which makes it very hard to keep up, even if we\n",
    "limit ourselves to one kind of RL model, such as a DQN. The goal of this chapter was to give you a practical view of different ideas that the field has developed.\n",
    "\n",
    "In the next chapter, we will continue discussing practical DQN applications from an engineering perspective by talking about ways to improve DQN performance without touching the underlying method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement Learning",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
