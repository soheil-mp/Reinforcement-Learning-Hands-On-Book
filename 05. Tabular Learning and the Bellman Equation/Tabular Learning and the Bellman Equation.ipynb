{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">Tabular Learning and the Bellman Equation</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this chapter, we'll look at a group of methods, called __Q-learning__, which have much more flexibility and power than the cross-entropy method we learnt. This chapter will establish the required background shared by those methods. We'll also revisit the FrozenLake environment and show how new concepts will fit with this environment.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Value, State, and Optimality\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### 1.1. Value of a State\n",
    "\n",
    "\n",
    "As you may remember in chapter 1, we defined the __value of a state__ as an expected total rewar􏰀d that is obtainable from the state. In this chapter, we will explore it further. Formally, the value of a state is defined as follow:\n",
    "\n",
    "<img width=\"200px\" src=\"./assets/vs.png\"> \n",
    "\n",
    "- <b>r<sub>t</sub></b> : Local reward obtained at the step t of the episode.\n",
    "\n",
    "<br>\n",
    "\n",
    "The total reward could be __discounted__ or not; it's up to us how to define it. Value is always calculated in the respect of some policy that our agent follows. To illustrate, let's consider a very simple environment with three states:\n",
    "\n",
    "<img width=\"500px\" src=\"./assets/transition_prob.png\">\n",
    "\n",
    "- State 1: The agent's initial state.\n",
    "- State 2: The final state that the agent is in after executing action \"right\" from the initial state. The reward obtained from this is 1.\n",
    "- State 3: The final state that the agent is in after action \"down\". The reward obtained from this is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment in Figure 1 has the following features:\n",
    "\n",
    "- It is always __deterministic__.\n",
    "- Every action succeeds.\n",
    "- We always start from state 1.\n",
    "- Once we reach to state 2 or state 3, the episode ends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to **calculate the value of state 1**. Keep in mind that for calculating the value of a state, we need to know the policy (agent's behavior) beforehand. Even in this environment which is super simple, the agent can have infinite amount of behaviours (which will affect the value of state 1). For example let's consider different policies and then calculate the value of state 1 for those policies:\n",
    "\n",
    "Policy | Value of State 1 \n",
    "--- | --- \n",
    "Always go to right | The value of state is 1.0 since if the agent goes right, it obtains 1 and the episode ends.\n",
    "Always go to down | The value of state is 2.0\n",
    "Go right with probability of 0.5 and go down with probability of 0.5 | The value of state is (1.0 X 0.5) + (2.0 X 0.5) = 1.5\n",
    "Go right in 10% of cases and go down in 90% of cases | The value of state is (1.0 X 0.1) + (2.0 X 0.9) = 1.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.2. Optimal Policy\n",
    "\n",
    "\n",
    "Now let'see how to get the __optimal policy__ for this agent. As you know, the goal of RL is to get as much total reward as possible. For the preceding example, the total reward is equal to the value of state 1 which is at the maximum at policy 2 (always going down).\n",
    "\n",
    "From the preceding example, you may have a false impression that we should always take the action with the highest reward. In general, it's not that simple. To demonstrate this, let's extend our preceding environment with yet another state that is reachable from state 3. State 3 is no longer a terminal state, but a transition to state 4, with a bad reward of -20. Once we've chosen the \"down\" action in state 1, this bad reward is unavoidable, as from state 3 we have only one exit. So, it's a trap for the agent who has decided that \"being greedy\" is a good strategy.\n",
    "\n",
    "<img width=\"400px\" src=\"./assets/extra_state.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "With that addition, our values for state 1 will be calculated in the following way:\n",
    "\n",
    "Policy | Value of State 1 \n",
    "--- | --- \n",
    "Always go to right | The value of state is 1.0\n",
    "Always go to down | The value of state is 2.0 + (-20) = -18\n",
    "Go right with probability of 0.5 and go down with probability of 0.5 | The value of state is (0.5 X 1.0) + (0.5 X (2.0 + -20)) = -8.5\n",
    "Go right in 10% of cases and go down in 90% of cases | The value of state is (0.1 X 1.0) + (0.9 X (2.0 + -20)) = -8\n",
    "\n",
    "So, the best policy for this new environment is policy number one which is \"always go right\".\n",
    "\n",
    "We spent some time discussing naïve and trivial environments so that you realize the complexity of this optimality problem and can appreciate the results of __Richard Bellman__ better. Richard was an American mathematician, who formulated and proved his famous __\"Bellman equation\"__. We'll talk about it in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 2. The Bellman Equation of Optimality\n",
    "\n",
    "<hr>\n",
    "\n",
    "To explain the Bellman equation, it's better to go a bit abstract. \n",
    "\n",
    "<img width=\"500px\" src=\"./assets/n_reachable_state.png\">\n",
    "\n",
    "1. Start with a deterministic case which all actions have a 100% guaranteed outcome. <br><br>\n",
    "2. The agent observes state S<sub>0</sub> and has N available actions. <br><br>\n",
    "3. Every action leads to another state, S<sub>1</sub>...S<sub>N</sub>, with a respective reward, r<sub>1</sub>...r<sub>N</sub>. <br><br>\n",
    "4. Assume that we know the values V<sub>i</sub> of all states connected to the state S<sub>0</sub>. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's discuss what will be the best course of action that the agent can take in such a state? \n",
    "\n",
    "- If we take action a<sub>i</sub>, then the value given to this action can be calculated using the following equation:\n",
    "\n",
    "$$V_0(a=a_i) = r_i + Vi$$ \n",
    "\n",
    "\n",
    "- In order to choose the best possible action, the agent needs to calculate the resulting values for every action. Then it should choose the maximum outcome. This can be shown with the following equation:\n",
    "\n",
    "$$V_0 = max_{a\\in1...N}(r_a + V_a)$$\n",
    "\n",
    "- In case of using a discount factor γ, we need to multiply the value of the next state by gamma:\n",
    "\n",
    "$$V_0 = max_{a\\in1...N}(r_a +  \\gamma V_a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to greedy example of previous section. However, there is one difference. When acting greedy, instead of only looking at the immediate reward for the action, we look at the immediate reward plus the long-term value of the state. Richard Bellman proved that with that extension, the agent's behavior will get the best possible outcome. In other words, it will be optimal. So, the preceding equation is called the **Bellman equation** of value (for a deterministic case).\n",
    "\n",
    "This idea can extended for a stochastic case (when our actions can end up in different states) as well. In this case, we need to calculate the expected value for every action, instead of just taking the value of the next state. To illustrate this, consider a single action from state S<sub>0</sub>, with three possible outcomes.\n",
    "\n",
    "<img width=\"500px\" src=\"./assets/transition_state.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding example, one action could lead to three different states with different probabilities (of course p<sub>1</sub> + p<sub>2</sub> + p<sub>3</sub> = 1). For example:\n",
    "\n",
    "- With probability p<sub>1</sub>, we can end up in state S<sub>1</sub> with reward of r<sub>1</sub>.\n",
    "- With probability p<sub>2</sub>, we can end up in state S<sub>2</sub> with reward of r<sub>2</sub>.\n",
    "- With probability p<sub>3</sub>, we can end up in state S<sub>3</sub> with reward of r<sub>3</sub>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can calculate the expected value after taking action a=1 by summing up all values and multiplying by their probabilities:\n",
    "\n",
    "<img width=\"500px\" src=\"./assets/form1.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "The preceding equation can be re-written more formally:\n",
    "\n",
    "<img width=\"500px\" src=\"./assets/form2.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "By combining the Bellman equation, for a deterministic case, with a value for stochastic actions, we get the Bellman optimality equation for a general case:\n",
    "\n",
    "<img width=\"550px\" src=\"./assets/form3.png\">\n",
    "\n",
    "- <code>p<sub>a,i→j</sub></code> means the probability of action a, issued in state i, to end up in state j.\n",
    "\n",
    "Let's interpret the equation: the optimal value of the state is equal to the action, which gives us the maximum possible expected immediate reward, plus discounted long-term reward for the next state. You may also notice that this definition is recursive: the value of the state is defined via the values of immediate reachable states.\n",
    "\n",
    "This recursion may look like cheating: we define some value, pretending that we already know it. However, this is a very powerful and common technique in computer science and even in math in general (proof by induction is based on the same trick). \n",
    "\n",
    "These values give us the best reward that we can obtain and also the optimal policy to obtain that reward. if our agent knows the value for every state, then it automatically knows how to gather all this reward. Thanks to Bellman's optimality proof, at every state the agent selects the action with the maximum expected reward for the action, which is a sum of the immediate reward and the one-step discounted long-term reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 3. Value of Action\n",
    "\n",
    "<hr>\n",
    "\n",
    "Value of action Q<sub>s,a</sub> is equal to the total reward we can get by taking action a in state s, and it can be defined via V<sub>s</sub>. This quantity gave a name to the whole family of methods called \"Q-learning\". In these methods, our primary objective is to get values of Q for every pair of state and action. \n",
    "\n",
    "- The action value Q for state s and action a is equal to the expected immediate reward and the discounted long-term reward of the destination state. The equation is as follows:\n",
    "\n",
    "<img width=\"550px\" src=\"./assets/form4.png\">\n",
    "\n",
    "- It is possible to define value of state V<sub>s</sub> via value of action Q<sub>s,a</sub> as well:\n",
    "\n",
    "<img width=\"150px\" src=\"./assets/form5.png\">\n",
    "\n",
    "- Finally, Q(s, a) can be expressed via itself, which will be used in the next chapter's topic of Q-learning:\n",
    "\n",
    "<img width=\"320px\" src=\"./assets/form6.png\">\n",
    "\n",
    "For example, consider an environment similar to FrozenLake which has a much simpler structure: There is one initial state S<sub>0</sub> surrounded by four target states (e.g. S<sub>1</sub>,S<sub>2</sub>,S<sub>3</sub>,S<sub>4</sub>) with different rewards.\n",
    "\n",
    "<img width=\"350px\" src=\"./assets/grid_env.png\">\n",
    "\n",
    "Every action is probabilistic in the same way as in FrozenLake: \n",
    "1. With a 33% chance our action will be executed without modifications.\n",
    "2. With a 33% chance we will slip to the left, relatively, of our target cell.\n",
    "3. With a 33% chance we will slip to the right. \n",
    "4. For simplicity, we use discount factor gamma=1.\n",
    "\n",
    "<img width=\"450px\" src=\"./assets/transition_diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the value of actions for Figure 6:\n",
    "- <b>Terminal states S<sub>1</sub> ... S<sub>4</sub></b>:\n",
    "    1. These states have no outbound connections, so Q for those states is zero (for all actions). \n",
    "    2. The values of the states are equal to their immediate reward since once we get there, our episode ends without any subsequent states: <br> V<sub>1</sub> = 1, V<sub>2</sub> = 2, V<sub>3</sub> = 3, V<sub>4</sub> = 4.\n",
    "\n",
    "<br>\n",
    "\n",
    "- __State 0__: \n",
    "    1. The values of actions are a bit more complicated for this state. \n",
    "        - In \"up\" action, the value of action is equal to the expected sum of the immediate reward plus long-term value for subsequent steps. We have no subsequent steps for any possible transition for the \"up\" action, so <img width=\"600px\" src=\"./assets/answer_q.png\">\n",
    "        \n",
    "        - Similary, in \"left\", \"right\", \"down\" actions, the value of action is equal to:\n",
    "        <img width=\"400px\" src=\"./assets/three_form.png\">\n",
    "        \n",
    "        - The final value for state s<sub>0</sub> is the maximum of those actions' values, which is 2.97."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's much simpler to make decisions about actions based on Q than based on V. In the case of Q, for each state the agent just needs to calculate Q for all available actions and choose the action with the largest value of Q.\n",
    "\n",
    "To do the same using values of states, the agent needs to know the values and also transition probabilities. However, in practice, we rarely know them in advance, so the agent needs to estimate transition probabilities for every action and state pair. \n",
    "\n",
    "Later in this chapter, we'll see this in practice by solving the FrozenLake environment both ways. However, to be able to do this, we have one important thing still missing: a general way to calculate those V<sub>s</sub> and Q<sub>s</sub>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 4. The Value Iteration Method\n",
    "\n",
    "<hr>\n",
    "\n",
    "In the simplistic example we just saw, to calculate the values of states and actions, we have exploited the structure of the environment: we had no loops in transitions, so we could start from terminal states, calculate their values and then proceed to the central state. However, just one loop in the environment builds an obstacle in our approach. Let's consider such an environment with two states:\n",
    "\n",
    "<img width=\"530px\" src=\"./assets/simple_environment.png\">\n",
    "\n",
    "We start from state S<sub>1</sub> , and the only action we can take leads us to state S<sub>2</sub>. We get reward r=1,and the only transition from S<sub>2</sub> is an action, which brings us back to the S<sub>1</sub>. So, the life of our agent is an infinite sequence of states [S<sub>1</sub>, S<sub>2</sub>, S<sub>1</sub>, S<sub>2</sub>, ...]. To deal with this infinity loop, we can use a discount factor γ = 0.9. Now, the question is, what are the values for both the states?\n",
    "\n",
    "The answer is not very complicated, though. Every transition from S<sub>1</sub> to S<sub>2</sub> gives us a reward of 1 and every back transition gives us 2. So, our sequence of rewards will be [1, 2, 1, 2, ....]. As there is only one action available in every state, our agent has no choice, so we can omit the max operation in formulas (there is only one alternative). The value for every state will be equal to the infinite sum:\n",
    "\n",
    "<img width=\"530px\" src=\"./assets/two_formu.png\">\n",
    "\n",
    "Strictly speaking, we cannot calculate the exact values for our states, but with γ = 0.9, the contribution of every transition quickly decreases over time. For example, after 10 steps, γ<sup>10</sup> = 0.910 = 0.349, but after 100 steps it becomes just 0.0000266. Due to this, we can stop after 50 iterations and still get quite a precise estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.736450674121663"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate state value V(s1)\n",
    "discount_factor = 0.9\n",
    "sum([1*discount_factor**(2*i) + 2*(discount_factor**(2*i+1)) \\\n",
    "     for i in range(50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.262752483911719"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate state value V(s2)\n",
    "discount_factor = 0.9\n",
    "sum([2*(discount_factor**(2*i)) + 1*discount_factor**(2*i+1) \\\n",
    "     for i in range(50)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding example could be used to get a gist of a more general procedure, called the __\"value iteration algorithm\"__ which allows us to numerically calculate the values of states and values of actions of MDPs with known transition probabilities and rewards. \n",
    "\n",
    "- The procedure for <u>values of states</u> includes the following steps:\n",
    "    1. Initialize values of all states V<sub>i</sub> to some initial value (usually zero)\n",
    "    2. For every s􏰀tate s in the MDP, perform the Bellman update: <img width=\"300px\" src=\"assets/vs_two.png\">\n",
    "    3. Repeat step 2 for some large number of steps or until changes become too small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The procedure for <u>action values (that is Q)</u>, includes the following steps:\n",
    "\n",
    "    1. Initialize all Q<sub>s,a</sub> to zero\n",
    "    2. For ever􏰀y state s and every action a in this state, perform update: <img width=\"400px\" src=\"assets/qsa_two.png\">\n",
    "    3. Repeat step 2\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so that's the theory. However, in practice, this method has several obvious limitations:\n",
    "\n",
    "1. __The state space should be discrete and small enough to perform multiple iterations over all states.__ This is not an issue for FrozenLake-4x4 or FrozenLake-8x8, but for CartPole, the observation is 4 float values, which represent some physical characteristics of the system. Even a small difference in those values could have an influence on the state's value. A potential solution for that could be discretization of our observation's values, for example, we can split the observation space of the CartPole into bins and treat every bin as an individual discrete state in space. However, this will create lots of practical problems, such as how large bin intervals should be and how much data from the environment we'll need to estimate our values. We'll address this issue in subsequent chapters, when we get to the usage of neural networks in Q-learning.\n",
    "\n",
    "\n",
    "2. __We rarely know the transition probability for the actions and rewards matrix.__ Remember that we observe the state, decide on an action and only then do we get the next observation and reward for the transition. We don't know what the probability is to get into state S<sub>1</sub> from state S<sub>0</sub> by issuing action a<sub>0</sub>. What we do have is the history from the agent's interaction with the environment. However, in Bellman's update, we need both a reward for every transition and the probability of this transition. So, the obvious answer to this issue is to use our agent's experience as an estimation for both unknowns. Rewards could be used as they are. We just need to remember what reward we've got on transition from S<sub>0</sub> to S<sub>1</sub>, using action a, but to estimate probabilities we need to maintain counters for every tuple (S<sub>0</sub>, S<sub>1</sub>, a<sub>0</sub>) and normalize them.\n",
    "\n",
    "\n",
    "Okay, now let's look at how the value iteration method will work for FrozenLake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 5. Value Iteration in Practice\n",
    "\n",
    "<hr>\n",
    "\n",
    "The central data structures in this example are as follows:\n",
    "\n",
    "- **Reward table:** <br> A dictionary with:\n",
    "    - The composite key \"<u>source state\" + \"action\" + \"target state</u>\".\n",
    "    - The value is obtained from the <u>immediate reward</u>. <br><br>\n",
    "- **Transitions table:** <br>A dictionary keeping counters of the experienced transitions. \n",
    "    - The key is the composite \"<u>state\" + \"action</u>\". \n",
    "    - The value is another <u>dictionary that maps the target state into a count of times that we've seen it</u>.\n",
    "    - For example, if in state 0 we execute action 1 ten times, after three times it leads us to state 4 and after seven times to state 5. In another word, entry with the key (0, 1) in this table will be a dict {4: 3, 5: 7}. We use this table to estimate the probabilities of our transitions. <br><br>\n",
    "- **Value table:** <br> A dictionary that maps a <u>state</u> into the <u>calculated value of this state</u>.\n",
    "\n",
    "\n",
    "The overall logic of our code is simple: in the loop, we play 100 random steps from the environment, populating the reward and transition tables. After those 100 steps, we perform a value iteration loop over all states, updating our value table. Then we play several full episodes to check our improvements using the updated value table. If the average reward for those test episodes is above the 0.8 boundary, then we stop training. During test episodes, we also update our reward and transition tables to use all data from the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent class\n",
    "class Agent:\n",
    "    \n",
    "    # The constructor\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor function.\n",
    "        \"\"\"\n",
    "        \n",
    "        # The environment\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        \n",
    "        # Resets the state of the environment and get an initial observation\n",
    "        self.state = self.env.reset()\n",
    "        \n",
    "        # The reward table\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        \n",
    "        # The transitions table\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        \n",
    "        # The value table\n",
    "        self.values = collections.defaultdict(float)\n",
    "\n",
    "        \n",
    "    # Function for playing N random steps and gathering experience\n",
    "    def play_n_random_steps(self, count):\n",
    "        \"\"\"\n",
    "        Play N random steps and gathering experience.\n",
    "        \"\"\"\n",
    "\n",
    "        # Iterate through count\n",
    "        for _ in range(count):\n",
    "\n",
    "            # Sample a random action\n",
    "            action = self.env.action_space.sample()\n",
    "\n",
    "            # Take action and get the observation, reward, done, info\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "\n",
    "            # Update the reward\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "\n",
    "            # Update the transition table\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "\n",
    "            # Update the state\n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "        \n",
    "\n",
    "    # Calculate the value of action    # ** MORE NOTES IN BELOW **\n",
    "    def calc_action_value(self, state, action):\n",
    "        \"\"\"\n",
    "        Calculate the value of action. This is used for two purposees:\n",
    "            1. To select the best action\n",
    "            2. To calculate the new value of the state on value iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract the transition counters for the given state and action\n",
    "        target_counts = self.transits[(state, action)]\n",
    "\n",
    "        # Get the total count of times we've executed the action from the state\n",
    "        total = sum(target_counts.values())\n",
    "\n",
    "        # Initialize the action value\n",
    "        action_value = 0.0\n",
    "\n",
    "        # Iterate through every target state that our action has landed on\n",
    "        for target_state, count in target_counts.items():\n",
    "\n",
    "            # Get the reward\n",
    "            reward = self.rewards[(state, action, target_state)]\n",
    "\n",
    "            # Update the action value using Bellman Equation \n",
    "            action_value += (count / total) * (reward + GAMMA * self.values[target_state])\n",
    "\n",
    "        return action_value\n",
    "\n",
    "    \n",
    "    # Function for selecting the best action from given state\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Selecting the best action from given state. \n",
    "        This action selection process is deterministic, as the play_n_random_steps() function introduces enough \n",
    "        exploration. So, our agent will behave greedily in regard to our value approximation.\n",
    "        \"\"\"\n",
    "        # Initialize the best action and best value\n",
    "        best_action, best_value = None, None\n",
    "\n",
    "        # Iterate through all possible actions\n",
    "        for action in range(self.env.action_space.n):\n",
    "\n",
    "            # Calculate the action value\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "\n",
    "            # If best_value is none OR less than action value\n",
    "            if (best_value is None) or (best_value < action_value):\n",
    "\n",
    "                # Update the best_value with action_value\n",
    "                best_value = action_value\n",
    "\n",
    "                # Update the best action with action\n",
    "                best_action = action\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    \n",
    "    # Function for playing one full episode \n",
    "    def play_episode(self, env):\n",
    "        \"\"\"\n",
    "        Playing one full episode using select_action function for finding the best action.\n",
    "        \"\"\"\n",
    "        # Initialize the toal reward\n",
    "        total_reward = 0.0\n",
    "\n",
    "        # Reset the state of the environment and returns an initial observation\n",
    "        state = env.reset()\n",
    "\n",
    "        # Infinite loop\n",
    "        while True:\n",
    "\n",
    "            # Select the best action \n",
    "            action = self.select_action(state)\n",
    "\n",
    "            # Take action and get the observation, reward, done, info\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # Update the reward\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "\n",
    "            # Update the transition table\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "\n",
    "            # Update the total reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # If terminal state\n",
    "            if is_done:\n",
    "\n",
    "                # Break the look\n",
    "                break\n",
    "\n",
    "            # Update the state\n",
    "            state = new_state\n",
    "\n",
    "        return total_reward\n",
    "\n",
    "    \n",
    "    # Value iteration function\n",
    "    def value_iteration(self):\n",
    "        \"\"\"\n",
    "        Value iteration function.\n",
    "        \"\"\"\n",
    "\n",
    "        # Iterate through states\n",
    "        for state in range(self.env.observation_space.n):\n",
    "\n",
    "            # Calculate the state values\n",
    "            state_values = [self.calc_action_value(state, action) \\\n",
    "                            for action in range(self.env.action_space.n)]\n",
    "\n",
    "            # Update the value of current state with the maximum value of the action available from the state\n",
    "            self.values[state] = max(state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.100\n",
      "Best reward updated 0.100 -> 0.200\n",
      "Best reward updated 0.200 -> 0.250\n",
      "Best reward updated 0.250 -> 0.350\n",
      "Best reward updated 0.350 -> 0.400\n",
      "Best reward updated 0.400 -> 0.500\n",
      "Best reward updated 0.500 -> 0.550\n",
      "Best reward updated 0.550 -> 0.600\n",
      "Best reward updated 0.600 -> 0.850\n",
      "Solved in 63 iterations!\n"
     ]
    }
   ],
   "source": [
    "# Execute the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create the environment\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    \n",
    "    # Initialize the agent\n",
    "    agent = Agent()\n",
    "    \n",
    "    # Summary writer of data (for TensorBoard)\n",
    "    writer = SummaryWriter(comment=\"-v-learning\")\n",
    "\n",
    "    # Initialize the iteration number\n",
    "    iter_no = 0\n",
    "\n",
    "    # Initialize the best reward\n",
    "    best_reward = 0.0\n",
    "\n",
    "    # Infinite loop\n",
    "    while True:\n",
    "\n",
    "        # Increment the iteration number\n",
    "        iter_no += 1\n",
    "\n",
    "        # Perform 100 random steps to fill our reward and transition tables with fresh data\n",
    "        agent.play_n_random_steps(100)\n",
    "\n",
    "        # Run value iteration over all states. \n",
    "        agent.value_iteration()\n",
    "\n",
    "        # Initialize the reward\n",
    "        reward = 0.0\n",
    "\n",
    "        # Iterate through TEST_EPISODES (episode numbers)\n",
    "        for _ in range(TEST_EPISODES):\n",
    "\n",
    "            # Update the reward\n",
    "            reward += agent.play_episode(test_env)\n",
    "\n",
    "        # Divide reward by TEST_EPISODES\n",
    "        reward /= TEST_EPISODES\n",
    "\n",
    "        # Write the values for reward to TensorBoard\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "\n",
    "        # If reward is higher than best_reward\n",
    "        if reward > best_reward:\n",
    "\n",
    "            # Print the reward & best reward\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "\n",
    "            # Update the best_reward\n",
    "            best_reward = reward\n",
    "\n",
    "            # If reward is higher than 0.8 then it is SOLVED\n",
    "            if reward > 0.80:\n",
    "                print(\"Solved in %d iterations!\" % iter_no)\n",
    "                break\n",
    "\n",
    "    # Close the writer            \n",
    "    writer.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__* NOTE ABOUT calc_action_value IN ABOVE:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logic behind the calc_action_value function is illustrated in the following diagram and we do the following:\n",
    "\n",
    "1. We extract transition counters for the given state and action from the transition table. Counters in this table have a form of dict, with target states as key and a count of experienced transitions as value. We sum all counters to obtain the total count of times we've executed the action from the state. We will use this total value later to go from an individual counter to probability.\n",
    "\n",
    "2. Then we iterate every target state that our action has landed on and calculate its contribution into the total action value using the Bellman equation. This contribution equals to immediate reward plus discounted value for the target state. We multiply this sum to the probability of this transition and add the result to the final action value.\n",
    "\n",
    "<img width=\"600px\" src=\"assets/state_value_calculation.png\">\n",
    "\n",
    "In our diagram, we have an illustration of a calculation of value for state s and action a. Imagine that during our experience, we have executed this action several times (c<sub>1</sub>+c<sub>2</sub>) and it ends up in one of two states, s<sub>1</sub> or s<sub>2</sub>. How many times we have switched to each of these states is stored in our transition table as dict {s<sub>1</sub> : c<sub>1</sub> , s<sub>2</sub> : c<sub>2</sub>}. Then, the approximate value for the state and action Q(s, a) will be equal to the probability of every state, multiplied to the value of the state. From the Bellman equation, this equals to the sum of the immediate reward and the discounted long-term state value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__* END OF NOTE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.12.2 at http://localhost:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir runs --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our solution is stochastic, and my experiments usually required from 12 to 100 iterations to reach a solution, but in all cases, it took less than a second to find a good policy that could solve the environment in 80% of runs. If you remember how many hours were required to achieve a 60% success ratio using Cross-entropy, then you can understand that this is a major improvement. There are several reasons for that.\n",
    "\n",
    "\n",
    "- First of all, the stochastic outcome of our actions, plus the length of the episodes (6-10 steps on average), makes it hard for the Cross-entropy method to understand what was done right in the episode and which step was a mistake. The value iteration works with individual values of state (or action) and incorporates the probabilistic outcome of actions naturally, by estimating probability and calculating the expected value. So, it's much simpler for the value iteration and requires much less data from the environment (which is called **sample efficiency** in RL).\n",
    "\n",
    "\n",
    "- The second reason is the fact that the value iteration doesn't need full episodes to start learning. In an extreme case, we can start updating our values just from the single example. However, for FrozenLake, due to the reward structure (we get 1 only after successfully reaching the target state), we still need to have at least one successful episode to start learning from a useful value table, which may be challenging to achieve in more complex environments. For example, you can try switching existing code to a larger version of FrozenLake, which has the name **FrozenLake8x8-v0.** The larger version of FrozenLake can take from 50 to 400 iterations to solve, and, according to TensorBoard charts, most of the time it waits for the first successful episode, then very quickly reaches convergence. The following is a chart with two lines. Orange corresponds to the reward during the training\n",
    "of **FrozenLake-v0 (4x4)** and blue is the reward of **FrozenLake8x8-v0.**:\n",
    "\n",
    "<img width=\"400px\" src=\"assets/convergence_frozen_lake.png\">\n",
    "\n",
    "Now it's time to compare the code that learns the values of states, as we just discussed, to the code that learns the values of actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 6. Q-learning for FrozenLake\n",
    "\n",
    "<hr>\n",
    "\n",
    "The difference in this example and the prvious one is really minor. These differences are as follows:\n",
    "\n",
    "1. The most obvious change is the value table. In the previous example, we kept the value of the state, so the key in the dictionary was just a state. Now we need to store values of the Q-function, which has two parameters: state and action, so the key in the value table is now a composite.\n",
    "\n",
    "2. The second difference is in our calc_action_value function. We just don't need it anymore, as our action values are stored in the value table. \n",
    "\n",
    "3. Finally, the most important change in the code is in the agent's value_iteration method. Before, it was just a wrapper around the calc_action_value call, which did the job of Bellman approximation. Now, as this function has gone and was replaced by a value table, we need to do this approximation in the value_iteration method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import collections\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ENV_NAME = \"FrozenLake-v0\"\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The agent class\n",
    "class Agent:\n",
    "    \n",
    "    # The constructor\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor function.\n",
    "        \"\"\"\n",
    "        # The environment\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        \n",
    "        # Resets the state of the environment and get an initial observation\n",
    "        self.state = self.env.reset()\n",
    "        \n",
    "        # The reward table\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        \n",
    "        # The transitions table\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        \n",
    "        # The value table\n",
    "        self.values = collections.defaultdict(float)\n",
    "        \n",
    "        \n",
    "    # Function for playing N random steps and gathering experience\n",
    "    def play_n_random_steps(self, count):\n",
    "        \"\"\"\n",
    "        Play N random steps and gathering experience.\n",
    "        \"\"\"\n",
    "        # Iterate through count\n",
    "        for _ in range(count):\n",
    "\n",
    "            # Sample a random action\n",
    "            action = self.env.action_space.sample()\n",
    "\n",
    "            # Take action and get the observation, reward, done, info\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "\n",
    "            # Update the reward\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "\n",
    "            # Update the transition table\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "\n",
    "            # Update the state\n",
    "            self.state = self.env.reset() if is_done else new_state\n",
    "            \n",
    "\n",
    "    # Function for selecting the best action from given state\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Selecting the best action from given state\n",
    "        \"\"\"\n",
    "        # Initialize the best action and best value\n",
    "        best_action, best_value = None, None\n",
    "\n",
    "        # Iterate through all possible actions\n",
    "        for action in range(self.env.action_space.n):\n",
    "\n",
    "            # Get the action value\n",
    "            action_value = self.values[(state, action)]\n",
    "\n",
    "            # If best_value is none OR less than action value\n",
    "            if (best_value is None) or (best_value < action_value):\n",
    "\n",
    "                # Update the best_value with action_value\n",
    "                best_value = action_value\n",
    "\n",
    "                # Update the best action with action\n",
    "                best_action = action\n",
    "\n",
    "        return best_action\n",
    "    \n",
    "    \n",
    "    # Function for playing one full episode \n",
    "    def play_episode(self, env):\n",
    "        \"\"\"\n",
    "        Playing one full episode using select_action function for finding the best action.\n",
    "        \"\"\"\n",
    "        # Initialize the toal reward\n",
    "        total_reward = 0.0\n",
    "\n",
    "        # Reset the state of the environment and returns an initial observation\n",
    "        state = env.reset()\n",
    "\n",
    "        # Infinite loop\n",
    "        while True:\n",
    "\n",
    "            # Select the best action \n",
    "            action = self.select_action(state)\n",
    "\n",
    "            # Take action and get the observation, reward, done, info\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "\n",
    "            # Update the reward\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "\n",
    "            # Update the transition table\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "\n",
    "            # Update the total reward\n",
    "            total_reward += reward\n",
    "\n",
    "            # If terminal state\n",
    "            if is_done:\n",
    "\n",
    "                # Break the look\n",
    "                break\n",
    "\n",
    "            # Update the state\n",
    "            state = new_state\n",
    "\n",
    "        return total_reward\n",
    "                        \n",
    "                \n",
    "    # Value iteration function\n",
    "    def value_iteration(self):\n",
    "        \"\"\"\n",
    "        Value iteration function\n",
    "        \"\"\"\n",
    "        # Iterate through state space number\n",
    "        for state in range(self.env.observation_space.n): \n",
    "\n",
    "            # Iterate through action space number\n",
    "            for action in range(self.env.action_space.n):\n",
    "\n",
    "                # Initialize the action value\n",
    "                action_value = 0.0\n",
    "\n",
    "                # Extract the transition counters for the given state and action\n",
    "                target_counts = self.transits[(state, action)]\n",
    "\n",
    "                # Get the total count of times we've executed the action from the state\n",
    "                total = sum(target_counts.values())\n",
    "\n",
    "                # Iterate through every target state that our action has landed on\n",
    "                for tgt_state, count in target_counts.items():\n",
    "\n",
    "                    # Get the reward\n",
    "                    reward = self.rewards[(state, action, tgt_state)]\n",
    "\n",
    "                    # Select the best action\n",
    "                    best_action = self.select_action(tgt_state)\n",
    "\n",
    "                    # Update the action value\n",
    "                    action_value += (count / total) * (reward + GAMMA * self.values[(tgt_state, best_action)])\n",
    "\n",
    "                    # Update the value table\n",
    "                    self.values[(state, action)] = action_value                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best reward updated 0.000 -> 0.100\n",
      "Best reward updated 0.100 -> 0.150\n",
      "Best reward updated 0.150 -> 0.300\n",
      "Best reward updated 0.300 -> 0.350\n",
      "Best reward updated 0.350 -> 0.750\n",
      "Best reward updated 0.750 -> 0.900\n",
      "Solved in 22 iterations!\n"
     ]
    }
   ],
   "source": [
    "# Execute the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create the environment\n",
    "    test_env = gym.make(ENV_NAME)\n",
    "    \n",
    "    # Initialize the agent\n",
    "    agent = Agent()\n",
    "    \n",
    "    # Summary writer of data (for TensorBoard)\n",
    "    writer = SummaryWriter(comment=\"-q-iteration\")\n",
    "\n",
    "    # Initialize the iteration number\n",
    "    iter_no = 0\n",
    "    \n",
    "    # Initialize the best reward\n",
    "    best_reward = 0.0\n",
    "    \n",
    "    # Infinite loop\n",
    "    while True:\n",
    "        \n",
    "        # Increment the iteration number\n",
    "        iter_no += 1\n",
    "        \n",
    "        # Perform 100 random steps to fill our reward and transition tables with fresh data\n",
    "        agent.play_n_random_steps(100)\n",
    "        \n",
    "        # Run value iteration over all states\n",
    "        agent.value_iteration()\n",
    "\n",
    "        # Initialize the reward\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Iterate through TEST_EPISODES (episode numbers)\n",
    "        for _ in range(TEST_EPISODES):\n",
    "            \n",
    "            # Update the reward\n",
    "            reward += agent.play_episode(test_env)\n",
    "            \n",
    "        # Divide reward by TEST_EPISODES\n",
    "        reward /= TEST_EPISODES\n",
    "        \n",
    "        # Write the values for reward to TensorBoard\n",
    "        writer.add_scalar(\"reward\", reward, iter_no)\n",
    "        \n",
    "        # If reward is higher than best_reward\n",
    "        if reward > best_reward:\n",
    "            \n",
    "            # Print the reward & best reward\n",
    "            print(\"Best reward updated %.3f -> %.3f\" % (best_reward, reward))\n",
    "            \n",
    "            # Update the best_reward\n",
    "            best_reward = reward\n",
    "            \n",
    "        # If reward is higher than 0.8 then it is SOLVED\n",
    "        if reward > 0.80:\n",
    "            print(\"Solved in %d iterations!\" % iter_no)\n",
    "            break\n",
    "            \n",
    "    # Close the writer            \n",
    "    writer.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is very similar to calc_action_value in the previous example and in fact it does almost the same thing. For the given state and action, it needs to calculate the value of this action using statistics about target states that we've reached with the action. To calculate this value, we use the Bellman equation and our counters, which allow us to approximate the probability of the target state. However, in Bellman's equation we have the value of the state and now we need to calculate it differently. Before, we had it stored in the value table (as we approximated the value of states), so we just took it from this table. We can't do this anymore, so we have to call the select_action method, which will choose for us the action with the largest Q-value, and then we take this Q-value as the value of the target state. Of course, we can implement another function which could calculate for us this value of state, but select_action does almost everything we need, so we will reuse it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I said, we don't have the calc_action_value method anymore, so, to select action, we just iterate over the actions and look up their values in our values table. It could look like a minor improvement, but if you think about the data that we used in calc_action_value, it may become obvious why the learning of the Q-function is much more popular in RL than the learning of the V-function.\n",
    "\n",
    "\n",
    "Our calc_action_value function uses both information about reward and probabilities. It's not a huge problem for the value iteration method, which relies on this information during training. However, in the next chapter, we'll learn about the value iteration method extension, which doesn't require probability approximation, but just takes it from the environment samples. For such methods, this dependency on probability adds an extra burden for the agent. In the case of Q-learning, what the agent needs to make the decision is just Q-values.\n",
    "\n",
    "\n",
    "I don't want to say that V-functions are completely useless, because they are an essential part of Actor-Critic method which we'll talk about in part three of this book. However, in the area of value learning, Q-functions is the definite favorite. With regard to convergence speed, both our versions are almost identical (but the Q-learning version requires four times more memory for the value table)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 7. Summary\n",
    "\n",
    "<hr>\n",
    "\n",
    "My congratulations, you've made another step towards understanding modern, state-of-the-art RL methods! We learned about some very important concepts that are widely used in deep RL: the value of state, the value of actions, and the Bellman equation in various forms. We saw the value iteration method, which is a very important building block in the area of Q-learning. Finally, we got to know how value iteration can improve our FrozenLake solution.\n",
    "\n",
    "\n",
    "In the next chapter, we'll learn about deep Q-networks, which started the deep RL revolution in 2013, by beating humans on lots of Atari 2600 games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "***THE END***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
