{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center;\">Stocks Trading Using RL</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "Rather than learning new methods to solve toy reinforcement learning (RL) problems in this chapter, we will try to utilize our deep Q-network (DQN) knowledge to deal with the much more practical problem of financial trading. I can't promise that the code will make you super rich on the stock market or Forex, because my goal is much less ambitious: to demonstrate how to go beyond the Atari games and apply RL to a different practical domain.\n",
    "\n",
    "In this chapter, we will:\n",
    "\n",
    "- Implement our own OpenAI Gym environment to simulate the stock market\n",
    "- Apply the DQN method that you learned in Chapter 6, Deep Q-Networks, and Chapter 8, DQN Extensions, to train an agent to trade stocks to maximize profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 01. Trading\n",
    "\n",
    "---\n",
    "\n",
    "There are a lot of financial instruments traded on markets every day: goods, stocks, and currencies. Even weather forecasts can be bought or sold using so- called \"weather derivatives,\" which is just a consequence of the complexity of the modern world and financial markets. If your income depends on future weather conditions, like a business growing crops, then you might want to hedge the risks by buying weather derivatives. All these different items have a price that changes over time. Trading is the activity of buying and selling financial instruments with different goals, like making a profit (investment), gaining protection from future price movement (hedging), or just getting what you need (like buying steel or exchanging USD for JPY to pay a contract).\n",
    "\n",
    "Since the first financial market was established, people have been trying to predict future price movements, as this promises many benefits, like \"profit from nowhere\" or protecting capital from sudden market movements.\n",
    "\n",
    "This problem is known to be complex, and there are a lot of financial consultants, investment funds, banks, and individual traders trying to predict the market and find the best moments to buy and sell to maximize profit.\n",
    "\n",
    "The question is: can we look at the problem from the RL angle? Let's say that we have some observation of the market, and we want to make a decision: buy, sell, or wait. If we buy before the price goes up, our profit will be positive; otherwise, we will get a negative reward. What we're trying to do is get as much profit as possible. The connections between market trading and RL are quite obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 02. Data\n",
    "\n",
    "---\n",
    "\n",
    "In our example, we will use the Russian stock market prices from the period of 2015- 2016, which are placed in Chapter08/data/ch08-small-quotes.tgz and have to be unpacked before model training.\n",
    "\n",
    "Inside the archive, we have CSV files with M1 bars, which means that every row in each CSV file corresponds to a single minute in time, and price movement during that minute is captured with four prices: open, high, low, and close. Here, an open price is the price at the beginning of the minute, high is the maximum price during the interval, low is the minimum price, and the close price is the last price of the minute time interval. Every minute interval is called a bar and allows us to have an idea of price movement within the interval. For example, in the YNDX_160101_161231.csv file (which has Yandex company stocks for 2016), we have 130k lines in this form:\n",
    "\n",
    "```\n",
    "                                <DATE>,<TIME>,<OPEN>,<HIGH>,<LOW>,<CLOSE>,<VOL>\n",
    "                                20160104,100100,1148.9,1148.9,1148.9,1148.9,0\n",
    "                                20160104,100200,1148.9,1148.9,1148.9,1148.9,50\n",
    "                                20160104,100300,1149.0,1149.0,1149.0,1149.0,33\n",
    "                                20160104,100400,1149.0,1149.0,1149.0,1149.0,4\n",
    "                                20160104,100500,1153.0,1153.0,1153.0,1153.0,0\n",
    "                                20160104,100600,1156.9,1157.9,1153.0,1153.0,43\n",
    "                                20160104,100700,1150.6,1150.6,1150.4,1150.4,5\n",
    "                                20160104,100800,1150.2,1150.2,1150.2,1150.2,4\n",
    "                                ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two columns are the date and time for the minute; the next four columns are open, high, low, and close prices; and the last value represents the number of buy and sell orders performed during the bar. The exact interpretation of this number is stock- and market-dependent, but usually, volumes give you an idea about how active the market was.\n",
    "\n",
    "The typical way to represent those prices is called a candlestick chart, where every bar is shown as a candle. Part of Yandex's quotes for one day in February 2016 is shown in the following chart. The archive contains two files with M1 data for 2016 and 2015. We will use data from 2016 for model training and data from 2015 for validation.\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig10.1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 03. Problem statements and key decisions\n",
    "\n",
    "---\n",
    "\n",
    "The finance domain is large and complex, so you can easily spend several years learning something new every day. In our example, we will just scratch the surface a bit with our RL tools, and our problem will be formulated as simply as possible, using price as an observation. We will investigate whether it will be possible for our agent to learn when the best time is to buy one single share and then close the position to maximize the profit. The purpose of this example is to show how flexible the RL model can be and what the first steps are that you usually need to take to apply RL to a real-life use case.\n",
    "\n",
    "As you already know, to formulate RL problems, three things are needed: observation of the environment, possible actions, and a reward system. In previous chapters, all three were already given to us, and the internal machinery of the environment was hidden. Now we're in a different situation, so we need to decide ourselves what our agent will see and what set of actions it can take. The reward system is also not given as a strict set of rules; rather, it will be guided by our feelings and knowledge of the domain, which gives us lots of flexibility.\n",
    "\n",
    "Flexibility, in this case, is good and bad at the same time. It's good that we have the freedom to pass some information to the agent that we feel will be important to learn efficiently. For example, you can pass to the trading agent not only prices, but also news or important statistics (which are known to influence financial markets a lot). The bad part is that this flexibility usually means that to find a good agent, you need to try a lot of variants of data representation, and it's not always obvious which will work better. In our case, we will implement the basic trading agent in its simplest form. The observation will include the following information:\n",
    "- N past bars, where each has open, high, low, and close prices\n",
    "- An indication that the share was bought some time ago (only one share at a time will be possible)\n",
    "- Profit or loss that we currently have from our current position (the share bought)\n",
    "\n",
    "\n",
    "At every step, after every minute's bar, the agent can take one of the following actions:\n",
    "- __Do nothing:__ skip the bar without taking an action\n",
    "- __Buy a share:__ if the agent has already got the share, nothing will be bought; otherwise, we will pay the commission, which is usually some small percentage of the current price\n",
    "- __Close the position:__ if we do not have a previously purchased share, nothing will happen; otherwise, we will pay the commission for the trade\n",
    "\n",
    "\n",
    "The reward that the agent receives can be expressed in various ways. On the one hand, we can split the reward into multiple steps during our ownership of the share. In that case, the reward on every step will be equal to the last bar's movement. On the other hand, the agent will receive the reward only after the close action and receive the full reward at once. At first sight, both variants should have the same final result, but maybe with different convergence speeds. However, in practice, the difference could be dramatic. We will implement both variants to compare them.\n",
    "\n",
    "One last decision to make is how to represent the prices in our environment observation. Ideally, we would like our agent to be independent of actual price values and take into account relative movement, such as \"the stock has grown 1% during the last bar\" or \"the stock has lost 5%.\" This makes sense, as different stocks' prices can vary, but they can have similar movement patterns. In finance, there is a branch of analytics called technical analysis that studies such patterns to help to make predictions from them. We would like our system to be able to discover the patterns (if they exist). To achieve this, we will convert every bar's open, high, low, and close prices to three numbers showing high, low, and close prices represented as a percentage of the open price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This representation has its own drawbacks, as we're potentially losing the information about key price levels. For example, it's known that markets have a tendency to bounce from round price numbers (like $8,000 per bitcoin) and levels that were turning points in the past. However, as already stated, we're just playing with the data here and checking the concept. Representation in the form of relative price movement will help the system to find repeating patterns in the price level (if they exist, of course), regardless of the absolute price position. Potentially, the NN could learn this on its own (it's just the mean price that needs to be subtracted from the absolute price values), but relative representation simplifies the NN's task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 04. The trading environment\n",
    "\n",
    "---\n",
    "\n",
    "As we have a lot of code (methods, utility classes in PTAN, and so on) that is supposed to work with OpenAI Gym, we will implement the trading functionality following Gym's Env class API, which should be familiar to you. Our environment is implemented in the StocksEnv class in the Chapter10/lib/environ.py module. It uses several internal classes to keep its state and encode observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import gym.spaces\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import EnvSpec\n",
    "import enum\n",
    "import numpy as np\n",
    "\n",
    "from lib import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar count\n",
    "DEFAULT_BARS_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commision percentage\n",
    "DEFAULT_COMMISSION_PERC = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for possible actions\n",
    "class Actions(enum.Enum):\n",
    "    \n",
    "    # Doing nothing\n",
    "    Skip = 0\n",
    "    \n",
    "    # Buying \n",
    "    Buy = 1\n",
    "    \n",
    "    # Selling \n",
    "    Close = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for states\n",
    "class State:\n",
    "    \n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, bars_count, commission_perc, reset_on_close, reward_on_close = True, volumes =True):\n",
    "        \n",
    "        # Testing\n",
    "        assert isinstance(bars_count, int)\n",
    "        assert bars_count > 0\n",
    "        assert isinstance(commission_perc, float)\n",
    "        assert commission_perc >= 0.0\n",
    "        assert isinstance(reset_on_close, bool)\n",
    "        assert isinstance(reward_on_close, bool)\n",
    "        \n",
    "        # Initiaze the bar count\n",
    "        self.bars_count = bars_count\n",
    "        \n",
    "        # Initialize the commission percentage\n",
    "        self.commission_perc = commission_perc\n",
    "        \n",
    "        # Initialize the reset on close\n",
    "        self.reset_on_close = reset_on_close\n",
    "        \n",
    "        # Initialize the reward on close\n",
    "        self.reward_on_close = reward_on_close\n",
    "        \n",
    "        # Initialize the volumes\n",
    "        self.volumes = volumes\n",
    "\n",
    "        \n",
    "    # Function for reseting\n",
    "    def reset(self, prices, offset):\n",
    "        \"\"\"\n",
    "        In the beginning, we don't have any shares bought, so our state has have_position=False and open_ price=0.0.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Testing\n",
    "        assert isinstance(prices, data.Prices)\n",
    "        assert offset >= self.bars_count - 1\n",
    "        \n",
    "        # Initialize have_position to false\n",
    "        self.have_position = False\n",
    "        \n",
    "        # Initialize the open price to zero\n",
    "        self.open_price = 0.0\n",
    "        \n",
    "        # Initialize the price to given price value\n",
    "        self._prices = prices\n",
    "        \n",
    "        # Initialize the offset to offset value\n",
    "        self._offset = offset\n",
    "\n",
    "\n",
    "    # Using getters and setters using @property\n",
    "    @property\n",
    "    \n",
    "    # Function for getting the shape\n",
    "    def shape(self):\n",
    "        \"\"\"\n",
    "        This function returns the shape of the state representation in a NumPy array. The State class is encoded \n",
    "        into a single vector, which includes prices with optional volumes and two numbers indicating the presence \n",
    "        of a bought share and position profit.\n",
    "        \"\"\"\n",
    "        \n",
    "        # [h, l, c] * bars + position_flag + rel_profit\n",
    "        \n",
    "        # If volume is defined\n",
    "        if self.volumes:\n",
    "            \n",
    "            return 4 * self.bars_count + 1 + 1,\n",
    "        \n",
    "        # If volume is NOT defined\n",
    "        else:\n",
    "            \n",
    "            return 3 * self.bars_count + 1 + 1,\n",
    "\n",
    "        \n",
    "    # Function for encoding the prices to get the final observations\n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "        The preceding method encodes prices at the current offset into a NumPy array, which will be the \n",
    "        observation of the agent.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Instantiate a n-dimensional array\n",
    "        res = np.ndarray(shape = self.shape, dtype = np.float32)\n",
    "        \n",
    "        # Initialize the shift with zero\n",
    "        shift = 0\n",
    "        \n",
    "        # Loop over the bar index\n",
    "        for bar_idx in range(-self.bars_count+1, 1):\n",
    "            \n",
    "            # Get the offset\n",
    "            ofs = self._offset + bar_idx\n",
    "            \n",
    "            # Add high price to res\n",
    "            res[shift] = self._prices.high[ofs]\n",
    "            \n",
    "            # Increment the shift\n",
    "            shift += 1\n",
    "            \n",
    "            # Add low price to res\n",
    "            res[shift] = self._prices.low[ofs]\n",
    "            \n",
    "            # Increment the shift\n",
    "            shift += 1\n",
    "            \n",
    "            # Add close price to res\n",
    "            res[shift] = self._prices.close[ofs]\n",
    "            \n",
    "            # Increment the shift\n",
    "            shift += 1\n",
    "            \n",
    "            # If volume is defined\n",
    "            if self.volumes:\n",
    "                \n",
    "                # Add volume to res\n",
    "                res[shift] = self._prices.volume[ofs]\n",
    "                \n",
    "                # Increment the shift\n",
    "                shift += 1\n",
    "                \n",
    "        # Add have_position to res\n",
    "        res[shift] = float(self.have_position)\n",
    "        \n",
    "        # Increment the shift\n",
    "        shift += 1\n",
    "        \n",
    "        # If have_position is NOT defined\n",
    "        if not self.have_position:\n",
    "            \n",
    "            # Add 0 to res\n",
    "            res[shift] = 0.0\n",
    "            \n",
    "        # If have_position is defined\n",
    "        else:\n",
    "            \n",
    "            # Add \"(close price / open price) - 1\" to res\n",
    "            res[shift] = self._cur_close() / self.open_price - 1.0\n",
    "            \n",
    "        return res\n",
    "\n",
    "    \n",
    "    # Function for calculating the current bar's close price\n",
    "    def _cur_close(self):\n",
    "        \"\"\"\n",
    "        Calculate real close price for the current bar\n",
    "        \"\"\"\n",
    "        # Get the open price\n",
    "        open = self._prices.open[self._offset]\n",
    "        \n",
    "        # # Get the close price\n",
    "        rel_close = self._prices.close[self._offset]\n",
    "        \n",
    "        # return \"open price * (1 + close price))\"\n",
    "        return open * (1.0 + rel_close)\n",
    "\n",
    "    \n",
    "    # Function for performing one step\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform one step in our price, adjust offset, check for the end of prices and handle position change.\n",
    "        Said differently, this function is responsible for performing one step in our environment. On exit, it has \n",
    "        to return the reward in a percentage and an indication of the episode ending.\n",
    "        \n",
    "        ARGUMENTS\n",
    "        ==========================\n",
    "            - action\n",
    "            \n",
    "        RETURNS\n",
    "        ==========================\n",
    "            - reward\n",
    "            - done\n",
    "        \"\"\"\n",
    "        \n",
    "        # Testing\n",
    "        assert isinstance(action, Actions)\n",
    "        \n",
    "        # Initialize the reward with zero\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Initialize the done with False\n",
    "        done = False\n",
    "        \n",
    "        # Get the current close price\n",
    "        close = self._cur_close()\n",
    "        \n",
    "        # If the agent has decided to buy a share\n",
    "        if (action == Actions.Buy) and (not self.have_position):\n",
    "            \n",
    "            # Change the state by setting have_position to true\n",
    "            self.have_position = True\n",
    "            \n",
    "            # Set the close price to open price\n",
    "            \"\"\"In our state, we assume the instant order execution at the current bar's close price, which is a \n",
    "            simplification on our side; normally, an order can be executed on a different price, which is called \n",
    "            price slippage.\"\"\"\n",
    "            self.open_price = close\n",
    "            \n",
    "            # Pay the commission\n",
    "            reward -= self.commission_perc\n",
    "            \n",
    "        # If the agent has decided to sell a share\n",
    "        elif (action == Actions.Close) and (self.have_position):\n",
    "            \n",
    "            # Pay the commission\n",
    "            reward -= self.commission_perc\n",
    "            \n",
    "            # Change the done flag by applying inplace bitwise OR operation to reset_on_close\n",
    "            done |= self.reset_on_close\n",
    "            \n",
    "            # If reward on close is defined\n",
    "            if self.reward_on_close:\n",
    "                \n",
    "                # Update reward\n",
    "                reward += 100.0 * (close / self.open_price - 1.0)\n",
    "                \n",
    "            # Set have_position to false\n",
    "            self.have_position = False\n",
    "            \n",
    "            # Set open price to zero\n",
    "            self.open_price = 0.0\n",
    "\n",
    "        # Increment offset\n",
    "        self._offset += 1\n",
    "        \n",
    "        # Set current close to previous close\n",
    "        prev_close = close\n",
    "        \n",
    "        # Get the current close\n",
    "        close = self._cur_close()\n",
    "        \n",
    "        # Get the done\n",
    "        done |= self._offset >= self._prices.close.shape[0]-1\n",
    "\n",
    "        # If have_position is true AND reward_on_close is false\n",
    "        if (self.have_position) and (not self.reward_on_close):\n",
    "            \n",
    "            # Update reward\n",
    "            reward += 100.0 * (close / prev_close - 1.0)\n",
    "\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for changing the shape of state so it's suitable for 1D convolution\n",
    "class State1D(State):\n",
    "    \"\"\"\n",
    "    The shape of this representation is different, as our prices are encoded as a 2D matrix suitable for a 1D \n",
    "    convolution operator.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using getters and setters using @property\n",
    "    @property\n",
    "    \n",
    "    # Function for getting the shape\n",
    "    def shape(self):\n",
    "        \n",
    "        # If volumes exists\n",
    "        if self.volumes:\n",
    "            \n",
    "            # Return the shape\n",
    "            return (6, self.bars_count)\n",
    "        \n",
    "        # If volumes does NOT exist\n",
    "        else:\n",
    "            \n",
    "            # Return the shape\n",
    "            return (5, self.bars_count)\n",
    "\n",
    "        \n",
    "    # Encoding the prices\n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "        This method encodes the prices in our matrix, depending on the current offset, whether we need volumes, \n",
    "        and whether we have stock.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize observations with zeros\n",
    "        res = np.zeros(shape = self.shape, dtype = np.float32)\n",
    "        \n",
    "        # Get the start\n",
    "        start = self._offset - (self.bars_count - 1)\n",
    "        \n",
    "        # Get the stop\n",
    "        stop = self._offset + 1\n",
    "        \n",
    "        # Add the high price to res\n",
    "        res[0] = self._prices.high[start:stop]\n",
    "        \n",
    "        # Add the low price to res\n",
    "        res[1] = self._prices.low[start:stop]\n",
    "        \n",
    "        # Add the close price to res\n",
    "        res[2] = self._prices.close[start:stop]\n",
    "        \n",
    "        # If volumes exists\n",
    "        if self.volumes:\n",
    "            \n",
    "            # Add the volume to res\n",
    "            res[3] = self._prices.volume[start:stop]\n",
    "            \n",
    "            # TODO: \n",
    "            dst = 4\n",
    "            \n",
    "        # If volumes does NOT exist\n",
    "        else:\n",
    "            \n",
    "            # TODO:\n",
    "            dst = 3\n",
    "            \n",
    "        # If have_position exists\n",
    "        if self.have_position:\n",
    "            \n",
    "            # TODO: \n",
    "            res[dst] = 1.0\n",
    "            \n",
    "            # TODO: \n",
    "            res[dst + 1] = self._cur_close() / self.open_price - 1.0\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for stock market environment\n",
    "class StocksEnv(gym.Env):\n",
    "    \n",
    "    # Set the meta data\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    # Specification for a particular instance of the environment. Used to register the parameters for official evaluations.\n",
    "    spec = EnvSpec(\"StocksEnv-v0\")\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self, prices, bars_count = DEFAULT_BARS_COUNT, commission = DEFAULT_COMMISSION_PERC,\n",
    "                 reset_on_close = True, state_1d = False, random_ofs_on_reset = True, reward_on_close = False,\n",
    "                 volumes = False):\n",
    "        \"\"\"\n",
    "        ARGUMENTS\n",
    "        ===============================\n",
    "            - prices: Contains one or more stock prices for one or more instruments as a dict, where keys are \n",
    "                      the instrument's name and the value is a container object data.Prices, which holds price \n",
    "                      data arrays.\n",
    "                      \n",
    "            - bars_count: The count of bars that we pass in the observation. By default, this is 10 bars.\n",
    "            \n",
    "            - commission: The percentage of the stock price that we have to pay to the broker on buying and selling \n",
    "                          the stock. By default, it's 0.1%.\n",
    "                          \n",
    "            - reset_on_close: If this parameter is set to True, which it is by default, every time the agent asks \n",
    "                              us to close the existing position (in other words, sell a share), we stop the episode. \n",
    "                              Otherwise, the episode will continue until the end of our time series, which is one \n",
    "                              year of data.\n",
    "                              \n",
    "            - conv_1d: This Boolean argument switches between different representations of price data in the \n",
    "                       observation passed to the agent. If it is set to True, observations have a 2D shape, with \n",
    "                       different price components for subsequent bars organized in rows. For example, high prices \n",
    "                       (max price for the bar) are placed on the first row, low prices on the second, and close \n",
    "                       prices on the third. This representation is suitable for doing 1D convolution on time series, \n",
    "                       where every row in the data has the same meaning as different color planes (red, green, or \n",
    "                       blue) in Atari 2D images. If we set this option to False, we have one single array of data \n",
    "                       with every bar's components placed together. This organization is convenient for a fully \n",
    "                       connected network architecture. Both representations are illustrated in Figure 10.2.\n",
    "                       \n",
    "            - random_ofs_on_reset: If the parameter is True (by default), on every reset of the environment, the \n",
    "                                   random offset in the time series will be chosen. Otherwise, we will start from \n",
    "                                   the beginning of the data.\n",
    "                                   \n",
    "            - reward_on_close: This Boolean parameter switches between the two reward schemes discussed previously. \n",
    "                               If it is set to True, the agent will receive a reward only on the \"close\" action issue.\n",
    "                               Otherwise, we will give a small reward every bar, corresponding to price movement \n",
    "                               during that bar.\n",
    "                               \n",
    "            - volumes: This argument switches on volumes in observations and is disabled by default.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Testing\n",
    "        assert isinstance(prices, dict)\n",
    "        \n",
    "        # Set the prices\n",
    "        self._prices = prices\n",
    "        \n",
    "        # If state_1d is true\n",
    "        if state_1d:\n",
    "            \n",
    "            # Set the 1D state\n",
    "            self._state = State1D(bars_count, commission, reset_on_close, reward_on_close = reward_on_close, \n",
    "                                  volumes = volumes)\n",
    "            \n",
    "        # If state_1d is false\n",
    "        else:\n",
    "            \n",
    "            # Set the state\n",
    "            self._state = State(bars_count, commission, reset_on_close, reward_on_close = reward_on_close, \n",
    "                                volumes = volumes)\n",
    "            \n",
    "        # Set the action space\n",
    "        self.action_space = gym.spaces.Discrete(n = len(Actions))\n",
    "        \n",
    "        # Set the observation space\n",
    "        self.observation_space = gym.spaces.Box(low = -np.inf, high = np.inf, shape = self._state.shape, \n",
    "                                                dtype = np.float32)\n",
    "        \n",
    "        # TODO\n",
    "        self.random_ofs_on_reset = random_ofs_on_reset\n",
    "        \n",
    "        # Set the seed\n",
    "        self.seed()\n",
    "\n",
    "        \n",
    "    # Function for reseting\n",
    "    def reset(self):\n",
    "        \n",
    "        # Make selection of the instrument and it's offset. Then reset the state\n",
    "        self._instrument = self.np_random.choice(list(self._prices.keys()))\n",
    "        \n",
    "        # Set the initial prices\n",
    "        prices = self._prices[self._instrument]\n",
    "        \n",
    "        # Set the initial bars\n",
    "        bars = self._state.bars_count\n",
    "        \n",
    "        # If random_ofs_on_reset is defined\n",
    "        if self.random_ofs_on_reset:\n",
    "            \n",
    "            # Define offset\n",
    "            offset = self.np_random.choice(prices.high.shape[0]-bars*10) + bars\n",
    "            \n",
    "        # If random_ofs_on_reset is NOT defined\n",
    "        else:\n",
    "            \n",
    "            # Define offset\n",
    "            offset = bars\n",
    "            \n",
    "        # Reset the states\n",
    "        self._state.reset(prices, offset)\n",
    "        \n",
    "        return self._state.encode()\n",
    "\n",
    "    \n",
    "    # Function for handling the action chosen by the agent and return the next observation, reward, and done flag\n",
    "    def step(self, action_idx):\n",
    "        \n",
    "        # Action\n",
    "        action = Actions(action_idx)\n",
    "        \n",
    "        # Reward and done flag\n",
    "        reward, done = self._state.step(action)\n",
    "        \n",
    "        # Observations\n",
    "        obs = self._state.encode()\n",
    "        \n",
    "        # Extra info\n",
    "        info = {\"instrument\": self._instrument, \"offset\": self._state._offset}\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "\n",
    "    \n",
    "    # Function for rendering the current state\n",
    "    def render(self, mode = 'human', close = False):\n",
    "        \"\"\"\n",
    "        Render the current state in human or machine-readable format. For example, the market environment could \n",
    "        render current prices as a chart to visualize what the agent sees at that moment. Our environment doesn't \n",
    "        support rendering, so this method does nothing.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # Function for calling on the environment's destruction to free the allocated resources\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # Function for setting the seeds\n",
    "    def seed(self, seed = None):\n",
    "        \n",
    "        # \n",
    "        self.np_random, seed1 = seeding.np_random(seed)\n",
    "        \n",
    "        # \n",
    "        seed2 = seeding.hash_seed(seed1 + 1) % 2 ** 31\n",
    "        \n",
    "        return [seed1, seed2]\n",
    "\n",
    "    \n",
    "    # Using function without creation of class\n",
    "    @classmethod\n",
    "    \n",
    "    # Getting the price dictionary from directory\n",
    "    def from_dir(cls, data_dir, **kwargs):\n",
    "        \n",
    "        # Get the price dictionary (open, high, low, close, and volume) using load_relative() in data.py\n",
    "        prices = {file: data.load_relative(file) for file in data.price_files(data_dir)}\n",
    "        \n",
    "        return StocksEnv(prices, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 05. Models\n",
    "\n",
    "---\n",
    "In this example, two architectures of DQN are used: a simple feed-forward network with three layers and a network with 1D convolution as a feature extractor, followed by two fully connected layers to output Q-values. Both of them use the dueling architecture described in Chapter 8, DQN Extensions. Double DQN and two-step Bellman unrolling have also been used. The rest of the process is the same as in a classical DQN (from Chapter 6, Deep Q-Networks).\n",
    "\n",
    "Both models are in Chapter10/lib/models.py and are very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 5.1. Model 1 - Simple Feed-Forward Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for simple feed-forward network\n",
    "class SimpleFFDQN(nn.Module):\n",
    "    \n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, obs_len, actions_n):\n",
    "        \n",
    "        # Inherite the parent's constructors\n",
    "        super(SimpleFFDQN, self).__init__()\n",
    "\n",
    "        # Sequenctial layer for getting Q-values\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(obs_len, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        # Sequenctial layer for getting advantages\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(obs_len, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions_n)\n",
    "        )\n",
    "\n",
    "    \n",
    "    # Function for doing feedforward\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Feedforward and get the Q-values\n",
    "        val = self.fc_val(x)\n",
    "        \n",
    "        # Feedforward and get the advantage values\n",
    "        adv = self.fc_adv(x)\n",
    "        \n",
    "        return val + (adv - adv.mean(dim = 1, keepdim = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 5.2. Model 2 - CNN and ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolutional model has a common feature extraction layer with the 1D convolution operations and two fully connected heads to output the value of the state and advantages for actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution and feedforward Network\n",
    "class DQNConv1D(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, shape, actions_n):\n",
    "        \n",
    "        # Inherite the parent's constructor\n",
    "        super(DQNConv1D, self).__init__()\n",
    "\n",
    "        # CNN layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(shape[0], 128, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 128, 5),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Output size of CNN\n",
    "        out_size = self._get_conv_out(shape)\n",
    "\n",
    "        # ANN layers for getting Q-Values\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        # ANN layers for getting advantage values\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions_n)\n",
    "        )\n",
    "\n",
    "        \n",
    "    # Function for getting the output size of CNN\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    \n",
    "    # Function for feedforward\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Feed data into CNN + Change the output shape\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        \n",
    "        # Feed CNN's output into FCL and get the Q-Values\n",
    "        val = self.fc_val(conv_out)\n",
    "        \n",
    "        # Feed CNN's output into FCL and get the advantage values\n",
    "        adv = self.fc_adv(conv_out)\n",
    "        \n",
    "        return val + (adv - adv.mean(dim = 1, keepdim = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep CNN + ANN Network\n",
    "class DQNConv1DLarge(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, shape, actions_n):\n",
    "        \n",
    "        # Inherite the parent's constructor\n",
    "        super(DQNConv1DLarge, self).__init__()\n",
    "\n",
    "        # CNN layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(shape[0], 32, 3),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.MaxPool1d(3, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Get the output size of CNN layer\n",
    "        out_size = self._get_conv_out(shape)\n",
    "\n",
    "        # ANN layers for getting Q-Values\n",
    "        self.fc_val = nn.Sequential(\n",
    "            nn.Linear(out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        # ANN layers for getting advantage values\n",
    "        self.fc_adv = nn.Sequential(\n",
    "            nn.Linear(out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions_n)\n",
    "        )\n",
    "\n",
    "        \n",
    "    # Function for getting the output size of CNN layer\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    \n",
    "    # Function for doing feedforward\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Feed data into CNN + Change the output shape\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        \n",
    "        # Feed CNN's output into FCL and get the Q-Values\n",
    "        val = self.fc_val(conv_out)\n",
    "        \n",
    "        # Feed CNN's output into FCL and get the advantage values\n",
    "        adv = self.fc_adv(conv_out)\n",
    "        \n",
    "        return val + (adv - adv.mean(dim=1, keepdim=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 06. Training code\n",
    "\n",
    "---\n",
    "\n",
    "We have two very similar training modules in this example: one for the feed-forward model and one for 1D convolutions. For both of them, there is nothing new added to our examples from Chapter 8, DQN Extensions:\n",
    "- They're using epsilon-greedy action selection to perform exploration. The epsilon linearly decays over the first 1M steps from 1.0 to 0.1.\n",
    "- A simple experience replay buffer of size 100k is being used, which is initially populated with 10k transitions.\n",
    "- For every 1,000 steps, we calculate the mean value for the fixed set of states to check the dynamics of the Q-values during the training.\n",
    "- For every 100k steps, we perform validation: 100 episodes are played on the training data and on previously unseen quotes. Characteristics of orders are recorded in TensorBoard, such as the mean profit, the mean count of bars, and the share held. This step allows us to check for overfitting conditions.\n",
    "\n",
    "The training modules are in Chapter10/train_model.py (feed-forward model) and Chapter10/train_model_conv.py (with a 1D convolutional layer). Both versions accept the same command-line options.\n",
    "\n",
    "To start the training, you need to pass training data with the --data option, which could be an individual CSV file or the whole directory with files. By default, the training module uses Yandex quotes for 2016 (file data/YNDX_160101_161231. csv). For the validation data, there is an option, --val, that takes Yandex 2015 quotes by default. Another required option will be -r, which is used to pass the name of the run. This name will be used in the TensorBoard run name and to create directories with saved models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 6.1. Training Feed-Forward Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import ptan\n",
    "import pathlib\n",
    "import gym.wrappers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
    "\n",
    "from lib import environ, data_loader, models, common, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for saving\n",
    "SAVES_DIR = pathlib.Path(\"saves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training stock data path\n",
    "STOCKS = \"data/YNDX_160101_161231.csv\"\n",
    "\n",
    "# Validation stock data path\n",
    "VAL_STOCKS = \"data/YNDX_150101_151231.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "BARS_COUNT = 10\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_FINAL = 0.1\n",
    "EPS_STEPS = 1000000\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "REPLAY_SIZE = 100000\n",
    "REPLAY_INITIAL = 10000\n",
    "REWARD_STEPS = 2\n",
    "LEARNING_RATE = 0.0001\n",
    "STATES_TO_EVALUATE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/YNDX_160101_161231.csv\n",
      "Read done, got 131542 rows, 99752 filtered, 0 open prices adjusted\n",
      "Reading data/YNDX_150101_151231.csv\n",
      "Read done, got 130566 rows, 104412 filtered, 0 open prices adjusted\n",
      "Episode 100: reward=-0, steps=9, speed=0.0 f/s, elapsed=0:00:07\n",
      "Episode 200: reward=0, steps=6, speed=0.0 f/s, elapsed=0:00:07\n",
      "Episode 300: reward=-0, steps=11, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 400: reward=0, steps=7, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 500: reward=-0, steps=5, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 600: reward=-2, steps=2, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 700: reward=-0, steps=5, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 800: reward=-0, steps=2, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 900: reward=-0, steps=8, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 1000: reward=-0, steps=13, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 1100: reward=-0, steps=2, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 1200: reward=0, steps=4, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 1300: reward=0, steps=7, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 1400: reward=-1, steps=9, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 1500: reward=-0, steps=5, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 1600: reward=-0, steps=2, speed=0.0 f/s, elapsed=0:00:08\n",
      "Episode 1700: reward=-1, steps=8, speed=70.6 f/s, elapsed=0:00:11\n",
      "Episode 1800: reward=0, steps=6, speed=70.6 f/s, elapsed=0:00:20\n",
      "Episode 1900: reward=-0, steps=6, speed=70.3 f/s, elapsed=0:00:31\n",
      "Episode 2000: reward=-0, steps=3, speed=70.0 f/s, elapsed=0:00:42\n",
      "Episode 2100: reward=-0, steps=4, speed=69.6 f/s, elapsed=0:00:52\n",
      "Episode 2200: reward=-0, steps=4, speed=69.4 f/s, elapsed=0:01:03\n",
      "Episode 2300: reward=-0, steps=5, speed=69.2 f/s, elapsed=0:01:14\n",
      "Episode 2400: reward=-0, steps=4, speed=69.0 f/s, elapsed=0:01:24\n",
      "Episode 2500: reward=-0, steps=4, speed=68.7 f/s, elapsed=0:01:35\n",
      "Episode 2600: reward=-0, steps=5, speed=68.5 f/s, elapsed=0:01:44\n",
      "Episode 2700: reward=-0, steps=13, speed=68.3 f/s, elapsed=0:01:56\n",
      "Episode 2800: reward=-0, steps=7, speed=68.0 f/s, elapsed=0:02:06\n",
      "Episode 2900: reward=1, steps=11, speed=67.6 f/s, elapsed=0:02:20\n",
      "Episode 3000: reward=-1, steps=6, speed=67.2 f/s, elapsed=0:02:33\n",
      "Episode 3100: reward=-0, steps=2, speed=67.0 f/s, elapsed=0:02:44\n",
      "Episode 3200: reward=-2, steps=16, speed=66.7 f/s, elapsed=0:02:58\n",
      "10000: tst: {'episode_reward': -0.16547308395453633, 'episode_steps': 138.12, 'order_profits': -0.1688812355822268, 'order_steps': 1.0}\n",
      "10000: val: {'episode_reward': -0.21769343843800862, 'episode_steps': 157.09, 'order_profits': -0.21768612624947498, 'order_steps': 1.01}\n",
      "Episode 3300: reward=-0, steps=2, speed=65.7 f/s, elapsed=0:03:31\n",
      "Episode 3400: reward=-0, steps=10, speed=65.4 f/s, elapsed=0:03:43\n",
      "Episode 3500: reward=-0, steps=5, speed=65.1 f/s, elapsed=0:03:55\n",
      "Episode 3600: reward=-0, steps=4, speed=64.9 f/s, elapsed=0:04:06\n",
      "Episode 3700: reward=-0, steps=5, speed=64.6 f/s, elapsed=0:04:18\n",
      "Episode 3800: reward=-0, steps=2, speed=64.4 f/s, elapsed=0:04:29\n",
      "Episode 3900: reward=-0, steps=8, speed=64.2 f/s, elapsed=0:04:40\n",
      "Episode 4000: reward=-0, steps=6, speed=64.0 f/s, elapsed=0:04:51\n",
      "Episode 4100: reward=-0, steps=3, speed=63.8 f/s, elapsed=0:05:03\n",
      "Episode 4200: reward=-0, steps=11, speed=63.5 f/s, elapsed=0:05:17\n",
      "Episode 4300: reward=-0, steps=6, speed=63.3 f/s, elapsed=0:05:28\n",
      "Episode 4400: reward=-0, steps=6, speed=63.0 f/s, elapsed=0:05:41\n",
      "Episode 4500: reward=-1, steps=5, speed=62.7 f/s, elapsed=0:05:53\n",
      "Episode 4600: reward=-0, steps=3, speed=62.5 f/s, elapsed=0:06:04\n",
      "Episode 4700: reward=0, steps=4, speed=62.3 f/s, elapsed=0:06:16\n",
      "Episode 4800: reward=0, steps=2, speed=62.0 f/s, elapsed=0:06:28\n",
      "Episode 4900: reward=-0, steps=10, speed=61.8 f/s, elapsed=0:06:40\n",
      "20000: tst: {'episode_reward': -0.08409072835672468, 'episode_steps': 126.54, 'order_profits': -0.09620158401310734, 'order_steps': 125.53}\n",
      "20000: val: {'episode_reward': 0.3681622108648519, 'episode_steps': 102.43, 'order_profits': 0.34935009827175356, 'order_steps': 101.43}\n",
      "Best validation reward updated: -0.218 -> 0.368, model saved\n",
      "Episode 5000: reward=-0, steps=2, speed=61.0 f/s, elapsed=0:07:09\n",
      "Episode 5100: reward=-0, steps=2, speed=60.8 f/s, elapsed=0:07:20\n",
      "Episode 5200: reward=-0, steps=3, speed=60.6 f/s, elapsed=0:07:32\n",
      "Episode 5300: reward=0, steps=5, speed=60.4 f/s, elapsed=0:07:43\n",
      "Episode 5400: reward=-0, steps=6, speed=60.3 f/s, elapsed=0:07:54\n",
      "Episode 5500: reward=-0, steps=8, speed=60.0 f/s, elapsed=0:08:09\n",
      "Episode 5600: reward=-0, steps=3, speed=59.6 f/s, elapsed=0:08:23\n",
      "Episode 5700: reward=-0, steps=4, speed=59.2 f/s, elapsed=0:08:38\n",
      "Episode 5800: reward=0, steps=4, speed=59.0 f/s, elapsed=0:08:51\n",
      "Episode 5900: reward=-0, steps=3, speed=58.8 f/s, elapsed=0:09:03\n",
      "Episode 6000: reward=-0, steps=4, speed=58.5 f/s, elapsed=0:09:17\n",
      "Episode 6100: reward=-0, steps=3, speed=58.3 f/s, elapsed=0:09:30\n",
      "Episode 6200: reward=0, steps=6, speed=58.2 f/s, elapsed=0:09:43\n",
      "Episode 6300: reward=0, steps=5, speed=58.1 f/s, elapsed=0:09:55\n",
      "Episode 6400: reward=-0, steps=4, speed=58.0 f/s, elapsed=0:10:07\n",
      "Episode 6500: reward=-0, steps=9, speed=57.9 f/s, elapsed=0:10:18\n",
      "30000: tst: {'episode_reward': 0.05911781736973104, 'episode_steps': 37.78, 'order_profits': 0.056927735822269544, 'order_steps': 15.44}\n",
      "30000: val: {'episode_reward': -0.24820922432473708, 'episode_steps': 28.88, 'order_profits': -0.2456840484077685, 'order_steps': 19.0}\n",
      "Episode 6600: reward=-0, steps=2, speed=57.5 f/s, elapsed=0:10:34\n",
      "Episode 6700: reward=-0, steps=12, speed=57.4 f/s, elapsed=0:10:45\n",
      "Episode 6800: reward=-0, steps=2, speed=57.3 f/s, elapsed=0:10:57\n",
      "Episode 6900: reward=0, steps=6, speed=57.2 f/s, elapsed=0:11:09\n",
      "Episode 7000: reward=0, steps=6, speed=57.1 f/s, elapsed=0:11:20\n",
      "Episode 7100: reward=0, steps=6, speed=57.1 f/s, elapsed=0:11:32\n",
      "Episode 7200: reward=0, steps=4, speed=57.0 f/s, elapsed=0:11:44\n",
      "Episode 7300: reward=-1, steps=3, speed=56.9 f/s, elapsed=0:11:56\n",
      "Episode 7400: reward=-0, steps=2, speed=56.8 f/s, elapsed=0:12:08\n",
      "Episode 7500: reward=-0, steps=4, speed=56.7 f/s, elapsed=0:12:19\n",
      "Episode 7600: reward=-0, steps=6, speed=56.6 f/s, elapsed=0:12:32\n",
      "Episode 7700: reward=-0, steps=8, speed=56.5 f/s, elapsed=0:12:43\n",
      "Episode 7800: reward=-0, steps=6, speed=56.4 f/s, elapsed=0:12:55\n",
      "Episode 7900: reward=-2, steps=6, speed=56.4 f/s, elapsed=0:13:07\n",
      "Episode 8000: reward=0, steps=4, speed=56.3 f/s, elapsed=0:13:18\n",
      "Episode 8100: reward=-0, steps=5, speed=56.2 f/s, elapsed=0:13:29\n",
      "40000: tst: {'episode_reward': -0.12332671704838033, 'episode_steps': 37.0, 'order_profits': -0.12405389563091539, 'order_steps': 9.47}\n",
      "40000: val: {'episode_reward': 0.031196047320786038, 'episode_steps': 23.86, 'order_profits': 0.029688311441259992, 'order_steps': 4.59}\n",
      "Episode 8200: reward=-0, steps=6, speed=55.9 f/s, elapsed=0:13:44\n",
      "Episode 8300: reward=-0, steps=5, speed=55.8 f/s, elapsed=0:13:55\n",
      "Episode 8400: reward=-1, steps=9, speed=55.7 f/s, elapsed=0:14:07\n",
      "Episode 8500: reward=0, steps=11, speed=55.7 f/s, elapsed=0:14:16\n",
      "Episode 8600: reward=-0, steps=13, speed=55.6 f/s, elapsed=0:14:29\n",
      "Episode 8700: reward=0, steps=8, speed=55.6 f/s, elapsed=0:14:41\n",
      "Episode 8800: reward=-0, steps=7, speed=55.4 f/s, elapsed=0:14:53\n",
      "Episode 8900: reward=-1, steps=9, speed=55.4 f/s, elapsed=0:15:04\n",
      "Episode 9000: reward=-0, steps=9, speed=55.3 f/s, elapsed=0:15:16\n",
      "Episode 9100: reward=-0, steps=2, speed=55.2 f/s, elapsed=0:15:28\n",
      "Episode 9200: reward=-1, steps=5, speed=55.2 f/s, elapsed=0:15:39\n",
      "Episode 9300: reward=1, steps=9, speed=55.1 f/s, elapsed=0:15:50\n",
      "Episode 9400: reward=1, steps=9, speed=55.1 f/s, elapsed=0:16:01\n",
      "Episode 9500: reward=-0, steps=4, speed=55.0 f/s, elapsed=0:16:12\n",
      "Episode 9600: reward=-0, steps=2, speed=55.0 f/s, elapsed=0:16:24\n",
      "Episode 9700: reward=-0, steps=4, speed=54.9 f/s, elapsed=0:16:35\n",
      "Episode 9800: reward=-1, steps=6, speed=54.8 f/s, elapsed=0:16:47\n",
      "50000: tst: {'episode_reward': -0.15454629197867212, 'episode_steps': 7.77, 'order_profits': -0.154533555657432, 'order_steps': 5.63}\n",
      "50000: val: {'episode_reward': -0.033915758570245126, 'episode_steps': 9.14, 'order_profits': -0.036758199946641326, 'order_steps': 6.96}\n",
      "Episode 9900: reward=-0, steps=4, speed=54.7 f/s, elapsed=0:17:01\n",
      "Episode 10000: reward=-0, steps=5, speed=54.6 f/s, elapsed=0:17:12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10100: reward=-0, steps=4, speed=54.6 f/s, elapsed=0:17:24\n",
      "Episode 10200: reward=-1, steps=3, speed=54.5 f/s, elapsed=0:17:35\n",
      "Episode 10300: reward=-0, steps=2, speed=54.4 f/s, elapsed=0:17:47\n",
      "Episode 10400: reward=-0, steps=6, speed=54.4 f/s, elapsed=0:18:00\n",
      "Episode 10500: reward=-0, steps=7, speed=54.3 f/s, elapsed=0:18:12\n",
      "Episode 10600: reward=-1, steps=4, speed=54.2 f/s, elapsed=0:18:25\n",
      "Episode 10700: reward=-0, steps=4, speed=54.2 f/s, elapsed=0:18:37\n",
      "Episode 10800: reward=0, steps=8, speed=54.1 f/s, elapsed=0:18:50\n",
      "Episode 10900: reward=-1, steps=4, speed=54.0 f/s, elapsed=0:19:03\n",
      "Episode 11000: reward=-1, steps=5, speed=53.9 f/s, elapsed=0:19:14\n",
      "Episode 11100: reward=1, steps=15, speed=53.9 f/s, elapsed=0:19:25\n",
      "Episode 11200: reward=-0, steps=3, speed=53.8 f/s, elapsed=0:19:37\n",
      "Episode 11300: reward=-0, steps=4, speed=53.7 f/s, elapsed=0:19:50\n",
      "Episode 11400: reward=-0, steps=3, speed=53.6 f/s, elapsed=0:20:03\n",
      "60000: tst: {'episode_reward': -0.11823248812868149, 'episode_steps': 8.97, 'order_profits': -0.11877052125292499, 'order_steps': 4.77}\n",
      "60000: val: {'episode_reward': -0.06996190128121697, 'episode_steps': 6.47, 'order_profits': -0.07053405342954751, 'order_steps': 2.5}\n",
      "Episode 11500: reward=0, steps=2, speed=53.5 f/s, elapsed=0:20:16\n",
      "Episode 11600: reward=1, steps=6, speed=53.4 f/s, elapsed=0:20:28\n",
      "Episode 11700: reward=1, steps=5, speed=53.3 f/s, elapsed=0:20:39\n",
      "Episode 11800: reward=0, steps=10, speed=53.2 f/s, elapsed=0:20:51\n",
      "Episode 11900: reward=-0, steps=10, speed=53.1 f/s, elapsed=0:21:03\n",
      "Episode 12000: reward=-0, steps=3, speed=53.0 f/s, elapsed=0:21:15\n",
      "Episode 12100: reward=0, steps=4, speed=52.9 f/s, elapsed=0:21:27\n",
      "Episode 12200: reward=-0, steps=3, speed=52.8 f/s, elapsed=0:21:40\n",
      "Episode 12300: reward=-0, steps=2, speed=52.7 f/s, elapsed=0:21:53\n",
      "Episode 12400: reward=-1, steps=4, speed=52.6 f/s, elapsed=0:22:06\n",
      "Episode 12500: reward=-0, steps=4, speed=52.5 f/s, elapsed=0:22:19\n",
      "Episode 12600: reward=-0, steps=9, speed=52.4 f/s, elapsed=0:22:32\n",
      "Episode 12700: reward=-0, steps=3, speed=52.2 f/s, elapsed=0:22:44\n",
      "Episode 12800: reward=-1, steps=6, speed=52.1 f/s, elapsed=0:22:58\n",
      "Episode 12900: reward=-0, steps=5, speed=52.0 f/s, elapsed=0:23:11\n",
      "Episode 13000: reward=0, steps=6, speed=51.8 f/s, elapsed=0:23:25\n",
      "Episode 13100: reward=-0, steps=12, speed=51.7 f/s, elapsed=0:23:39\n",
      "70000: tst: {'episode_reward': -0.010814431930668477, 'episode_steps': 27.63, 'order_profits': -0.011129458297033901, 'order_steps': 3.61}\n",
      "70000: val: {'episode_reward': -0.023972414797457865, 'episode_steps': 19.61, 'order_profits': -0.02500047516890228, 'order_steps': 4.4}\n",
      "Episode 13200: reward=-0, steps=6, speed=51.4 f/s, elapsed=0:23:56\n",
      "Episode 13300: reward=0, steps=5, speed=51.2 f/s, elapsed=0:24:11\n",
      "Episode 13400: reward=-1, steps=7, speed=51.1 f/s, elapsed=0:24:25\n",
      "Episode 13500: reward=-0, steps=3, speed=50.9 f/s, elapsed=0:24:37\n",
      "Episode 13600: reward=0, steps=5, speed=50.8 f/s, elapsed=0:24:53\n",
      "Episode 13700: reward=-1, steps=12, speed=50.6 f/s, elapsed=0:25:05\n",
      "Episode 13800: reward=-0, steps=10, speed=50.5 f/s, elapsed=0:25:20\n",
      "Episode 13900: reward=0, steps=7, speed=50.3 f/s, elapsed=0:25:35\n",
      "Episode 14000: reward=-1, steps=6, speed=50.2 f/s, elapsed=0:25:48\n",
      "Episode 14100: reward=-0, steps=12, speed=50.0 f/s, elapsed=0:26:03\n",
      "Episode 14200: reward=0, steps=4, speed=49.9 f/s, elapsed=0:26:18\n",
      "Episode 14300: reward=-0, steps=7, speed=49.7 f/s, elapsed=0:26:31\n",
      "Episode 14400: reward=0, steps=3, speed=49.6 f/s, elapsed=0:26:45\n",
      "Episode 14500: reward=-0, steps=3, speed=49.4 f/s, elapsed=0:27:01\n",
      "Episode 14600: reward=0, steps=15, speed=49.2 f/s, elapsed=0:27:18\n",
      "Episode 14700: reward=-0, steps=6, speed=49.0 f/s, elapsed=0:27:34\n",
      "80000: tst: {'episode_reward': -0.07975705276532401, 'episode_steps': 8.45, 'order_profits': -0.08007413252329072, 'order_steps': 3.47}\n",
      "80000: val: {'episode_reward': -0.028608466374638177, 'episode_steps': 8.44, 'order_profits': -0.029275131170885666, 'order_steps': 4.92}\n",
      "Episode 14800: reward=-0, steps=5, speed=48.8 f/s, elapsed=0:27:50\n",
      "Episode 14900: reward=-1, steps=8, speed=48.6 f/s, elapsed=0:28:05\n",
      "Episode 15000: reward=-0, steps=5, speed=48.4 f/s, elapsed=0:28:19\n",
      "Episode 15100: reward=-0, steps=11, speed=48.3 f/s, elapsed=0:28:34\n",
      "Episode 15200: reward=-0, steps=3, speed=48.2 f/s, elapsed=0:28:48\n",
      "Episode 15300: reward=-0, steps=2, speed=48.0 f/s, elapsed=0:29:02\n",
      "Episode 15400: reward=0, steps=5, speed=47.9 f/s, elapsed=0:29:16\n",
      "Episode 15500: reward=-2, steps=12, speed=47.8 f/s, elapsed=0:29:32\n",
      "Episode 15600: reward=-1, steps=3, speed=47.4 f/s, elapsed=0:29:53\n",
      "Episode 15700: reward=-1, steps=9, speed=47.1 f/s, elapsed=0:30:12\n",
      "Episode 15800: reward=0, steps=11, speed=46.9 f/s, elapsed=0:30:29\n",
      "Episode 15900: reward=0, steps=4, speed=46.7 f/s, elapsed=0:30:45\n",
      "Episode 16000: reward=-0, steps=4, speed=46.5 f/s, elapsed=0:31:02\n",
      "Episode 16100: reward=0, steps=6, speed=46.3 f/s, elapsed=0:31:18\n",
      "Episode 16200: reward=-0, steps=3, speed=46.1 f/s, elapsed=0:31:35\n",
      "Episode 16300: reward=1, steps=9, speed=45.9 f/s, elapsed=0:31:52\n",
      "Episode 16400: reward=-0, steps=2, speed=45.7 f/s, elapsed=0:32:07\n",
      "90000: tst: {'episode_reward': -0.09581397744515269, 'episode_steps': 16.16, 'order_profits': -0.09642948379301915, 'order_steps': 5.21}\n",
      "90000: val: {'episode_reward': 0.029065578263668134, 'episode_steps': 13.85, 'order_profits': 0.027768090733152943, 'order_steps': 3.6}\n",
      "Episode 16500: reward=-0, steps=7, speed=45.5 f/s, elapsed=0:32:28\n",
      "Episode 16600: reward=0, steps=3, speed=45.3 f/s, elapsed=0:32:44\n",
      "Episode 16700: reward=-0, steps=9, speed=45.2 f/s, elapsed=0:33:01\n",
      "Episode 16800: reward=-0, steps=4, speed=45.1 f/s, elapsed=0:33:16\n",
      "Episode 16900: reward=0, steps=5, speed=45.0 f/s, elapsed=0:33:32\n",
      "Episode 17000: reward=-1, steps=13, speed=44.8 f/s, elapsed=0:33:48\n",
      "Episode 17100: reward=-0, steps=2, speed=44.7 f/s, elapsed=0:34:04\n",
      "Episode 17200: reward=-0, steps=4, speed=44.5 f/s, elapsed=0:34:21\n",
      "Episode 17300: reward=-0, steps=8, speed=44.4 f/s, elapsed=0:34:39\n",
      "Episode 17400: reward=-0, steps=4, speed=44.2 f/s, elapsed=0:34:55\n",
      "Episode 17500: reward=-0, steps=2, speed=44.0 f/s, elapsed=0:35:09\n",
      "Episode 17600: reward=-0, steps=5, speed=43.9 f/s, elapsed=0:35:25\n",
      "Episode 17700: reward=0, steps=8, speed=43.8 f/s, elapsed=0:35:40\n",
      "Episode 17800: reward=-0, steps=2, speed=43.6 f/s, elapsed=0:35:57\n",
      "Episode 17900: reward=-0, steps=5, speed=43.6 f/s, elapsed=0:36:13\n",
      "Episode 18000: reward=1, steps=9, speed=43.5 f/s, elapsed=0:36:28\n",
      "100000: tst: {'episode_reward': -0.08914433491479104, 'episode_steps': 7.33, 'order_profits': -0.08988653571388992, 'order_steps': 4.15}\n",
      "100000: val: {'episode_reward': -0.09015003755090714, 'episode_steps': 7.51, 'order_profits': -0.09089642418520935, 'order_steps': 4.63}\n",
      "Episode 18100: reward=-0, steps=3, speed=43.3 f/s, elapsed=0:36:47\n",
      "Episode 18200: reward=1, steps=8, speed=43.2 f/s, elapsed=0:37:02\n",
      "Episode 18300: reward=-0, steps=7, speed=43.1 f/s, elapsed=0:37:20\n",
      "Episode 18400: reward=-0, steps=3, speed=42.9 f/s, elapsed=0:37:36\n",
      "Episode 18500: reward=-0, steps=9, speed=42.9 f/s, elapsed=0:37:53\n",
      "Episode 18600: reward=0, steps=7, speed=42.7 f/s, elapsed=0:38:10\n",
      "Episode 18700: reward=0, steps=6, speed=42.6 f/s, elapsed=0:38:28\n",
      "Episode 18800: reward=-0, steps=3, speed=42.5 f/s, elapsed=0:38:43\n",
      "Episode 18900: reward=-0, steps=4, speed=42.5 f/s, elapsed=0:38:58\n",
      "Episode 19000: reward=0, steps=4, speed=42.3 f/s, elapsed=0:39:15\n",
      "Episode 19100: reward=-0, steps=2, speed=42.2 f/s, elapsed=0:39:30\n",
      "Episode 19200: reward=-0, steps=9, speed=42.0 f/s, elapsed=0:39:52\n",
      "Episode 19300: reward=-0, steps=7, speed=41.9 f/s, elapsed=0:40:09\n",
      "Episode 19400: reward=1, steps=5, speed=41.8 f/s, elapsed=0:40:25\n",
      "Episode 19500: reward=-0, steps=8, speed=41.7 f/s, elapsed=0:40:42\n",
      "Episode 19600: reward=-0, steps=5, speed=41.6 f/s, elapsed=0:40:57\n",
      "Episode 19700: reward=-0, steps=5, speed=41.5 f/s, elapsed=0:41:10\n",
      "110000: tst: {'episode_reward': -0.11100929958061627, 'episode_steps': 10.31, 'order_profits': -0.11119729098029812, 'order_steps': 2.99}\n",
      "110000: val: {'episode_reward': -0.06689806000417892, 'episode_steps': 8.8, 'order_profits': -0.06753698817538784, 'order_steps': 3.28}\n",
      "Episode 19800: reward=-1, steps=14, speed=41.4 f/s, elapsed=0:41:27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19900: reward=0, steps=8, speed=41.3 f/s, elapsed=0:41:45\n",
      "Episode 20000: reward=-0, steps=2, speed=41.1 f/s, elapsed=0:42:01\n",
      "Episode 20100: reward=0, steps=9, speed=41.0 f/s, elapsed=0:42:18\n",
      "Episode 20200: reward=-0, steps=3, speed=41.0 f/s, elapsed=0:42:33\n",
      "Episode 20300: reward=-0, steps=3, speed=40.8 f/s, elapsed=0:42:51\n",
      "Episode 20400: reward=-0, steps=2, speed=40.7 f/s, elapsed=0:43:08\n",
      "Episode 20500: reward=-0, steps=8, speed=40.7 f/s, elapsed=0:43:26\n",
      "Episode 20600: reward=-0, steps=4, speed=40.6 f/s, elapsed=0:43:41\n",
      "Episode 20700: reward=0, steps=10, speed=40.6 f/s, elapsed=0:43:56\n",
      "Episode 20800: reward=-0, steps=5, speed=40.5 f/s, elapsed=0:44:14\n",
      "Episode 20900: reward=-0, steps=5, speed=40.3 f/s, elapsed=0:44:32\n",
      "Episode 21000: reward=-0, steps=6, speed=40.3 f/s, elapsed=0:44:48\n",
      "Episode 21100: reward=-0, steps=8, speed=40.2 f/s, elapsed=0:45:04\n",
      "Episode 21200: reward=-0, steps=6, speed=40.2 f/s, elapsed=0:45:21\n",
      "Episode 21300: reward=-1, steps=3, speed=40.1 f/s, elapsed=0:45:36\n",
      "120000: tst: {'episode_reward': -0.08852919032661583, 'episode_steps': 12.53, 'order_profits': -0.08928055591996222, 'order_steps': 9.18}\n",
      "120000: val: {'episode_reward': -0.06752220685243877, 'episode_steps': 10.18, 'order_profits': -0.06893622554420477, 'order_steps': 7.0}\n",
      "Episode 21400: reward=-0, steps=2, speed=40.0 f/s, elapsed=0:45:55\n",
      "Episode 21500: reward=-0, steps=3, speed=39.9 f/s, elapsed=0:46:13\n",
      "Episode 21600: reward=-0, steps=4, speed=39.8 f/s, elapsed=0:46:29\n",
      "Episode 21700: reward=1, steps=6, speed=39.8 f/s, elapsed=0:46:46\n",
      "Episode 21800: reward=0, steps=5, speed=39.8 f/s, elapsed=0:47:01\n",
      "Episode 21900: reward=-0, steps=5, speed=39.7 f/s, elapsed=0:47:19\n",
      "Episode 22000: reward=-0, steps=6, speed=39.7 f/s, elapsed=0:47:36\n",
      "Episode 22100: reward=-0, steps=7, speed=39.5 f/s, elapsed=0:47:55\n",
      "Episode 22200: reward=-0, steps=4, speed=39.4 f/s, elapsed=0:48:14\n",
      "Episode 22300: reward=-0, steps=4, speed=39.3 f/s, elapsed=0:48:31\n",
      "Episode 22400: reward=-1, steps=4, speed=39.2 f/s, elapsed=0:48:47\n",
      "Episode 22500: reward=-0, steps=4, speed=39.1 f/s, elapsed=0:49:03\n",
      "Episode 22600: reward=-0, steps=10, speed=39.1 f/s, elapsed=0:49:18\n",
      "Episode 22700: reward=-1, steps=8, speed=39.0 f/s, elapsed=0:49:36\n",
      "Episode 22800: reward=-0, steps=13, speed=39.1 f/s, elapsed=0:49:54\n",
      "Episode 22900: reward=-0, steps=7, speed=39.1 f/s, elapsed=0:50:10\n",
      "130000: tst: {'episode_reward': -0.16281207550953306, 'episode_steps': 11.35, 'order_profits': -0.1628319492768478, 'order_steps': 4.93}\n",
      "130000: val: {'episode_reward': -0.004687113895298127, 'episode_steps': 7.27, 'order_profits': -0.005299138532159796, 'order_steps': 2.95}\n",
      "Episode 23000: reward=-0, steps=4, speed=39.0 f/s, elapsed=0:50:27\n",
      "Episode 23100: reward=0, steps=3, speed=39.0 f/s, elapsed=0:50:44\n",
      "Episode 23200: reward=-0, steps=11, speed=39.0 f/s, elapsed=0:51:01\n",
      "Episode 23300: reward=0, steps=6, speed=39.0 f/s, elapsed=0:51:16\n",
      "Episode 23400: reward=-0, steps=7, speed=39.0 f/s, elapsed=0:51:31\n",
      "Episode 23500: reward=-1, steps=15, speed=38.9 f/s, elapsed=0:51:52\n",
      "Episode 23600: reward=-0, steps=3, speed=38.9 f/s, elapsed=0:52:08\n",
      "Episode 23700: reward=0, steps=16, speed=38.8 f/s, elapsed=0:52:24\n",
      "Episode 23800: reward=-1, steps=5, speed=38.8 f/s, elapsed=0:52:41\n",
      "Episode 23900: reward=-0, steps=3, speed=38.7 f/s, elapsed=0:52:59\n",
      "Episode 24000: reward=0, steps=4, speed=38.6 f/s, elapsed=0:53:19\n",
      "Episode 24100: reward=-2, steps=14, speed=38.6 f/s, elapsed=0:53:37\n",
      "Episode 24200: reward=-1, steps=12, speed=38.5 f/s, elapsed=0:53:56\n",
      "Episode 24300: reward=-0, steps=3, speed=38.5 f/s, elapsed=0:54:11\n",
      "Episode 24400: reward=-0, steps=2, speed=38.3 f/s, elapsed=0:54:33\n",
      "Episode 24500: reward=-0, steps=3, speed=38.2 f/s, elapsed=0:54:50\n",
      "140000: tst: {'episode_reward': -0.07815026947212504, 'episode_steps': 12.63, 'order_profits': -0.07859917869112, 'order_steps': 2.92}\n",
      "140000: val: {'episode_reward': -0.1979300404527264, 'episode_steps': 17.34, 'order_profits': -0.1978306844203845, 'order_steps': 10.18}\n",
      "Episode 24600: reward=2, steps=8, speed=38.1 f/s, elapsed=0:55:09\n",
      "Episode 24700: reward=-1, steps=10, speed=38.1 f/s, elapsed=0:55:25\n",
      "Episode 24800: reward=-0, steps=3, speed=38.1 f/s, elapsed=0:55:42\n",
      "Episode 24900: reward=-0, steps=11, speed=38.1 f/s, elapsed=0:56:01\n",
      "Episode 25000: reward=0, steps=3, speed=38.0 f/s, elapsed=0:56:19\n",
      "Episode 25100: reward=-0, steps=4, speed=37.9 f/s, elapsed=0:56:38\n",
      "Episode 25200: reward=-0, steps=2, speed=37.9 f/s, elapsed=0:56:55\n",
      "Episode 25300: reward=-0, steps=7, speed=37.8 f/s, elapsed=0:57:11\n",
      "Episode 25400: reward=-0, steps=3, speed=37.8 f/s, elapsed=0:57:27\n",
      "Episode 25500: reward=-1, steps=4, speed=37.7 f/s, elapsed=0:57:46\n",
      "Episode 25600: reward=1, steps=7, speed=37.7 f/s, elapsed=0:58:02\n",
      "Episode 25700: reward=-0, steps=3, speed=37.7 f/s, elapsed=0:58:18\n",
      "Episode 25800: reward=0, steps=4, speed=37.7 f/s, elapsed=0:58:35\n",
      "Episode 25900: reward=-1, steps=3, speed=37.7 f/s, elapsed=0:58:52\n",
      "Episode 26000: reward=-0, steps=5, speed=37.6 f/s, elapsed=0:59:09\n",
      "Episode 26100: reward=0, steps=3, speed=37.6 f/s, elapsed=0:59:27\n",
      "Episode 26200: reward=1, steps=7, speed=37.5 f/s, elapsed=0:59:46\n",
      "150000: tst: {'episode_reward': -0.04999391654325572, 'episode_steps': 13.48, 'order_profits': -0.05082755129849259, 'order_steps': 5.44}\n",
      "150000: val: {'episode_reward': 0.028955936099152438, 'episode_steps': 7.88, 'order_profits': 0.02769191681579898, 'order_steps': 3.56}\n",
      "Episode 26300: reward=-1, steps=18, speed=37.4 f/s, elapsed=1:00:09\n",
      "Episode 26400: reward=-0, steps=2, speed=37.3 f/s, elapsed=1:00:28\n",
      "Episode 26500: reward=-0, steps=16, speed=37.2 f/s, elapsed=1:00:47\n",
      "Episode 26600: reward=-1, steps=7, speed=37.2 f/s, elapsed=1:01:03\n",
      "Episode 26700: reward=-0, steps=3, speed=37.2 f/s, elapsed=1:01:19\n",
      "Episode 26800: reward=0, steps=4, speed=37.3 f/s, elapsed=1:01:35\n",
      "Episode 26900: reward=-0, steps=6, speed=37.4 f/s, elapsed=1:01:50\n",
      "Episode 27000: reward=-0, steps=7, speed=37.4 f/s, elapsed=1:02:06\n",
      "Episode 27100: reward=-0, steps=5, speed=37.5 f/s, elapsed=1:02:23\n",
      "Episode 27200: reward=-0, steps=5, speed=37.5 f/s, elapsed=1:02:41\n",
      "Episode 27300: reward=0, steps=7, speed=37.6 f/s, elapsed=1:02:56\n",
      "Episode 27400: reward=-0, steps=3, speed=37.6 f/s, elapsed=1:03:11\n",
      "Episode 27500: reward=0, steps=4, speed=37.6 f/s, elapsed=1:03:27\n",
      "Episode 27600: reward=0, steps=5, speed=37.7 f/s, elapsed=1:03:43\n",
      "Episode 27700: reward=-0, steps=5, speed=37.7 f/s, elapsed=1:03:57\n",
      "160000: tst: {'episode_reward': -0.04628487104499238, 'episode_steps': 17.69, 'order_profits': -0.0465913661948866, 'order_steps': 6.7}\n",
      "160000: val: {'episode_reward': 0.0037411558905943994, 'episode_steps': 13.67, 'order_profits': 0.0026276329544762406, 'order_steps': 5.23}\n",
      "Episode 27800: reward=-0, steps=17, speed=37.7 f/s, elapsed=1:04:17\n",
      "Episode 27900: reward=-0, steps=7, speed=37.7 f/s, elapsed=1:04:32\n",
      "Episode 28000: reward=-0, steps=3, speed=37.8 f/s, elapsed=1:04:47\n",
      "Episode 28100: reward=-0, steps=4, speed=37.8 f/s, elapsed=1:05:03\n",
      "Episode 28200: reward=-0, steps=2, speed=37.9 f/s, elapsed=1:05:21\n",
      "Episode 28300: reward=-0, steps=6, speed=37.9 f/s, elapsed=1:05:38\n",
      "Episode 28400: reward=-0, steps=7, speed=37.9 f/s, elapsed=1:05:51\n",
      "Episode 28500: reward=-0, steps=10, speed=38.0 f/s, elapsed=1:06:09\n",
      "Episode 28600: reward=-1, steps=5, speed=38.0 f/s, elapsed=1:06:25\n",
      "Episode 28700: reward=-1, steps=9, speed=38.0 f/s, elapsed=1:06:43\n",
      "Episode 28800: reward=-1, steps=12, speed=38.1 f/s, elapsed=1:06:58\n",
      "Episode 28900: reward=-0, steps=10, speed=38.1 f/s, elapsed=1:07:14\n",
      "Episode 29000: reward=-1, steps=12, speed=38.1 f/s, elapsed=1:07:28\n",
      "Episode 29100: reward=-1, steps=9, speed=38.1 f/s, elapsed=1:07:45\n",
      "Episode 29200: reward=-0, steps=2, speed=38.1 f/s, elapsed=1:08:02\n",
      "Episode 29300: reward=-0, steps=5, speed=38.1 f/s, elapsed=1:08:16\n",
      "170000: tst: {'episode_reward': -0.10828305086741309, 'episode_steps': 5.19, 'order_profits': -0.10860777311236287, 'order_steps': 3.24}\n",
      "170000: val: {'episode_reward': -0.12441948431891832, 'episode_steps': 6.32, 'order_profits': -0.1251549944014811, 'order_steps': 3.97}\n",
      "Episode 29400: reward=0, steps=7, speed=38.1 f/s, elapsed=1:08:30\n",
      "Episode 29500: reward=-0, steps=2, speed=38.2 f/s, elapsed=1:08:46\n",
      "Episode 29600: reward=-0, steps=4, speed=38.1 f/s, elapsed=1:09:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29700: reward=0, steps=18, speed=38.1 f/s, elapsed=1:09:21\n",
      "Episode 29800: reward=0, steps=3, speed=38.1 f/s, elapsed=1:09:38\n",
      "Episode 29900: reward=-0, steps=3, speed=38.0 f/s, elapsed=1:09:56\n",
      "Episode 30000: reward=-0, steps=4, speed=37.9 f/s, elapsed=1:10:16\n",
      "Episode 30100: reward=-0, steps=2, speed=37.9 f/s, elapsed=1:10:32\n",
      "Episode 30200: reward=-7, steps=10, speed=37.9 f/s, elapsed=1:10:49\n",
      "Episode 30300: reward=-0, steps=4, speed=37.8 f/s, elapsed=1:11:05\n",
      "Episode 30400: reward=-0, steps=3, speed=37.8 f/s, elapsed=1:11:23\n",
      "Episode 30500: reward=-0, steps=3, speed=37.8 f/s, elapsed=1:11:39\n",
      "Episode 30600: reward=-0, steps=4, speed=37.8 f/s, elapsed=1:11:53\n",
      "Episode 30700: reward=-0, steps=4, speed=37.8 f/s, elapsed=1:12:10\n",
      "Episode 30800: reward=-1, steps=2, speed=37.7 f/s, elapsed=1:12:31\n",
      "Episode 30900: reward=1, steps=7, speed=37.7 f/s, elapsed=1:12:48\n",
      "180000: tst: {'episode_reward': -0.009242630591963655, 'episode_steps': 15.82, 'order_profits': -0.009493421970382707, 'order_steps': 4.97}\n",
      "180000: val: {'episode_reward': -0.009251945258348408, 'episode_steps': 12.32, 'order_profits': -0.008998538069578444, 'order_steps': 3.26}\n",
      "Episode 31000: reward=-0, steps=12, speed=37.6 f/s, elapsed=1:13:12\n",
      "Episode 31100: reward=-0, steps=4, speed=37.6 f/s, elapsed=1:13:29\n",
      "Episode 31200: reward=-1, steps=21, speed=37.6 f/s, elapsed=1:13:49\n",
      "Episode 31300: reward=-0, steps=7, speed=37.5 f/s, elapsed=1:14:08\n",
      "Episode 31400: reward=-0, steps=6, speed=37.5 f/s, elapsed=1:14:27\n",
      "Episode 31500: reward=-2, steps=28, speed=37.4 f/s, elapsed=1:14:45\n",
      "Episode 31600: reward=-0, steps=10, speed=37.3 f/s, elapsed=1:15:05\n",
      "Episode 31700: reward=-1, steps=13, speed=37.3 f/s, elapsed=1:15:20\n",
      "Episode 31800: reward=0, steps=6, speed=37.3 f/s, elapsed=1:15:38\n",
      "Episode 31900: reward=0, steps=5, speed=37.3 f/s, elapsed=1:15:53\n",
      "Episode 32000: reward=-0, steps=9, speed=37.3 f/s, elapsed=1:16:10\n",
      "Episode 32100: reward=0, steps=8, speed=37.3 f/s, elapsed=1:16:25\n",
      "Episode 32200: reward=-0, steps=10, speed=37.4 f/s, elapsed=1:16:41\n",
      "Episode 32300: reward=-0, steps=5, speed=37.4 f/s, elapsed=1:16:56\n",
      "Episode 32400: reward=-0, steps=6, speed=37.5 f/s, elapsed=1:17:09\n",
      "Episode 32500: reward=-0, steps=9, speed=37.5 f/s, elapsed=1:17:25\n",
      "190000: tst: {'episode_reward': -0.1877338481624951, 'episode_steps': 15.53, 'order_profits': -0.18887450134925884, 'order_steps': 11.21}\n",
      "190000: val: {'episode_reward': -0.09316568593412812, 'episode_steps': 10.39, 'order_profits': -0.09497437639140233, 'order_steps': 7.04}\n",
      "Episode 32600: reward=-0, steps=8, speed=37.4 f/s, elapsed=1:17:43\n",
      "Episode 32700: reward=0, steps=4, speed=37.5 f/s, elapsed=1:17:58\n",
      "Episode 32800: reward=-1, steps=8, speed=37.5 f/s, elapsed=1:18:14\n",
      "Episode 32900: reward=-0, steps=6, speed=37.6 f/s, elapsed=1:18:34\n",
      "Episode 33000: reward=0, steps=5, speed=37.6 f/s, elapsed=1:18:52\n",
      "Episode 33100: reward=-1, steps=12, speed=37.6 f/s, elapsed=1:19:09\n",
      "Episode 33200: reward=-0, steps=3, speed=37.7 f/s, elapsed=1:19:26\n",
      "Episode 33300: reward=-0, steps=9, speed=37.7 f/s, elapsed=1:19:44\n",
      "Episode 33400: reward=-0, steps=3, speed=37.7 f/s, elapsed=1:20:00\n",
      "Episode 33500: reward=-1, steps=12, speed=37.8 f/s, elapsed=1:20:17\n",
      "Episode 33600: reward=-0, steps=3, speed=37.8 f/s, elapsed=1:20:34\n",
      "Episode 33700: reward=-0, steps=4, speed=37.8 f/s, elapsed=1:20:51\n",
      "Episode 33800: reward=-1, steps=7, speed=37.9 f/s, elapsed=1:21:06\n",
      "Episode 33900: reward=-0, steps=4, speed=37.9 f/s, elapsed=1:21:23\n",
      "Episode 34000: reward=-1, steps=11, speed=37.9 f/s, elapsed=1:21:41\n",
      "200000: tst: {'episode_reward': -0.02275247813128106, 'episode_steps': 38.61, 'order_profits': -0.026769544710963072, 'order_steps': 25.83}\n",
      "200000: val: {'episode_reward': -0.05632603873491791, 'episode_steps': 21.87, 'order_profits': -0.05832773198144548, 'order_steps': 12.12}\n",
      "Episode 34100: reward=-0, steps=4, speed=37.8 f/s, elapsed=1:22:03\n",
      "Episode 34200: reward=0, steps=6, speed=37.8 f/s, elapsed=1:22:20\n",
      "Episode 34300: reward=-0, steps=9, speed=37.9 f/s, elapsed=1:22:35\n",
      "Episode 34400: reward=-0, steps=4, speed=37.9 f/s, elapsed=1:22:53\n",
      "Episode 34500: reward=-0, steps=13, speed=37.9 f/s, elapsed=1:23:10\n",
      "Episode 34600: reward=0, steps=4, speed=38.0 f/s, elapsed=1:23:28\n",
      "Episode 34700: reward=-1, steps=7, speed=38.0 f/s, elapsed=1:23:44\n",
      "Episode 34800: reward=-1, steps=5, speed=38.0 f/s, elapsed=1:24:02\n",
      "Episode 34900: reward=0, steps=5, speed=38.1 f/s, elapsed=1:24:20\n",
      "Episode 35000: reward=-0, steps=7, speed=38.1 f/s, elapsed=1:24:38\n",
      "Episode 35100: reward=-0, steps=5, speed=38.1 f/s, elapsed=1:24:57\n",
      "Episode 35200: reward=-1, steps=7, speed=38.1 f/s, elapsed=1:25:13\n",
      "Episode 35300: reward=-0, steps=16, speed=38.2 f/s, elapsed=1:25:30\n",
      "Episode 35400: reward=-1, steps=6, speed=38.2 f/s, elapsed=1:25:46\n",
      "Episode 35500: reward=-0, steps=8, speed=38.2 f/s, elapsed=1:26:04\n",
      "210000: tst: {'episode_reward': 0.016133182869549522, 'episode_steps': 15.57, 'order_profits': 0.014449963925617142, 'order_steps': 8.66}\n",
      "210000: val: {'episode_reward': -0.05402492954758432, 'episode_steps': 10.86, 'order_profits': -0.05491920109644195, 'order_steps': 5.42}\n",
      "Episode 35600: reward=-0, steps=10, speed=38.0 f/s, elapsed=1:43:35\n",
      "Episode 35700: reward=-0, steps=9, speed=37.9 f/s, elapsed=1:43:54\n",
      "Episode 35800: reward=-0, steps=7, speed=37.9 f/s, elapsed=1:44:12\n",
      "Episode 35900: reward=-0, steps=4, speed=37.6 f/s, elapsed=1:44:38\n",
      "Episode 36000: reward=-0, steps=12, speed=37.6 f/s, elapsed=1:44:57\n",
      "Episode 36100: reward=-1, steps=4, speed=37.6 f/s, elapsed=1:45:13\n",
      "Episode 36200: reward=-0, steps=6, speed=37.5 f/s, elapsed=1:45:30\n",
      "Episode 36300: reward=0, steps=6, speed=37.5 f/s, elapsed=1:45:51\n",
      "Episode 36400: reward=-0, steps=7, speed=37.5 f/s, elapsed=1:46:08\n",
      "Episode 36500: reward=-2, steps=15, speed=37.5 f/s, elapsed=1:46:29\n",
      "Episode 36600: reward=-1, steps=15, speed=37.5 f/s, elapsed=1:46:49\n",
      "Episode 36700: reward=-0, steps=4, speed=37.4 f/s, elapsed=1:47:12\n",
      "Episode 36800: reward=-0, steps=5, speed=37.3 f/s, elapsed=1:47:32\n",
      "Episode 36900: reward=-0, steps=5, speed=37.3 f/s, elapsed=1:47:50\n",
      "220000: tst: {'episode_reward': -0.012539229987540888, 'episode_steps': 18.47, 'order_profits': -0.014586931963408512, 'order_steps': 10.91}\n",
      "220000: val: {'episode_reward': -0.046930911897299185, 'episode_steps': 12.29, 'order_profits': -0.04927503319011064, 'order_steps': 7.99}\n",
      "Episode 37000: reward=-0, steps=6, speed=37.2 f/s, elapsed=1:48:12\n",
      "Episode 37100: reward=-0, steps=5, speed=37.2 f/s, elapsed=1:48:33\n",
      "Episode 37200: reward=-0, steps=7, speed=37.2 f/s, elapsed=1:48:51\n",
      "Episode 37300: reward=-0, steps=6, speed=37.1 f/s, elapsed=1:49:12\n",
      "Episode 37400: reward=-0, steps=8, speed=37.1 f/s, elapsed=1:49:34\n",
      "Episode 37500: reward=0, steps=9, speed=37.1 f/s, elapsed=1:49:53\n",
      "Episode 37600: reward=-0, steps=5, speed=37.1 f/s, elapsed=1:50:12\n",
      "Episode 37700: reward=-0, steps=2, speed=37.1 f/s, elapsed=1:50:32\n",
      "Episode 37800: reward=-1, steps=4, speed=37.1 f/s, elapsed=1:50:49\n",
      "Episode 37900: reward=-0, steps=4, speed=37.1 f/s, elapsed=1:51:07\n",
      "Episode 38000: reward=-0, steps=6, speed=37.0 f/s, elapsed=1:51:27\n",
      "Episode 38100: reward=-0, steps=10, speed=37.0 f/s, elapsed=1:51:46\n",
      "Episode 38200: reward=-1, steps=6, speed=37.0 f/s, elapsed=1:52:01\n",
      "Episode 38300: reward=0, steps=6, speed=36.9 f/s, elapsed=1:52:20\n",
      "Episode 38400: reward=-0, steps=4, speed=36.9 f/s, elapsed=1:52:38\n",
      "230000: tst: {'episode_reward': -0.035654781015620536, 'episode_steps': 18.22, 'order_profits': -0.03697229907274162, 'order_steps': 6.84}\n",
      "230000: val: {'episode_reward': -0.04937934793409186, 'episode_steps': 10.69, 'order_profits': -0.05044161297239245, 'order_steps': 4.47}\n",
      "Episode 38500: reward=-0, steps=3, speed=36.8 f/s, elapsed=1:52:56\n",
      "Episode 38600: reward=-1, steps=11, speed=36.8 f/s, elapsed=1:53:13\n",
      "Episode 38700: reward=-0, steps=2, speed=36.9 f/s, elapsed=1:53:30\n",
      "Episode 38800: reward=-0, steps=6, speed=36.8 f/s, elapsed=1:53:49\n",
      "Episode 38900: reward=0, steps=4, speed=36.8 f/s, elapsed=1:54:07\n",
      "Episode 39000: reward=-1, steps=5, speed=36.7 f/s, elapsed=1:54:26\n",
      "Episode 39100: reward=-0, steps=13, speed=36.7 f/s, elapsed=1:54:44\n",
      "Episode 39200: reward=-0, steps=5, speed=36.6 f/s, elapsed=1:55:04\n",
      "Episode 39300: reward=-0, steps=3, speed=36.6 f/s, elapsed=1:55:24\n",
      "Episode 39400: reward=0, steps=4, speed=36.6 f/s, elapsed=1:55:42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 39500: reward=-0, steps=2, speed=36.6 f/s, elapsed=1:55:59\n",
      "Episode 39600: reward=0, steps=4, speed=36.5 f/s, elapsed=1:56:20\n",
      "Episode 39700: reward=-0, steps=12, speed=36.4 f/s, elapsed=1:56:39\n",
      "Episode 39800: reward=-0, steps=10, speed=36.3 f/s, elapsed=1:56:57\n",
      "Episode 39900: reward=-0, steps=4, speed=36.3 f/s, elapsed=1:57:14\n",
      "Episode 40000: reward=0, steps=4, speed=36.3 f/s, elapsed=1:57:31\n",
      "240000: tst: {'episode_reward': -0.11372361795930877, 'episode_steps': 7.31, 'order_profits': -0.11364899715566795, 'order_steps': 5.24}\n",
      "240000: val: {'episode_reward': -0.11036891272248134, 'episode_steps': 6.17, 'order_profits': -0.11128866508097313, 'order_steps': 4.04}\n",
      "Episode 40100: reward=0, steps=10, speed=36.2 f/s, elapsed=1:57:50\n",
      "Episode 40200: reward=0, steps=2, speed=36.2 f/s, elapsed=1:58:07\n",
      "Episode 40300: reward=0, steps=14, speed=36.2 f/s, elapsed=1:58:27\n",
      "Episode 40400: reward=-1, steps=7, speed=36.2 f/s, elapsed=1:58:43\n",
      "Episode 40500: reward=0, steps=4, speed=36.2 f/s, elapsed=1:59:00\n",
      "Episode 40600: reward=-0, steps=3, speed=36.2 f/s, elapsed=1:59:18\n",
      "Episode 40700: reward=0, steps=5, speed=36.2 f/s, elapsed=1:59:34\n",
      "Episode 40800: reward=-0, steps=4, speed=36.1 f/s, elapsed=1:59:54\n",
      "Episode 40900: reward=-0, steps=6, speed=36.1 f/s, elapsed=2:00:12\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Stock path\n",
    "    data = STOCKS\n",
    "    \n",
    "    # Year to train on\n",
    "    year = None\n",
    "    \n",
    "    # Validation path\n",
    "    val = VAL_STOCKS\n",
    "    \n",
    "    # Run name\n",
    "    run_name = \"run_name\"\n",
    "    \n",
    "    # Get the device type\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Path for saving\n",
    "    saves_path = SAVES_DIR / f\"simple-{run_name}\"\n",
    "    \n",
    "    # Make saving directry (if doesn't exist)\n",
    "    saves_path.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    # Training path\n",
    "    data_path = pathlib.Path(data)\n",
    "    \n",
    "    # Validation path\n",
    "    val_path = pathlib.Path(val)\n",
    "\n",
    "    # If year is defined OR data_path already exists\n",
    "    if (year is not None) or (data_path.is_file()):\n",
    "        \n",
    "        # If year is defined\n",
    "        if year is not None:\n",
    "            \n",
    "            # Load the stock data\n",
    "            stock_data = data_loader.load_year_data(year)\n",
    "            \n",
    "        # If year is NOT defined\n",
    "        else:\n",
    "            \n",
    "            # Load the stock data\n",
    "            stock_data = {\"YNDX\": data_loader.load_relative(data_path)}\n",
    "            \n",
    "        # Instantiate the environment for training\n",
    "        env = environ.StocksEnv(stock_data, bars_count = BARS_COUNT)\n",
    "        \n",
    "        # Instantiate the environment for testing\n",
    "        env_tst = environ.StocksEnv(stock_data, bars_count = BARS_COUNT)\n",
    "        \n",
    "    # If data_path exists\n",
    "    elif data_path.is_dir():\n",
    "        \n",
    "        # Instantiate the environment for training\n",
    "        env = environ.StocksEnv.from_dir(data_path, bars_count = BARS_COUNT)\n",
    "        \n",
    "        # Instantiate the environment for testing\n",
    "        env_tst = environ.StocksEnv.from_dir(data_path, bars_count = BARS_COUNT)\n",
    "        \n",
    "    # If year is not defined AND data_path is not exists\n",
    "    else:\n",
    "        \n",
    "        # Raise error\n",
    "        raise RuntimeError(\"No data to train on\")\n",
    "\n",
    "    # Wrap the environment with time limit\n",
    "    env = gym.wrappers.TimeLimit(env, max_episode_steps = 1000)\n",
    "    \n",
    "    # Get the validation data\n",
    "    val_data = {\"YNDX\": data_loader.load_relative(val_path)}\n",
    "    \n",
    "    # Instantiate the environment for validation\n",
    "    env_val = environ.StocksEnv(val_data, bars_count = BARS_COUNT)\n",
    "\n",
    "    # Instantiate the sourse network\n",
    "    net = models.SimpleFFDQN(env.observation_space.shape[0], env.action_space.n).to(device)\n",
    "    \n",
    "    # Instantiate the target network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "    # Instantiate the action selector (epsilon-greedy)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(EPS_START)\n",
    "    \n",
    "    # Track epsilon\n",
    "    eps_tracker = ptan.actions.EpsilonTracker(selector, EPS_START, EPS_FINAL, EPS_STEPS)\n",
    "    \n",
    "    # Instantiate the DQN agent\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "    \n",
    "    # Instantiate the experience source\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, GAMMA, steps_count = REWARD_STEPS)\n",
    "    \n",
    "    # Instantiate the experience replay buffer\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)\n",
    "    \n",
    "    # Instantiate the adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
    "\n",
    "    \n",
    "    # Function for procesing each batch\n",
    "    def process_batch(engine, batch):\n",
    "        \n",
    "        # Reset the optimizer's weight to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss_v = common.calc_loss(batch, net, tgt_net.target_model, gamma = GAMMA ** REWARD_STEPS, device = device)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss_v.backward()\n",
    "        \n",
    "        # Do optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track epsilon\n",
    "        eps_tracker.frame(engine.state.iteration)\n",
    "\n",
    "        # \n",
    "        if getattr(engine.state, \"eval_states\", None) is None:\n",
    "            \n",
    "            # Sample from buffer\n",
    "            eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
    "            \n",
    "            # Convert each state into numpy array\n",
    "            eval_states = [np.array(transition.state, copy = False) for transition in eval_states]\n",
    "            \n",
    "            # Update the evaluation states in engine\n",
    "            engine.state.eval_states = np.array(eval_states, copy=False)\n",
    "\n",
    "        return {\"loss\": loss_v.item(), \"epsilon\": selector.epsilon,}\n",
    "\n",
    "    # Instantiate the engine\n",
    "    engine = Engine(process_batch)\n",
    "    \n",
    "    # Setup the ignite\n",
    "    tb = common.setup_ignite(engine, exp_source, f\"simple-{run_name}\", extra_metrics=('values_mean',))\n",
    "\n",
    "    # \n",
    "    @engine.on(ptan.ignite.PeriodEvents.ITERS_1000_COMPLETED)\n",
    "    \n",
    "    # Function for synching\n",
    "    def sync_eval(engine: Engine):\n",
    "        \n",
    "        # Sync source network with target network\n",
    "        tgt_net.sync()\n",
    "\n",
    "        # Calculate the mean of values\n",
    "        mean_val = common.calc_values_of_states(engine.state.eval_states, net, device = device)\n",
    "        \n",
    "        # Update the metrics in engine\n",
    "        engine.state.metrics[\"values_mean\"] = mean_val\n",
    "        \n",
    "        # \n",
    "        if getattr(engine.state, \"best_mean_val\", None) is None:\n",
    "            \n",
    "            # Update best mean value in engine\n",
    "            engine.state.best_mean_val = mean_val\n",
    "            \n",
    "        # If the mean values is more than best mean value\n",
    "        if engine.state.best_mean_val < mean_val:\n",
    "            \n",
    "            # Report\n",
    "            print(\"%d: Best mean value updated %.3f -> %.3f\" % (engine.state.iteration, engine.state.best_mean_val, mean_val))\n",
    "            \n",
    "            # Get the path\n",
    "            path = saves_path / (\"mean_value-%.3f.data\" % mean_val)\n",
    "            \n",
    "            # Save the weights\n",
    "            torch.save(net.state_dict(), path)\n",
    "            \n",
    "            # Update the best mean value in engine\n",
    "            engine.state.best_mean_val = mean_val\n",
    "\n",
    "    # \n",
    "    @engine.on(ptan.ignite.PeriodEvents.ITERS_10000_COMPLETED)\n",
    "    \n",
    "    # Function for doing validation\n",
    "    def validate(engine: Engine):\n",
    "        \n",
    "        # Test the model on testset\n",
    "        res = validation.validation_run(env_tst, net, device = device)\n",
    "        \n",
    "        # Report\n",
    "        print(\"%d: tst: %s\" % (engine.state.iteration, res))\n",
    "        \n",
    "        # Loop over keys and items in res\n",
    "        for key, val in res.items():\n",
    "            \n",
    "            # Update the metrics\n",
    "            engine.state.metrics[key + \"_tst\"] = val\n",
    "            \n",
    "        # Test the model on validation set\n",
    "        res = validation.validation_run(env_val, net, device=device)\n",
    "        \n",
    "        # Report\n",
    "        print(\"%d: val: %s\" % (engine.state.iteration, res))\n",
    "        \n",
    "        # Loop over keys and items in res\n",
    "        for key, val in res.items():\n",
    "            \n",
    "            # Update the metrics\n",
    "            engine.state.metrics[key + \"_val\"] = val\n",
    "            \n",
    "        # Get the validation reward\n",
    "        val_reward = res['episode_reward']\n",
    "        \n",
    "        # \n",
    "        if getattr(engine.state, \"best_val_reward\", None) is None:\n",
    "            \n",
    "            # Assign val_reward to best_val_reward\n",
    "            engine.state.best_val_reward = val_reward\n",
    "            \n",
    "        # If val_reward is larger than best_val_reward\n",
    "        if engine.state.best_val_reward < val_reward:\n",
    "            \n",
    "            # Report\n",
    "            print(\"Best validation reward updated: %.3f -> %.3f, model saved\" % (engine.state.best_val_reward, \n",
    "                                                                                 val_reward))\n",
    "            \n",
    "            # Assign val_reward to best_val_reward\n",
    "            engine.state.best_val_reward = val_reward\n",
    "            \n",
    "            # Get the path\n",
    "            path = saves_path / (\"val_reward-%.3f.data\" % val_reward)\n",
    "            \n",
    "            # Save the weight\n",
    "            torch.save(net.state_dict(), path)\n",
    "\n",
    "    # Instantiate the period event (if 1000 iteration got completed)\n",
    "    event = ptan.ignite.PeriodEvents.ITERS_10000_COMPLETED\n",
    "    \n",
    "    # Get the test metrics\n",
    "    tst_metrics = [m + \"_tst\" for m in validation.METRICS]\n",
    "    \n",
    "    # Test handler\n",
    "    tst_handler = tb_logger.OutputHandler(tag = \"test\", metric_names = tst_metrics)\n",
    "    \n",
    "    # \n",
    "    tb.attach(engine, log_handler = tst_handler, event_name = event)\n",
    "\n",
    "    # Get the validation metrics\n",
    "    val_metrics = [m + \"_val\" for m in validation.METRICS]\n",
    "    \n",
    "    # Validation handler\n",
    "    val_handler = tb_logger.OutputHandler(tag = \"validation\", metric_names = val_metrics)\n",
    "    \n",
    "    # \n",
    "    tb.attach(engine, log_handler = val_handler, event_name = event)\n",
    "\n",
    "    # Run the engine\n",
    "    engine.run(common.batch_generator(buffer, REPLAY_INITIAL, BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 6.2. Training 1D Convolutions Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import ptan\n",
    "import pathlib\n",
    "import gym.wrappers\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from ignite.engine import Engine\n",
    "from ignite.contrib.handlers import tensorboard_logger as tb_logger\n",
    "\n",
    "from lib import environ, data_loader, models, common, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving directory\n",
    "SAVES_DIR = pathlib.Path(\"saves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data path\n",
    "STOCKS = \"data/YNDX_160101_161231.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data path\n",
    "VAL_STOCKS = \"data/YNDX_150101_151231.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "BARS_COUNT = 10\n",
    "\n",
    "EPS_START = 1.0\n",
    "EPS_FINAL = 0.1\n",
    "EPS_STEPS = 1000000\n",
    "\n",
    "GAMMA = 0.99\n",
    "\n",
    "REPLAY_SIZE = 100000\n",
    "REPLAY_INITIAL = 10000\n",
    "REWARD_STEPS = 2\n",
    "LEARNING_RATE = 0.0001\n",
    "STATES_TO_EVALUATE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/YNDX_160101_161231.csv\n",
      "Read done, got 131542 rows, 99752 filtered, 0 open prices adjusted\n",
      "Reading data/YNDX_150101_151231.csv\n",
      "Read done, got 130566 rows, 104412 filtered, 0 open prices adjusted\n",
      "Episode 100: reward=-0, steps=11, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 200: reward=-0, steps=6, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 300: reward=-0, steps=2, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 400: reward=-0, steps=2, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 500: reward=-0, steps=4, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 600: reward=-0, steps=3, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 700: reward=0, steps=4, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 800: reward=-0, steps=3, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 900: reward=-0, steps=16, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 1000: reward=-0, steps=15, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 1100: reward=-0, steps=5, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 1200: reward=-0, steps=3, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 1300: reward=-0, steps=5, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 1400: reward=-0, steps=6, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 1500: reward=-0, steps=2, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 1600: reward=-0, steps=4, speed=0.0 f/s, elapsed=0:00:13\n",
      "Episode 1700: reward=-0, steps=7, speed=58.1 f/s, elapsed=0:00:17\n",
      "Episode 1800: reward=-0, steps=10, speed=57.9 f/s, elapsed=0:00:30\n",
      "Episode 1900: reward=0, steps=4, speed=57.6 f/s, elapsed=0:00:44\n",
      "Episode 2000: reward=-0, steps=18, speed=57.3 f/s, elapsed=0:00:58\n",
      "Episode 2100: reward=-0, steps=4, speed=57.0 f/s, elapsed=0:01:13\n",
      "Episode 2200: reward=-0, steps=7, speed=56.8 f/s, elapsed=0:01:26\n",
      "Episode 2300: reward=-0, steps=11, speed=56.6 f/s, elapsed=0:01:39\n",
      "Episode 2400: reward=0, steps=13, speed=56.4 f/s, elapsed=0:01:53\n",
      "Episode 2500: reward=-0, steps=3, speed=56.2 f/s, elapsed=0:02:04\n",
      "Episode 2600: reward=-0, steps=6, speed=56.0 f/s, elapsed=0:02:17\n",
      "Episode 2700: reward=-0, steps=5, speed=55.8 f/s, elapsed=0:02:29\n",
      "Episode 2800: reward=0, steps=9, speed=55.6 f/s, elapsed=0:02:42\n",
      "Episode 2900: reward=0, steps=2, speed=55.4 f/s, elapsed=0:02:54\n",
      "Episode 3000: reward=0, steps=4, speed=55.2 f/s, elapsed=0:03:07\n",
      "Episode 3100: reward=0, steps=9, speed=55.1 f/s, elapsed=0:03:21\n",
      "Episode 3200: reward=-0, steps=2, speed=54.9 f/s, elapsed=0:03:35\n",
      "Episode 3300: reward=-1, steps=5, speed=54.8 f/s, elapsed=0:03:48\n",
      "10000: tst: {'episode_reward': -0.061663161152185335, 'episode_steps': 167.72, 'order_profits': -0.08051584282511552, 'order_steps': 166.72}\n",
      "10000: val: {'episode_reward': -0.04107490793404212, 'episode_steps': 164.86, 'order_profits': -0.07869557435833972, 'order_steps': 163.83}\n",
      "Episode 3400: reward=-0, steps=12, speed=53.9 f/s, elapsed=0:04:35\n",
      "Episode 3500: reward=-1, steps=2, speed=53.8 f/s, elapsed=0:04:47\n",
      "Episode 3600: reward=-0, steps=4, speed=53.8 f/s, elapsed=0:04:59\n",
      "Episode 3700: reward=-1, steps=7, speed=53.6 f/s, elapsed=0:05:12\n",
      "Episode 3800: reward=-0, steps=9, speed=53.6 f/s, elapsed=0:05:24\n",
      "Episode 3900: reward=0, steps=4, speed=53.4 f/s, elapsed=0:05:38\n",
      "Episode 4000: reward=0, steps=5, speed=53.2 f/s, elapsed=0:05:51\n",
      "Episode 4100: reward=-1, steps=2, speed=53.0 f/s, elapsed=0:06:06\n",
      "Episode 4200: reward=-0, steps=8, speed=52.9 f/s, elapsed=0:06:19\n",
      "Episode 4300: reward=-0, steps=6, speed=52.7 f/s, elapsed=0:06:33\n",
      "Episode 4400: reward=-0, steps=2, speed=52.7 f/s, elapsed=0:06:45\n",
      "Episode 4500: reward=-0, steps=3, speed=52.5 f/s, elapsed=0:06:59\n",
      "Episode 4600: reward=0, steps=2, speed=52.5 f/s, elapsed=0:07:11\n",
      "Episode 4700: reward=-1, steps=4, speed=52.4 f/s, elapsed=0:07:26\n",
      "Episode 4800: reward=-1, steps=14, speed=52.2 f/s, elapsed=0:07:38\n",
      "Episode 4900: reward=0, steps=3, speed=51.9 f/s, elapsed=0:07:57\n",
      "20000: tst: {'episode_reward': -0.39108458265702495, 'episode_steps': 267.1, 'order_profits': -0.4153644672532538, 'order_steps': 128.93}\n",
      "20000: val: {'episode_reward': 0.26655436192080445, 'episode_steps': 308.27, 'order_profits': 0.23640600956120644, 'order_steps': 146.34}\n",
      "Best validation reward updated: -0.041 -> 0.267, model saved\n",
      "Episode 5000: reward=-0, steps=3, speed=51.0 f/s, elapsed=0:09:18\n",
      "Episode 5100: reward=-0, steps=6, speed=51.0 f/s, elapsed=0:09:31\n",
      "Episode 5200: reward=0, steps=3, speed=50.9 f/s, elapsed=0:09:44\n",
      "Episode 5300: reward=-0, steps=4, speed=50.8 f/s, elapsed=0:09:58\n",
      "Episode 5400: reward=-0, steps=9, speed=50.7 f/s, elapsed=0:10:12\n",
      "Episode 5500: reward=-0, steps=5, speed=50.7 f/s, elapsed=0:10:24\n",
      "Episode 5600: reward=-0, steps=2, speed=50.7 f/s, elapsed=0:10:36\n",
      "Episode 5700: reward=-1, steps=4, speed=50.6 f/s, elapsed=0:10:48\n",
      "Episode 5800: reward=-1, steps=8, speed=50.5 f/s, elapsed=0:11:02\n",
      "Episode 5900: reward=-0, steps=4, speed=50.5 f/s, elapsed=0:11:14\n",
      "Episode 6000: reward=0, steps=11, speed=50.5 f/s, elapsed=0:11:28\n",
      "Episode 6100: reward=-0, steps=6, speed=50.4 f/s, elapsed=0:11:41\n",
      "Episode 6200: reward=-0, steps=13, speed=50.4 f/s, elapsed=0:11:53\n",
      "Episode 6300: reward=-0, steps=6, speed=50.4 f/s, elapsed=0:12:06\n",
      "Episode 6400: reward=-0, steps=20, speed=50.4 f/s, elapsed=0:12:19\n",
      "Episode 6500: reward=0, steps=21, speed=50.4 f/s, elapsed=0:12:32\n",
      "30000: tst: {'episode_reward': -0.3151551204463444, 'episode_steps': 269.24, 'order_profits': -0.34132892227010075, 'order_steps': 143.74}\n",
      "30000: val: {'episode_reward': -0.08011628194768292, 'episode_steps': 279.96, 'order_profits': -0.10815507442435161, 'order_steps': 142.19}\n",
      "Episode 6600: reward=-0, steps=10, speed=49.6 f/s, elapsed=0:13:35\n",
      "Episode 6700: reward=-0, steps=6, speed=49.6 f/s, elapsed=0:13:46\n",
      "Episode 6800: reward=-0, steps=6, speed=49.6 f/s, elapsed=0:13:58\n",
      "Episode 6900: reward=-1, steps=5, speed=49.6 f/s, elapsed=0:14:09\n",
      "Episode 7000: reward=-0, steps=7, speed=49.7 f/s, elapsed=0:14:21\n",
      "Episode 7100: reward=-0, steps=2, speed=49.7 f/s, elapsed=0:14:32\n",
      "Episode 7200: reward=-0, steps=3, speed=49.7 f/s, elapsed=0:14:44\n",
      "Episode 7300: reward=-0, steps=3, speed=49.7 f/s, elapsed=0:14:57\n",
      "Episode 7400: reward=-0, steps=2, speed=49.7 f/s, elapsed=0:15:11\n",
      "Episode 7500: reward=-0, steps=6, speed=49.7 f/s, elapsed=0:15:23\n",
      "Episode 7600: reward=-1, steps=4, speed=49.7 f/s, elapsed=0:15:36\n",
      "Episode 7700: reward=-0, steps=2, speed=49.7 f/s, elapsed=0:15:48\n",
      "Episode 7800: reward=0, steps=6, speed=49.8 f/s, elapsed=0:16:01\n",
      "Episode 7900: reward=-0, steps=4, speed=49.8 f/s, elapsed=0:16:14\n",
      "Episode 8000: reward=-0, steps=5, speed=49.8 f/s, elapsed=0:16:25\n",
      "Episode 8100: reward=-0, steps=8, speed=49.8 f/s, elapsed=0:16:38\n",
      "40000: tst: {'episode_reward': -0.10461313478278093, 'episode_steps': 266.38, 'order_profits': -0.12130419016591702, 'order_steps': 145.78}\n",
      "40000: val: {'episode_reward': -0.36572154349386216, 'episode_steps': 298.63, 'order_profits': -0.4071034089034671, 'order_steps': 159.27}\n",
      "Episode 8200: reward=-0, steps=9, speed=49.0 f/s, elapsed=0:17:42\n",
      "Episode 8300: reward=1, steps=7, speed=49.0 f/s, elapsed=0:17:54\n",
      "Episode 8400: reward=-0, steps=5, speed=49.0 f/s, elapsed=0:18:06\n",
      "Episode 8500: reward=-0, steps=6, speed=49.1 f/s, elapsed=0:18:19\n",
      "Episode 8600: reward=-0, steps=3, speed=49.1 f/s, elapsed=0:18:31\n",
      "Episode 8700: reward=0, steps=13, speed=49.1 f/s, elapsed=0:18:45\n",
      "Episode 8800: reward=-0, steps=5, speed=49.1 f/s, elapsed=0:18:57\n",
      "Episode 8900: reward=-1, steps=9, speed=49.1 f/s, elapsed=0:19:10\n",
      "Episode 9000: reward=-0, steps=3, speed=49.1 f/s, elapsed=0:19:23\n",
      "Episode 9100: reward=0, steps=12, speed=49.2 f/s, elapsed=0:19:36\n",
      "Episode 9200: reward=-0, steps=6, speed=49.2 f/s, elapsed=0:19:49\n",
      "Episode 9300: reward=-0, steps=7, speed=49.2 f/s, elapsed=0:20:00\n",
      "Episode 9400: reward=0, steps=6, speed=49.2 f/s, elapsed=0:20:13\n",
      "Episode 9500: reward=-0, steps=2, speed=49.2 f/s, elapsed=0:20:26\n",
      "Episode 9600: reward=0, steps=2, speed=49.2 f/s, elapsed=0:20:38\n",
      "Episode 9700: reward=-0, steps=4, speed=49.3 f/s, elapsed=0:20:51\n",
      "50000: tst: {'episode_reward': -0.30557461336119013, 'episode_steps': 286.14, 'order_profits': -0.32041573457818173, 'order_steps': 143.51}\n",
      "50000: val: {'episode_reward': 0.053244679706721684, 'episode_steps': 279.02, 'order_profits': 0.021924955858506776, 'order_steps': 138.64}\n",
      "Episode 9800: reward=0, steps=12, speed=48.5 f/s, elapsed=0:21:56\n",
      "Episode 9900: reward=0, steps=8, speed=48.5 f/s, elapsed=0:22:09\n",
      "Episode 10000: reward=-0, steps=13, speed=48.5 f/s, elapsed=0:22:22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10100: reward=-0, steps=9, speed=48.6 f/s, elapsed=0:22:36\n",
      "Episode 10200: reward=-1, steps=4, speed=48.6 f/s, elapsed=0:22:48\n",
      "Episode 10300: reward=0, steps=9, speed=48.6 f/s, elapsed=0:22:59\n",
      "Episode 10400: reward=-0, steps=12, speed=48.6 f/s, elapsed=0:23:11\n",
      "Episode 10500: reward=-0, steps=4, speed=48.6 f/s, elapsed=0:23:24\n",
      "Episode 10600: reward=0, steps=11, speed=48.6 f/s, elapsed=0:23:38\n",
      "Episode 10700: reward=-0, steps=2, speed=48.6 f/s, elapsed=0:23:51\n",
      "Episode 10800: reward=-0, steps=11, speed=48.6 f/s, elapsed=0:24:05\n",
      "Episode 10900: reward=-0, steps=8, speed=48.6 f/s, elapsed=0:24:17\n",
      "Episode 11000: reward=-0, steps=2, speed=48.6 f/s, elapsed=0:24:30\n",
      "Episode 11100: reward=-0, steps=4, speed=48.5 f/s, elapsed=0:24:42\n",
      "Episode 11200: reward=-0, steps=4, speed=48.5 f/s, elapsed=0:24:56\n",
      "Episode 11300: reward=-0, steps=9, speed=48.5 f/s, elapsed=0:25:08\n",
      "60000: tst: {'episode_reward': 0.1481011370630059, 'episode_steps': 285.25, 'order_profits': 0.13052907300045086, 'order_steps': 125.22}\n",
      "60000: val: {'episode_reward': -0.26214014345304437, 'episode_steps': 284.83, 'order_profits': -0.2860700426950001, 'order_steps': 125.54}\n",
      "Episode 11400: reward=-0, steps=8, speed=47.7 f/s, elapsed=0:26:17\n",
      "Episode 11500: reward=-0, steps=7, speed=47.6 f/s, elapsed=0:26:32\n",
      "Episode 11600: reward=-0, steps=5, speed=47.6 f/s, elapsed=0:26:47\n",
      "Episode 11700: reward=0, steps=4, speed=47.6 f/s, elapsed=0:27:02\n",
      "Episode 11800: reward=-0, steps=3, speed=47.5 f/s, elapsed=0:27:16\n",
      "Episode 11900: reward=-0, steps=5, speed=47.5 f/s, elapsed=0:27:30\n",
      "Episode 12000: reward=-0, steps=5, speed=47.5 f/s, elapsed=0:27:43\n",
      "Episode 12100: reward=-0, steps=5, speed=47.2 f/s, elapsed=0:28:02\n",
      "Episode 12200: reward=-0, steps=8, speed=47.0 f/s, elapsed=0:28:21\n",
      "Episode 12300: reward=0, steps=7, speed=46.8 f/s, elapsed=0:28:37\n",
      "Episode 12400: reward=-0, steps=8, speed=46.7 f/s, elapsed=0:28:53\n",
      "Episode 12500: reward=0, steps=5, speed=46.5 f/s, elapsed=0:29:08\n",
      "Episode 12600: reward=-1, steps=13, speed=46.4 f/s, elapsed=0:29:22\n",
      "Episode 12700: reward=-0, steps=3, speed=46.3 f/s, elapsed=0:29:37\n",
      "Episode 12800: reward=-0, steps=3, speed=46.2 f/s, elapsed=0:29:53\n",
      "Episode 12900: reward=-0, steps=7, speed=46.1 f/s, elapsed=0:30:08\n",
      "70000: tst: {'episode_reward': -0.4051049606191653, 'episode_steps': 308.98, 'order_profits': -0.4270953591924665, 'order_steps': 154.04}\n",
      "70000: val: {'episode_reward': -0.0486152479953547, 'episode_steps': 290.52, 'order_profits': -0.07849864225135268, 'order_steps': 148.17}\n",
      "Episode 13000: reward=-0, steps=13, speed=45.3 f/s, elapsed=0:31:25\n",
      "Episode 13100: reward=-0, steps=4, speed=45.2 f/s, elapsed=0:31:41\n",
      "Episode 13200: reward=-1, steps=5, speed=45.1 f/s, elapsed=0:31:58\n",
      "Episode 13300: reward=-0, steps=8, speed=45.0 f/s, elapsed=0:32:14\n",
      "Episode 13400: reward=-0, steps=4, speed=44.8 f/s, elapsed=0:32:31\n",
      "Episode 13500: reward=-0, steps=5, speed=44.7 f/s, elapsed=0:32:47\n",
      "Episode 13600: reward=-0, steps=2, speed=44.6 f/s, elapsed=0:33:04\n",
      "Episode 13700: reward=1, steps=14, speed=44.6 f/s, elapsed=0:33:20\n",
      "Episode 13800: reward=0, steps=2, speed=44.4 f/s, elapsed=0:33:38\n",
      "Episode 13900: reward=1, steps=4, speed=44.4 f/s, elapsed=0:33:56\n",
      "Episode 14000: reward=0, steps=5, speed=44.3 f/s, elapsed=0:34:13\n",
      "Episode 14100: reward=1, steps=8, speed=44.2 f/s, elapsed=0:34:29\n",
      "Episode 14200: reward=-0, steps=12, speed=44.1 f/s, elapsed=0:34:45\n",
      "Episode 14300: reward=0, steps=3, speed=44.0 f/s, elapsed=0:35:01\n",
      "Episode 14400: reward=0, steps=5, speed=43.9 f/s, elapsed=0:35:17\n",
      "80000: tst: {'episode_reward': -0.05537087113202944, 'episode_steps': 283.26, 'order_profits': -0.07422295964377432, 'order_steps': 132.68}\n",
      "80000: val: {'episode_reward': 0.04162843029066053, 'episode_steps': 281.21, 'order_profits': 0.016740051280323166, 'order_steps': 139.33}\n",
      "Episode 14500: reward=-1, steps=4, speed=43.2 f/s, elapsed=0:36:35\n",
      "Episode 14600: reward=0, steps=3, speed=43.1 f/s, elapsed=0:36:54\n",
      "Episode 14700: reward=0, steps=5, speed=43.0 f/s, elapsed=0:37:10\n",
      "Episode 14800: reward=-0, steps=2, speed=43.0 f/s, elapsed=0:37:25\n",
      "Episode 14900: reward=-0, steps=5, speed=42.9 f/s, elapsed=0:37:40\n",
      "Episode 15000: reward=-0, steps=10, speed=42.8 f/s, elapsed=0:37:56\n",
      "Episode 15100: reward=-0, steps=6, speed=42.7 f/s, elapsed=0:38:13\n",
      "Episode 15200: reward=-0, steps=7, speed=42.6 f/s, elapsed=0:38:29\n",
      "Episode 15300: reward=0, steps=8, speed=42.5 f/s, elapsed=0:38:46\n",
      "Episode 15400: reward=-0, steps=2, speed=42.5 f/s, elapsed=0:39:03\n",
      "Episode 15500: reward=-0, steps=7, speed=42.4 f/s, elapsed=0:39:22\n",
      "Episode 15600: reward=-0, steps=3, speed=42.4 f/s, elapsed=0:39:37\n",
      "Episode 15700: reward=0, steps=10, speed=42.3 f/s, elapsed=0:39:53\n",
      "Episode 15800: reward=-0, steps=3, speed=42.2 f/s, elapsed=0:40:10\n",
      "Episode 15900: reward=-0, steps=5, speed=42.1 f/s, elapsed=0:40:30\n",
      "90000: tst: {'episode_reward': 0.048289293688645187, 'episode_steps': 315.44, 'order_profits': 0.02631005708469657, 'order_steps': 156.35}\n",
      "90000: val: {'episode_reward': 0.5558281364576677, 'episode_steps': 307.89, 'order_profits': 0.5315284700428887, 'order_steps': 147.8}\n",
      "Best validation reward updated: 0.267 -> 0.556, model saved\n",
      "Episode 16000: reward=-0, steps=10, speed=41.4 f/s, elapsed=0:41:58\n",
      "Episode 16100: reward=-0, steps=9, speed=41.4 f/s, elapsed=0:42:15\n",
      "Episode 16200: reward=-0, steps=2, speed=41.3 f/s, elapsed=0:42:31\n",
      "Episode 16300: reward=-0, steps=5, speed=41.3 f/s, elapsed=0:42:48\n",
      "Episode 16400: reward=-0, steps=6, speed=41.3 f/s, elapsed=0:43:04\n",
      "Episode 16500: reward=-0, steps=4, speed=41.2 f/s, elapsed=0:43:22\n",
      "Episode 16600: reward=0, steps=6, speed=41.2 f/s, elapsed=0:43:40\n",
      "Episode 16700: reward=-0, steps=8, speed=41.1 f/s, elapsed=0:43:55\n",
      "Episode 16800: reward=-0, steps=13, speed=41.1 f/s, elapsed=0:44:13\n",
      "Episode 16900: reward=-0, steps=9, speed=41.0 f/s, elapsed=0:44:30\n",
      "Episode 17000: reward=-1, steps=13, speed=41.0 f/s, elapsed=0:44:46\n",
      "Episode 17100: reward=-0, steps=4, speed=41.0 f/s, elapsed=0:45:03\n",
      "Episode 17200: reward=-0, steps=2, speed=40.9 f/s, elapsed=0:45:22\n",
      "Episode 17300: reward=-0, steps=10, speed=40.9 f/s, elapsed=0:45:39\n",
      "Episode 17400: reward=1, steps=7, speed=40.9 f/s, elapsed=0:45:55\n",
      "100000: tst: {'episode_reward': -0.03878033847928983, 'episode_steps': 281.01, 'order_profits': -0.04980149741675116, 'order_steps': 119.97}\n",
      "100000: val: {'episode_reward': -0.025088716853128492, 'episode_steps': 284.39, 'order_profits': -0.04808654875418391, 'order_steps': 139.71428571428572}\n",
      "Episode 17500: reward=0, steps=6, speed=40.2 f/s, elapsed=0:47:11\n",
      "Episode 17600: reward=-0, steps=4, speed=40.2 f/s, elapsed=0:47:26\n",
      "Episode 17700: reward=-0, steps=5, speed=40.2 f/s, elapsed=0:47:44\n",
      "Episode 17800: reward=-0, steps=5, speed=40.2 f/s, elapsed=0:47:59\n",
      "Episode 17900: reward=0, steps=2, speed=40.1 f/s, elapsed=0:48:18\n",
      "Episode 18000: reward=-0, steps=7, speed=40.1 f/s, elapsed=0:48:37\n",
      "Episode 18100: reward=0, steps=25, speed=40.0 f/s, elapsed=0:48:54\n",
      "Episode 18200: reward=0, steps=10, speed=40.0 f/s, elapsed=0:49:13\n",
      "Episode 18300: reward=-1, steps=16, speed=39.9 f/s, elapsed=0:49:31\n",
      "Episode 18400: reward=-0, steps=3, speed=39.9 f/s, elapsed=0:49:49\n",
      "Episode 18500: reward=-1, steps=8, speed=39.8 f/s, elapsed=0:50:08\n",
      "Episode 18600: reward=-0, steps=12, speed=39.7 f/s, elapsed=0:50:31\n",
      "Episode 18700: reward=-0, steps=6, speed=39.7 f/s, elapsed=0:50:47\n",
      "Episode 18800: reward=0, steps=16, speed=39.6 f/s, elapsed=0:51:05\n",
      "Episode 18900: reward=0, steps=8, speed=39.6 f/s, elapsed=0:51:23\n",
      "110000: tst: {'episode_reward': -0.41604778577183976, 'episode_steps': 277.21, 'order_profits': -0.4425607470700231, 'order_steps': 136.75510204081633}\n",
      "110000: val: {'episode_reward': -0.3708145687519898, 'episode_steps': 279.16, 'order_profits': -0.40109141699081774, 'order_steps': 153.06}\n",
      "Episode 19000: reward=-0, steps=19, speed=39.0 f/s, elapsed=0:52:42\n",
      "Episode 19100: reward=1, steps=14, speed=39.0 f/s, elapsed=0:52:59\n",
      "Episode 19200: reward=-1, steps=10, speed=39.0 f/s, elapsed=0:53:16\n",
      "Episode 19300: reward=-0, steps=7, speed=39.0 f/s, elapsed=0:53:34\n",
      "Episode 19400: reward=-1, steps=6, speed=38.9 f/s, elapsed=0:53:54\n",
      "Episode 19500: reward=-1, steps=20, speed=38.9 f/s, elapsed=0:54:13\n",
      "Episode 19600: reward=-0, steps=16, speed=38.9 f/s, elapsed=0:54:31\n",
      "Episode 19700: reward=-1, steps=8, speed=38.9 f/s, elapsed=0:54:48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19800: reward=-0, steps=2, speed=38.9 f/s, elapsed=0:55:04\n",
      "Episode 19900: reward=-0, steps=10, speed=38.9 f/s, elapsed=0:55:23\n",
      "Episode 20000: reward=0, steps=7, speed=38.9 f/s, elapsed=0:55:41\n",
      "Episode 20100: reward=1, steps=9, speed=38.9 f/s, elapsed=0:55:59\n",
      "Episode 20200: reward=-0, steps=8, speed=38.9 f/s, elapsed=0:56:15\n",
      "Episode 20300: reward=0, steps=4, speed=38.8 f/s, elapsed=0:56:32\n",
      "Episode 20400: reward=1, steps=6, speed=38.8 f/s, elapsed=0:56:50\n",
      "120000: tst: {'episode_reward': 0.0967929133922643, 'episode_steps': 314.98, 'order_profits': 0.07257138652029868, 'order_steps': 156.63}\n",
      "120000: val: {'episode_reward': 0.13342067575363592, 'episode_steps': 315.02, 'order_profits': 0.10165642952697351, 'order_steps': 153.7070707070707}\n",
      "Episode 20500: reward=-1, steps=25, speed=38.2 f/s, elapsed=0:58:17\n",
      "Episode 20600: reward=0, steps=3, speed=38.2 f/s, elapsed=0:58:34\n",
      "Episode 20700: reward=-0, steps=9, speed=38.3 f/s, elapsed=0:58:50\n",
      "Episode 20800: reward=-0, steps=7, speed=38.3 f/s, elapsed=0:59:09\n",
      "Episode 20900: reward=-0, steps=18, speed=38.4 f/s, elapsed=0:59:25\n",
      "Episode 21000: reward=0, steps=2, speed=38.4 f/s, elapsed=0:59:40\n",
      "Episode 21100: reward=-0, steps=6, speed=38.5 f/s, elapsed=0:59:58\n",
      "Episode 21200: reward=-0, steps=5, speed=38.5 f/s, elapsed=1:00:11\n",
      "Episode 21300: reward=0, steps=5, speed=38.6 f/s, elapsed=1:00:26\n",
      "Episode 21400: reward=-0, steps=5, speed=38.6 f/s, elapsed=1:00:43\n",
      "Episode 21500: reward=0, steps=4, speed=38.7 f/s, elapsed=1:00:58\n",
      "Episode 21600: reward=-0, steps=6, speed=38.7 f/s, elapsed=1:01:16\n",
      "Episode 21700: reward=-0, steps=5, speed=38.7 f/s, elapsed=1:01:33\n",
      "Episode 21800: reward=-0, steps=3, speed=38.8 f/s, elapsed=1:01:48\n",
      "Episode 21900: reward=-0, steps=4, speed=38.8 f/s, elapsed=1:02:04\n",
      "130000: tst: {'episode_reward': -0.05432252454448596, 'episode_steps': 217.03, 'order_profits': -0.07244366404824935, 'order_steps': 154.65}\n",
      "130000: val: {'episode_reward': 0.030658765412204355, 'episode_steps': 193.25, 'order_profits': -0.00442542389474389, 'order_steps': 157.68}\n",
      "Episode 22000: reward=-0, steps=5, speed=38.3 f/s, elapsed=1:02:58\n",
      "Episode 22100: reward=-0, steps=2, speed=38.3 f/s, elapsed=1:03:15\n",
      "Episode 22200: reward=-0, steps=2, speed=38.4 f/s, elapsed=1:03:31\n",
      "Episode 22300: reward=-1, steps=3, speed=38.4 f/s, elapsed=1:03:47\n",
      "Episode 22400: reward=-0, steps=6, speed=38.4 f/s, elapsed=1:04:05\n",
      "Episode 22500: reward=-0, steps=4, speed=38.4 f/s, elapsed=1:04:23\n",
      "Episode 22600: reward=-0, steps=8, speed=38.4 f/s, elapsed=1:04:43\n",
      "Episode 22700: reward=-1, steps=10, speed=38.5 f/s, elapsed=1:05:02\n",
      "Episode 22800: reward=-1, steps=3, speed=38.5 f/s, elapsed=1:05:20\n",
      "Episode 22900: reward=-1, steps=7, speed=38.5 f/s, elapsed=1:05:36\n",
      "Episode 23000: reward=-0, steps=14, speed=38.5 f/s, elapsed=1:05:53\n",
      "Episode 23100: reward=-0, steps=9, speed=38.5 f/s, elapsed=1:06:10\n",
      "Episode 23200: reward=1, steps=5, speed=38.5 f/s, elapsed=1:06:28\n",
      "Episode 23300: reward=-0, steps=5, speed=38.5 f/s, elapsed=1:06:47\n",
      "Episode 23400: reward=-1, steps=8, speed=38.5 f/s, elapsed=1:07:05\n",
      "140000: tst: {'episode_reward': 0.13269935040566166, 'episode_steps': 212.05, 'order_profits': 0.10845926339692907, 'order_steps': 168.38}\n",
      "140000: val: {'episode_reward': -0.01540024032895435, 'episode_steps': 163.65, 'order_profits': -0.04069281997588445, 'order_steps': 143.12}\n",
      "Episode 23500: reward=-0, steps=16, speed=38.0 f/s, elapsed=1:08:05\n",
      "Episode 23600: reward=-0, steps=6, speed=38.0 f/s, elapsed=1:08:23\n",
      "Episode 23700: reward=-0, steps=5, speed=38.0 f/s, elapsed=1:08:43\n",
      "Episode 23800: reward=-0, steps=6, speed=38.0 f/s, elapsed=1:09:01\n",
      "Episode 23900: reward=0, steps=11, speed=38.0 f/s, elapsed=1:09:21\n",
      "Episode 24000: reward=0, steps=10, speed=38.0 f/s, elapsed=1:09:39\n",
      "Episode 24100: reward=-1, steps=7, speed=38.0 f/s, elapsed=1:09:57\n",
      "Episode 24200: reward=-1, steps=10, speed=38.0 f/s, elapsed=1:10:14\n",
      "Episode 24300: reward=0, steps=11, speed=38.0 f/s, elapsed=1:10:33\n",
      "Episode 24400: reward=-0, steps=2, speed=38.0 f/s, elapsed=1:10:49\n",
      "Episode 24500: reward=-1, steps=11, speed=38.1 f/s, elapsed=1:11:03\n",
      "Episode 24600: reward=-0, steps=7, speed=38.1 f/s, elapsed=1:11:20\n",
      "Episode 24700: reward=-0, steps=8, speed=38.1 f/s, elapsed=1:11:38\n",
      "Episode 24800: reward=-1, steps=6, speed=38.1 f/s, elapsed=1:11:55\n",
      "150000: tst: {'episode_reward': 0.09073243162557489, 'episode_steps': 128.51, 'order_profits': 0.07766362637051276, 'order_steps': 126.38}\n",
      "150000: val: {'episode_reward': 0.5402927909062886, 'episode_steps': 154.71, 'order_profits': 0.5198381378907557, 'order_steps': 152.58}\n",
      "Episode 24900: reward=-0, steps=7, speed=37.6 f/s, elapsed=1:12:43\n",
      "Episode 25000: reward=-0, steps=9, speed=37.6 f/s, elapsed=1:13:03\n",
      "Episode 25100: reward=0, steps=7, speed=37.6 f/s, elapsed=1:13:20\n",
      "Episode 25200: reward=-0, steps=5, speed=37.6 f/s, elapsed=1:13:37\n",
      "Episode 25300: reward=-0, steps=6, speed=37.6 f/s, elapsed=1:13:52\n",
      "Episode 25400: reward=-1, steps=15, speed=37.7 f/s, elapsed=1:14:09\n",
      "Episode 25500: reward=-1, steps=3, speed=37.7 f/s, elapsed=1:14:26\n",
      "Episode 25600: reward=-1, steps=9, speed=37.7 f/s, elapsed=1:14:41\n",
      "Episode 25700: reward=-0, steps=3, speed=37.7 f/s, elapsed=1:14:58\n",
      "Episode 25800: reward=-0, steps=5, speed=37.7 f/s, elapsed=1:15:15\n",
      "Episode 25900: reward=-1, steps=3, speed=37.7 f/s, elapsed=1:15:32\n",
      "Episode 26000: reward=-0, steps=7, speed=37.7 f/s, elapsed=1:15:48\n",
      "Episode 26100: reward=-1, steps=8, speed=37.7 f/s, elapsed=1:16:05\n",
      "Episode 26200: reward=-0, steps=4, speed=37.7 f/s, elapsed=1:16:22\n",
      "Episode 26300: reward=1, steps=5, speed=37.7 f/s, elapsed=1:16:37\n",
      "Episode 26400: reward=-0, steps=8, speed=37.8 f/s, elapsed=1:16:52\n",
      "160000: tst: {'episode_reward': -0.1607416811079923, 'episode_steps': 172.08, 'order_profits': -0.17080669175324834, 'order_steps': 154.06}\n",
      "160000: val: {'episode_reward': -0.17356334500670215, 'episode_steps': 160.96, 'order_profits': -0.20659837970916725, 'order_steps': 148.84}\n",
      "Episode 26500: reward=-0, steps=5, speed=37.2 f/s, elapsed=1:17:44\n",
      "Episode 26600: reward=0, steps=3, speed=37.2 f/s, elapsed=1:18:02\n",
      "Episode 26700: reward=-1, steps=3, speed=37.3 f/s, elapsed=1:18:17\n",
      "Episode 26800: reward=-1, steps=23, speed=37.3 f/s, elapsed=1:18:34\n",
      "Episode 26900: reward=0, steps=5, speed=37.3 f/s, elapsed=1:18:53\n",
      "Episode 27000: reward=-0, steps=6, speed=37.4 f/s, elapsed=1:19:10\n",
      "Episode 27100: reward=-0, steps=7, speed=37.4 f/s, elapsed=1:19:24\n",
      "Episode 27200: reward=0, steps=9, speed=37.5 f/s, elapsed=1:19:40\n",
      "Episode 27300: reward=0, steps=13, speed=37.6 f/s, elapsed=1:19:56\n",
      "Episode 27400: reward=-0, steps=7, speed=37.6 f/s, elapsed=1:20:13\n",
      "Episode 27500: reward=-1, steps=17, speed=37.6 f/s, elapsed=1:20:29\n",
      "Episode 27600: reward=1, steps=14, speed=37.6 f/s, elapsed=1:20:46\n",
      "Episode 27700: reward=-1, steps=7, speed=37.7 f/s, elapsed=1:21:04\n",
      "Episode 27800: reward=-0, steps=5, speed=37.7 f/s, elapsed=1:21:18\n",
      "Episode 27900: reward=-0, steps=15, speed=37.7 f/s, elapsed=1:21:35\n",
      "Episode 28000: reward=-0, steps=7, speed=37.7 f/s, elapsed=1:21:51\n",
      "170000: tst: {'episode_reward': 0.046778992120963696, 'episode_steps': 92.34, 'order_profits': 0.03942285056020245, 'order_steps': 83.48}\n",
      "170000: val: {'episode_reward': 0.10531358443133856, 'episode_steps': 45.72, 'order_profits': 0.10083609757807858, 'order_steps': 38.98}\n",
      "Episode 28100: reward=-0, steps=4, speed=37.4 f/s, elapsed=1:22:23\n",
      "Episode 28200: reward=-0, steps=4, speed=37.4 f/s, elapsed=1:22:38\n",
      "Episode 28300: reward=-0, steps=4, speed=37.5 f/s, elapsed=1:22:53\n",
      "Episode 28400: reward=-0, steps=8, speed=37.4 f/s, elapsed=1:23:07\n",
      "Episode 28500: reward=-0, steps=6, speed=37.3 f/s, elapsed=1:23:26\n",
      "Episode 28600: reward=-0, steps=12, speed=37.3 f/s, elapsed=1:23:45\n",
      "Episode 28700: reward=-0, steps=5, speed=37.2 f/s, elapsed=1:24:07\n",
      "Episode 28800: reward=-0, steps=4, speed=37.1 f/s, elapsed=1:24:25\n",
      "Episode 28900: reward=-0, steps=12, speed=37.1 f/s, elapsed=1:24:43\n",
      "Episode 29000: reward=-1, steps=8, speed=37.0 f/s, elapsed=1:25:03\n",
      "Episode 29100: reward=-0, steps=23, speed=37.0 f/s, elapsed=1:25:23\n",
      "Episode 29200: reward=-1, steps=4, speed=36.9 f/s, elapsed=1:25:42\n",
      "Episode 29300: reward=0, steps=2, speed=36.8 f/s, elapsed=1:26:02\n",
      "Episode 29400: reward=0, steps=11, speed=36.7 f/s, elapsed=1:26:25\n",
      "Episode 29500: reward=-0, steps=9, speed=36.6 f/s, elapsed=1:26:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180000: tst: {'episode_reward': -0.09321567300533565, 'episode_steps': 38.84, 'order_profits': -0.09572511615879524, 'order_steps': 32.57}\n",
      "180000: val: {'episode_reward': 0.09456477651961395, 'episode_steps': 22.66, 'order_profits': 0.0905417472755056, 'order_steps': 18.96}\n",
      "Episode 29600: reward=-1, steps=5, speed=36.4 f/s, elapsed=1:27:12\n",
      "Episode 29700: reward=-0, steps=7, speed=36.3 f/s, elapsed=1:27:32\n",
      "Episode 29800: reward=-0, steps=10, speed=36.3 f/s, elapsed=1:27:49\n",
      "Episode 29900: reward=-1, steps=16, speed=36.3 f/s, elapsed=1:28:08\n",
      "Episode 30000: reward=-0, steps=12, speed=36.3 f/s, elapsed=1:28:25\n",
      "Episode 30100: reward=-0, steps=2, speed=36.2 f/s, elapsed=1:28:45\n",
      "Episode 30200: reward=-0, steps=5, speed=36.1 f/s, elapsed=1:29:05\n",
      "Episode 30300: reward=-0, steps=7, speed=36.1 f/s, elapsed=1:29:27\n",
      "Episode 30400: reward=-0, steps=4, speed=36.1 f/s, elapsed=1:29:45\n",
      "Episode 30500: reward=-1, steps=5, speed=36.1 f/s, elapsed=1:30:03\n",
      "Episode 30600: reward=-0, steps=11, speed=36.0 f/s, elapsed=1:30:23\n",
      "Episode 30700: reward=-0, steps=4, speed=36.0 f/s, elapsed=1:30:41\n",
      "Episode 30800: reward=-0, steps=7, speed=36.0 f/s, elapsed=1:31:03\n",
      "Episode 30900: reward=-0, steps=11, speed=36.0 f/s, elapsed=1:31:22\n",
      "Episode 31000: reward=-2, steps=4, speed=35.9 f/s, elapsed=1:31:41\n",
      "190000: tst: {'episode_reward': -0.132092969196768, 'episode_steps': 48.7, 'order_profits': -0.13131156235213906, 'order_steps': 17.57}\n",
      "190000: val: {'episode_reward': 0.14722885341545042, 'episode_steps': 28.37, 'order_profits': 0.14308289947873123, 'order_steps': 10.2}\n",
      "Episode 31100: reward=-0, steps=9, speed=35.7 f/s, elapsed=1:32:09\n",
      "Episode 31200: reward=-1, steps=13, speed=35.6 f/s, elapsed=1:32:30\n",
      "Episode 31300: reward=-1, steps=10, speed=35.6 f/s, elapsed=1:32:49\n",
      "Episode 31400: reward=-0, steps=11, speed=35.5 f/s, elapsed=1:33:09\n",
      "Episode 31500: reward=-0, steps=3, speed=35.4 f/s, elapsed=1:33:30\n",
      "Episode 31600: reward=-0, steps=5, speed=35.4 f/s, elapsed=1:33:48\n",
      "Episode 31700: reward=0, steps=8, speed=35.4 f/s, elapsed=1:34:07\n",
      "Episode 31800: reward=0, steps=8, speed=35.2 f/s, elapsed=1:34:33\n",
      "Episode 31900: reward=-0, steps=5, speed=35.1 f/s, elapsed=1:34:55\n",
      "Episode 32000: reward=-0, steps=3, speed=35.0 f/s, elapsed=1:35:13\n",
      "Episode 32100: reward=-0, steps=4, speed=35.0 f/s, elapsed=1:35:33\n",
      "Episode 32200: reward=-0, steps=11, speed=34.9 f/s, elapsed=1:35:53\n",
      "Episode 32300: reward=-1, steps=10, speed=34.9 f/s, elapsed=1:36:13\n",
      "Episode 32400: reward=-0, steps=4, speed=34.9 f/s, elapsed=1:36:32\n",
      "Episode 32500: reward=0, steps=11, speed=34.9 f/s, elapsed=1:36:49\n",
      "Episode 32600: reward=-0, steps=5, speed=34.9 f/s, elapsed=1:37:06\n",
      "200000: tst: {'episode_reward': 0.0162979369946596, 'episode_steps': 17.45, 'order_profits': 0.0137733358276969, 'order_steps': 13.83}\n",
      "200000: val: {'episode_reward': 0.04113506471546177, 'episode_steps': 10.96, 'order_profits': 0.039155907045010346, 'order_steps': 7.74}\n",
      "Episode 32700: reward=-0, steps=5, speed=34.9 f/s, elapsed=1:37:28\n",
      "Episode 32800: reward=-0, steps=7, speed=34.9 f/s, elapsed=1:37:47\n",
      "Episode 32900: reward=-0, steps=15, speed=34.9 f/s, elapsed=1:38:06\n",
      "Episode 33000: reward=-0, steps=7, speed=34.9 f/s, elapsed=1:38:23\n",
      "Episode 33100: reward=0, steps=3, speed=35.0 f/s, elapsed=1:38:41\n",
      "Episode 33200: reward=-1, steps=21, speed=35.0 f/s, elapsed=1:38:58\n",
      "Episode 33300: reward=-0, steps=6, speed=35.1 f/s, elapsed=1:39:16\n",
      "Episode 33400: reward=0, steps=11, speed=35.1 f/s, elapsed=1:39:32\n",
      "Episode 33500: reward=-1, steps=4, speed=35.2 f/s, elapsed=1:39:51\n",
      "Episode 33600: reward=-0, steps=3, speed=35.2 f/s, elapsed=1:40:08\n",
      "Episode 33700: reward=-0, steps=4, speed=35.3 f/s, elapsed=1:40:25\n",
      "Episode 33800: reward=-0, steps=4, speed=35.2 f/s, elapsed=1:40:46\n",
      "Episode 33900: reward=-1, steps=7, speed=35.2 f/s, elapsed=1:41:02\n",
      "Episode 34000: reward=-1, steps=5, speed=35.2 f/s, elapsed=1:41:21\n",
      "Episode 34100: reward=-0, steps=4, speed=35.2 f/s, elapsed=1:41:41\n",
      "210000: tst: {'episode_reward': -0.08472454168560006, 'episode_steps': 11.48, 'order_profits': -0.08566934019054707, 'order_steps': 4.78}\n",
      "210000: val: {'episode_reward': -0.03550968554932914, 'episode_steps': 9.17, 'order_profits': -0.03712440042623357, 'order_steps': 4.76}\n",
      "Episode 34200: reward=0, steps=12, speed=35.2 f/s, elapsed=1:42:01\n",
      "Episode 34300: reward=-0, steps=6, speed=35.2 f/s, elapsed=1:42:20\n",
      "Episode 34400: reward=-0, steps=7, speed=35.3 f/s, elapsed=1:42:38\n",
      "Episode 34500: reward=-0, steps=2, speed=35.3 f/s, elapsed=1:42:57\n",
      "Episode 34600: reward=-0, steps=9, speed=35.4 f/s, elapsed=1:43:15\n",
      "Episode 34700: reward=-0, steps=6, speed=35.4 f/s, elapsed=1:43:32\n",
      "Episode 34800: reward=-0, steps=6, speed=35.4 f/s, elapsed=1:43:47\n",
      "Episode 34900: reward=-0, steps=5, speed=35.5 f/s, elapsed=1:44:05\n",
      "Episode 35000: reward=0, steps=6, speed=35.5 f/s, elapsed=1:44:22\n",
      "Episode 35100: reward=0, steps=6, speed=35.6 f/s, elapsed=1:44:41\n",
      "Episode 35200: reward=-0, steps=8, speed=35.5 f/s, elapsed=1:45:02\n",
      "Episode 35300: reward=-0, steps=9, speed=35.5 f/s, elapsed=1:45:19\n",
      "Episode 35400: reward=-0, steps=6, speed=35.5 f/s, elapsed=1:45:38\n",
      "Episode 35500: reward=-0, steps=3, speed=35.5 f/s, elapsed=1:45:56\n",
      "Episode 35600: reward=-1, steps=7, speed=35.6 f/s, elapsed=1:46:12\n",
      "220000: tst: {'episode_reward': -0.12156945356244923, 'episode_steps': 12.96, 'order_profits': -0.12231952132199736, 'order_steps': 8.4}\n",
      "220000: val: {'episode_reward': -0.02004936622824432, 'episode_steps': 8.3, 'order_profits': -0.021291251842792142, 'order_steps': 5.24}\n",
      "Episode 35700: reward=0, steps=10, speed=35.5 f/s, elapsed=1:46:34\n",
      "Episode 35800: reward=-0, steps=2, speed=35.6 f/s, elapsed=1:46:54\n",
      "Episode 35900: reward=-0, steps=15, speed=35.6 f/s, elapsed=1:47:13\n",
      "Episode 36000: reward=0, steps=7, speed=35.7 f/s, elapsed=1:47:29\n",
      "Episode 36100: reward=0, steps=9, speed=35.6 f/s, elapsed=1:47:47\n",
      "Episode 36200: reward=-0, steps=6, speed=35.5 f/s, elapsed=1:48:11\n",
      "Episode 36300: reward=0, steps=3, speed=35.5 f/s, elapsed=1:48:31\n",
      "Episode 36400: reward=-0, steps=7, speed=35.3 f/s, elapsed=1:48:56\n",
      "Episode 36500: reward=-0, steps=2, speed=35.3 f/s, elapsed=1:49:13\n",
      "Episode 36600: reward=-0, steps=3, speed=35.3 f/s, elapsed=1:49:32\n",
      "Episode 36700: reward=-0, steps=4, speed=35.3 f/s, elapsed=1:49:51\n",
      "Episode 36800: reward=0, steps=5, speed=35.4 f/s, elapsed=1:50:08\n",
      "Episode 36900: reward=-0, steps=25, speed=35.4 f/s, elapsed=1:50:27\n",
      "Episode 37000: reward=-0, steps=3, speed=35.5 f/s, elapsed=1:50:46\n",
      "Episode 37100: reward=0, steps=8, speed=35.5 f/s, elapsed=1:51:03\n",
      "230000: tst: {'episode_reward': -0.09202699261785131, 'episode_steps': 11.68, 'order_profits': -0.09238707959350072, 'order_steps': 2.2}\n",
      "230000: val: {'episode_reward': -0.0897274560124815, 'episode_steps': 7.93, 'order_profits': -0.09008563285600721, 'order_steps': 2.33}\n",
      "Episode 37200: reward=-1, steps=7, speed=35.4 f/s, elapsed=1:51:23\n",
      "Episode 37300: reward=-0, steps=3, speed=35.3 f/s, elapsed=1:51:40\n",
      "Episode 37400: reward=-0, steps=9, speed=35.4 f/s, elapsed=1:51:57\n",
      "Episode 37500: reward=0, steps=2, speed=35.4 f/s, elapsed=1:52:15\n",
      "Episode 37600: reward=-0, steps=2, speed=35.4 f/s, elapsed=1:52:31\n",
      "Episode 37700: reward=0, steps=6, speed=35.5 f/s, elapsed=1:52:49\n",
      "Episode 37800: reward=-0, steps=10, speed=35.5 f/s, elapsed=1:53:07\n",
      "Episode 37900: reward=-0, steps=2, speed=35.4 f/s, elapsed=1:53:26\n",
      "Episode 38000: reward=-0, steps=2, speed=35.4 f/s, elapsed=1:53:47\n",
      "Episode 38100: reward=0, steps=12, speed=35.1 f/s, elapsed=1:54:17\n",
      "Episode 38200: reward=-0, steps=2, speed=35.0 f/s, elapsed=1:54:39\n",
      "Episode 38300: reward=-0, steps=11, speed=35.0 f/s, elapsed=1:54:58\n",
      "Episode 38400: reward=-0, steps=4, speed=35.0 f/s, elapsed=1:55:18\n",
      "Episode 38500: reward=-1, steps=16, speed=35.1 f/s, elapsed=1:55:36\n",
      "Episode 38600: reward=-0, steps=3, speed=35.2 f/s, elapsed=1:55:53\n",
      "240000: tst: {'episode_reward': -0.04055206738539579, 'episode_steps': 10.96, 'order_profits': -0.04111936574549846, 'order_steps': 2.93}\n",
      "240000: val: {'episode_reward': -0.07134522938168023, 'episode_steps': 9.66, 'order_profits': -0.07201974007413929, 'order_steps': 3.25}\n",
      "Episode 38700: reward=-1, steps=7, speed=35.2 f/s, elapsed=1:56:13\n",
      "Episode 38800: reward=-0, steps=7, speed=35.2 f/s, elapsed=1:56:32\n",
      "Episode 38900: reward=-0, steps=4, speed=35.2 f/s, elapsed=1:56:50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 39000: reward=-0, steps=4, speed=35.3 f/s, elapsed=1:57:08\n",
      "Episode 39100: reward=-0, steps=9, speed=35.3 f/s, elapsed=1:57:26\n",
      "Episode 39200: reward=-0, steps=6, speed=35.3 f/s, elapsed=1:57:45\n",
      "Episode 39300: reward=0, steps=3, speed=35.3 f/s, elapsed=1:58:04\n",
      "Episode 39400: reward=-0, steps=7, speed=35.2 f/s, elapsed=1:58:22\n",
      "Episode 39500: reward=-0, steps=4, speed=35.0 f/s, elapsed=1:58:48\n",
      "Episode 39600: reward=-0, steps=2, speed=35.0 f/s, elapsed=1:59:06\n",
      "Episode 39700: reward=-0, steps=2, speed=35.0 f/s, elapsed=1:59:24\n",
      "Episode 39800: reward=0, steps=3, speed=35.0 f/s, elapsed=1:59:40\n",
      "Episode 39900: reward=-0, steps=6, speed=35.0 f/s, elapsed=1:59:58\n",
      "Episode 40000: reward=-0, steps=8, speed=35.1 f/s, elapsed=2:00:16\n",
      "Episode 40100: reward=0, steps=11, speed=35.1 f/s, elapsed=2:00:35\n",
      "Episode 40200: reward=-0, steps=4, speed=35.2 f/s, elapsed=2:00:53\n",
      "250000: tst: {'episode_reward': -0.07425977188820414, 'episode_steps': 12.21, 'order_profits': -0.07460277689730918, 'order_steps': 5.78}\n",
      "250000: val: {'episode_reward': -0.1078186620497707, 'episode_steps': 8.78, 'order_profits': -0.10879660288761368, 'order_steps': 4.69}\n",
      "Episode 40300: reward=-1, steps=5, speed=35.1 f/s, elapsed=2:01:18\n",
      "Episode 40400: reward=-0, steps=4, speed=35.0 f/s, elapsed=2:01:36\n",
      "Episode 40500: reward=0, steps=3, speed=35.0 f/s, elapsed=2:01:55\n",
      "Episode 40600: reward=-0, steps=2, speed=35.1 f/s, elapsed=2:02:12\n",
      "Episode 40700: reward=0, steps=4, speed=35.2 f/s, elapsed=2:02:28\n",
      "Episode 40800: reward=-0, steps=7, speed=35.2 f/s, elapsed=2:02:47\n",
      "Episode 40900: reward=-0, steps=10, speed=35.2 f/s, elapsed=2:03:07\n",
      "Episode 41000: reward=-0, steps=4, speed=35.3 f/s, elapsed=2:03:26\n",
      "Episode 41100: reward=-1, steps=6, speed=35.3 f/s, elapsed=2:03:43\n",
      "Episode 41200: reward=-0, steps=6, speed=35.3 f/s, elapsed=2:03:59\n",
      "Episode 41300: reward=-0, steps=5, speed=35.3 f/s, elapsed=2:04:20\n",
      "Episode 41400: reward=-1, steps=3, speed=35.3 f/s, elapsed=2:04:38\n",
      "Episode 41500: reward=-0, steps=4, speed=35.4 f/s, elapsed=2:04:54\n",
      "Episode 41600: reward=-0, steps=6, speed=35.4 f/s, elapsed=2:05:12\n",
      "Episode 41700: reward=-0, steps=4, speed=35.5 f/s, elapsed=2:05:27\n",
      "260000: tst: {'episode_reward': -0.08525701575023507, 'episode_steps': 11.87, 'order_profits': -0.08606456517252717, 'order_steps': 8.3}\n",
      "260000: val: {'episode_reward': -0.013294333428001414, 'episode_steps': 6.56, 'order_profits': -0.014130607459454754, 'order_steps': 4.33}\n",
      "Episode 41800: reward=-0, steps=7, speed=35.4 f/s, elapsed=2:05:49\n",
      "Episode 41900: reward=-0, steps=8, speed=35.4 f/s, elapsed=2:06:07\n",
      "Episode 42000: reward=0, steps=4, speed=35.4 f/s, elapsed=2:06:25\n",
      "Episode 42100: reward=0, steps=4, speed=35.5 f/s, elapsed=2:06:42\n",
      "Episode 42200: reward=0, steps=15, speed=35.6 f/s, elapsed=2:06:58\n",
      "Episode 42300: reward=-0, steps=11, speed=35.7 f/s, elapsed=2:07:18\n",
      "Episode 42400: reward=-1, steps=2, speed=35.7 f/s, elapsed=2:07:35\n",
      "Episode 42500: reward=-0, steps=8, speed=35.8 f/s, elapsed=2:07:50\n",
      "Episode 42600: reward=-0, steps=5, speed=35.9 f/s, elapsed=2:08:09\n",
      "Episode 42700: reward=0, steps=3, speed=36.0 f/s, elapsed=2:08:26\n",
      "Episode 42800: reward=-0, steps=7, speed=36.1 f/s, elapsed=2:08:43\n",
      "Episode 42900: reward=-0, steps=10, speed=36.1 f/s, elapsed=2:09:00\n",
      "Episode 43000: reward=-0, steps=7, speed=36.2 f/s, elapsed=2:09:18\n",
      "Episode 43100: reward=-0, steps=8, speed=36.3 f/s, elapsed=2:09:34\n",
      "Episode 43200: reward=0, steps=7, speed=36.4 f/s, elapsed=2:09:51\n",
      "270000: tst: {'episode_reward': -0.09775831930311465, 'episode_steps': 9.41, 'order_profits': -0.09812576601539438, 'order_steps': 2.81}\n",
      "270000: val: {'episode_reward': -0.11435536445990961, 'episode_steps': 7.63, 'order_profits': -0.11459114627179165, 'order_steps': 2.07}\n",
      "Episode 43300: reward=0, steps=4, speed=36.4 f/s, elapsed=2:10:08\n",
      "Episode 43400: reward=-0, steps=5, speed=36.4 f/s, elapsed=2:10:25\n",
      "Episode 43500: reward=-0, steps=7, speed=36.5 f/s, elapsed=2:10:42\n",
      "Episode 43600: reward=-0, steps=7, speed=36.6 f/s, elapsed=2:10:58\n",
      "Episode 43700: reward=-0, steps=5, speed=36.6 f/s, elapsed=2:11:14\n",
      "Episode 43800: reward=-1, steps=4, speed=36.7 f/s, elapsed=2:11:31\n",
      "Episode 43900: reward=-0, steps=4, speed=36.8 f/s, elapsed=2:11:48\n",
      "Episode 44000: reward=-0, steps=4, speed=36.8 f/s, elapsed=2:12:02\n",
      "Episode 44100: reward=-1, steps=8, speed=36.9 f/s, elapsed=2:12:18\n",
      "Episode 44200: reward=-1, steps=4, speed=37.0 f/s, elapsed=2:12:35\n",
      "Episode 44300: reward=-0, steps=6, speed=37.0 f/s, elapsed=2:12:51\n",
      "Episode 44400: reward=-0, steps=4, speed=37.1 f/s, elapsed=2:13:07\n",
      "Episode 44500: reward=-0, steps=3, speed=37.2 f/s, elapsed=2:13:22\n",
      "Episode 44600: reward=-0, steps=11, speed=37.2 f/s, elapsed=2:13:39\n",
      "Episode 44700: reward=-0, steps=5, speed=37.3 f/s, elapsed=2:13:56\n",
      "280000: tst: {'episode_reward': -0.12915847507700834, 'episode_steps': 10.45, 'order_profits': -0.12933314896363818, 'order_steps': 3.77}\n",
      "280000: val: {'episode_reward': 0.031045232236752006, 'episode_steps': 6.04, 'order_profits': 0.03055761120925623, 'order_steps': 2.34}\n",
      "Episode 44800: reward=-2, steps=20, speed=37.2 f/s, elapsed=2:14:13\n",
      "Episode 44900: reward=-0, steps=9, speed=37.3 f/s, elapsed=2:14:29\n",
      "Episode 45000: reward=-0, steps=7, speed=37.4 f/s, elapsed=2:14:44\n",
      "Episode 45100: reward=-0, steps=8, speed=37.4 f/s, elapsed=2:15:00\n",
      "Episode 45200: reward=0, steps=3, speed=37.5 f/s, elapsed=2:15:16\n",
      "Episode 45300: reward=-0, steps=4, speed=37.5 f/s, elapsed=2:15:31\n",
      "Episode 45400: reward=-0, steps=6, speed=37.6 f/s, elapsed=2:15:46\n",
      "Episode 45500: reward=0, steps=18, speed=37.6 f/s, elapsed=2:16:00\n",
      "Episode 45600: reward=-0, steps=2, speed=37.7 f/s, elapsed=2:16:16\n",
      "Episode 45700: reward=0, steps=3, speed=37.8 f/s, elapsed=2:16:32\n",
      "Episode 45800: reward=-0, steps=11, speed=37.8 f/s, elapsed=2:16:49\n",
      "Episode 45900: reward=-0, steps=7, speed=37.8 f/s, elapsed=2:17:05\n",
      "Episode 46000: reward=-1, steps=12, speed=37.9 f/s, elapsed=2:17:19\n",
      "Episode 46100: reward=-0, steps=4, speed=37.9 f/s, elapsed=2:17:35\n",
      "Episode 46200: reward=-0, steps=4, speed=38.0 f/s, elapsed=2:17:49\n",
      "Episode 46300: reward=-0, steps=2, speed=38.0 f/s, elapsed=2:18:05\n",
      "290000: tst: {'episode_reward': -0.004807798715421745, 'episode_steps': 10.96, 'order_profits': -0.0056764062243255566, 'order_steps': 5.97}\n",
      "290000: val: {'episode_reward': -0.019235564086148838, 'episode_steps': 8.72, 'order_profits': -0.02079318607950415, 'order_steps': 4.25}\n",
      "Episode 46400: reward=-0, steps=5, speed=37.9 f/s, elapsed=2:25:39\n",
      "Episode 46500: reward=-0, steps=8, speed=37.8 f/s, elapsed=2:25:56\n",
      "Episode 46600: reward=-0, steps=15, speed=37.8 f/s, elapsed=2:26:18\n",
      "Episode 46700: reward=-1, steps=11, speed=37.7 f/s, elapsed=2:26:37\n",
      "Episode 46800: reward=-0, steps=8, speed=37.7 f/s, elapsed=2:26:55\n",
      "Episode 46900: reward=0, steps=7, speed=37.7 f/s, elapsed=2:27:12\n",
      "Episode 47000: reward=-0, steps=9, speed=37.7 f/s, elapsed=2:27:30\n",
      "Episode 47100: reward=-0, steps=3, speed=37.6 f/s, elapsed=2:27:49\n",
      "Episode 47200: reward=-0, steps=7, speed=37.5 f/s, elapsed=2:28:09\n",
      "Episode 47300: reward=-1, steps=6, speed=37.4 f/s, elapsed=2:28:32\n",
      "Episode 47400: reward=-0, steps=10, speed=37.2 f/s, elapsed=2:29:03\n",
      "Episode 47500: reward=-0, steps=3, speed=37.2 f/s, elapsed=2:29:24\n",
      "Episode 47600: reward=-0, steps=4, speed=37.2 f/s, elapsed=2:29:43\n",
      "Episode 47700: reward=-0, steps=3, speed=37.1 f/s, elapsed=2:30:03\n",
      "Episode 47800: reward=-0, steps=4, speed=37.1 f/s, elapsed=2:30:20\n",
      "300000: tst: {'episode_reward': -0.0508037867943562, 'episode_steps': 27.13, 'order_profits': -0.05131452156109173, 'order_steps': 5.34}\n",
      "300000: val: {'episode_reward': 0.005103483079578386, 'episode_steps': 16.01, 'order_profits': 0.004229207818934565, 'order_steps': 3.86}\n",
      "Episode 47900: reward=0, steps=31, speed=37.0 f/s, elapsed=2:30:43\n",
      "Episode 48000: reward=-0, steps=11, speed=36.9 f/s, elapsed=2:31:04\n",
      "Episode 48100: reward=0, steps=3, speed=36.9 f/s, elapsed=2:31:24\n",
      "Episode 48200: reward=0, steps=5, speed=36.9 f/s, elapsed=2:31:44\n",
      "Episode 48300: reward=0, steps=10, speed=36.9 f/s, elapsed=2:32:03\n",
      "Episode 48400: reward=-0, steps=11, speed=36.9 f/s, elapsed=2:32:19\n",
      "Episode 48500: reward=-0, steps=6, speed=36.9 f/s, elapsed=2:32:38\n",
      "Episode 48600: reward=2, steps=2, speed=36.8 f/s, elapsed=2:32:55\n",
      "Episode 48700: reward=1, steps=7, speed=36.8 f/s, elapsed=2:33:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 48800: reward=-0, steps=5, speed=36.8 f/s, elapsed=2:33:32\n",
      "Episode 48900: reward=-0, steps=9, speed=36.8 f/s, elapsed=2:33:51\n",
      "Episode 49000: reward=-0, steps=9, speed=36.8 f/s, elapsed=2:34:08\n",
      "Episode 49100: reward=0, steps=12, speed=36.8 f/s, elapsed=2:34:27\n",
      "Episode 49200: reward=-1, steps=4, speed=36.8 f/s, elapsed=2:34:48\n",
      "310000: tst: {'episode_reward': 0.03842843471559472, 'episode_steps': 28.78, 'order_profits': 0.03542893470790201, 'order_steps': 21.82}\n",
      "310000: val: {'episode_reward': -0.056307926657165164, 'episode_steps': 17.69, 'order_profits': -0.05828946025275457, 'order_steps': 13.5}\n",
      "Episode 49300: reward=-0, steps=4, speed=36.6 f/s, elapsed=2:35:14\n",
      "Episode 49400: reward=0, steps=6, speed=36.6 f/s, elapsed=2:35:35\n",
      "Episode 49500: reward=-0, steps=2, speed=36.6 f/s, elapsed=2:35:55\n",
      "Episode 49600: reward=0, steps=11, speed=36.5 f/s, elapsed=2:36:15\n",
      "Episode 49700: reward=-1, steps=4, speed=36.5 f/s, elapsed=2:36:35\n",
      "Episode 49800: reward=-0, steps=3, speed=36.5 f/s, elapsed=2:36:55\n",
      "Episode 49900: reward=-0, steps=9, speed=36.5 f/s, elapsed=2:37:13\n",
      "Episode 50000: reward=-0, steps=5, speed=36.4 f/s, elapsed=2:37:34\n",
      "Episode 50100: reward=-1, steps=4, speed=36.3 f/s, elapsed=2:37:57\n",
      "Episode 50200: reward=0, steps=3, speed=36.3 f/s, elapsed=2:38:16\n",
      "Episode 50300: reward=0, steps=5, speed=36.2 f/s, elapsed=2:38:34\n",
      "Episode 50400: reward=-0, steps=8, speed=36.2 f/s, elapsed=2:38:52\n",
      "Episode 50500: reward=-0, steps=4, speed=36.2 f/s, elapsed=2:39:11\n",
      "Episode 50600: reward=-0, steps=3, speed=36.2 f/s, elapsed=2:39:29\n",
      "Episode 50700: reward=0, steps=3, speed=36.2 f/s, elapsed=2:39:48\n",
      "320000: tst: {'episode_reward': -0.03181479832425897, 'episode_steps': 11.84, 'order_profits': -0.03231372659899621, 'order_steps': 2.58}\n",
      "320000: val: {'episode_reward': -0.00956600094837781, 'episode_steps': 8.59, 'order_profits': -0.010109104095060046, 'order_steps': 2.5}\n",
      "Episode 50800: reward=0, steps=14, speed=36.1 f/s, elapsed=2:40:09\n",
      "Episode 50900: reward=0, steps=2, speed=36.0 f/s, elapsed=2:40:28\n",
      "Episode 51000: reward=0, steps=6, speed=36.0 f/s, elapsed=2:40:47\n",
      "Episode 51100: reward=-0, steps=6, speed=36.1 f/s, elapsed=2:41:04\n",
      "Episode 51200: reward=-0, steps=5, speed=36.1 f/s, elapsed=2:41:22\n",
      "Episode 51300: reward=-0, steps=12, speed=36.1 f/s, elapsed=2:41:41\n",
      "Episode 51400: reward=-0, steps=9, speed=36.1 f/s, elapsed=2:42:03\n",
      "Episode 51500: reward=-0, steps=4, speed=36.0 f/s, elapsed=2:42:26\n",
      "Episode 51600: reward=-0, steps=17, speed=35.9 f/s, elapsed=2:42:49\n",
      "Episode 51700: reward=0, steps=8, speed=35.8 f/s, elapsed=2:43:08\n",
      "Episode 51800: reward=0, steps=6, speed=35.8 f/s, elapsed=2:43:28\n",
      "Episode 51900: reward=-0, steps=6, speed=35.8 f/s, elapsed=2:43:47\n",
      "Episode 52000: reward=-0, steps=15, speed=35.7 f/s, elapsed=2:44:07\n",
      "Episode 52100: reward=0, steps=7, speed=35.5 f/s, elapsed=2:44:37\n",
      "Episode 52200: reward=-0, steps=2, speed=35.3 f/s, elapsed=2:45:05\n",
      "330000: tst: {'episode_reward': -0.03231139956863775, 'episode_steps': 13.64, 'order_profits': -0.033140247233215424, 'order_steps': 5.45}\n",
      "330000: val: {'episode_reward': 0.010026286606531867, 'episode_steps': 9.06, 'order_profits': 0.009486220036294346, 'order_steps': 3.6}\n",
      "Episode 52300: reward=0, steps=2, speed=35.0 f/s, elapsed=2:45:37\n",
      "Episode 52400: reward=-0, steps=12, speed=35.0 f/s, elapsed=2:45:58\n",
      "Episode 52500: reward=-0, steps=2, speed=34.8 f/s, elapsed=2:46:25\n",
      "Episode 52600: reward=-0, steps=9, speed=34.7 f/s, elapsed=2:46:46\n",
      "Episode 52700: reward=-0, steps=4, speed=34.7 f/s, elapsed=2:47:04\n",
      "Episode 52800: reward=-0, steps=8, speed=34.7 f/s, elapsed=2:47:23\n",
      "Episode 52900: reward=-0, steps=8, speed=34.6 f/s, elapsed=2:47:45\n",
      "Episode 53000: reward=1, steps=6, speed=34.6 f/s, elapsed=2:48:05\n",
      "Episode 53100: reward=-1, steps=3, speed=34.4 f/s, elapsed=2:48:32\n",
      "Episode 53200: reward=-0, steps=2, speed=34.3 f/s, elapsed=2:48:56\n",
      "Episode 53300: reward=-0, steps=6, speed=34.1 f/s, elapsed=2:49:23\n",
      "Episode 53400: reward=-0, steps=19, speed=34.1 f/s, elapsed=2:49:46\n",
      "Episode 53500: reward=-0, steps=2, speed=34.1 f/s, elapsed=2:50:07\n",
      "Episode 53600: reward=-0, steps=2, speed=34.0 f/s, elapsed=2:50:28\n",
      "340000: tst: {'episode_reward': -0.07813267286985787, 'episode_steps': 12.9, 'order_profits': -0.07861482473809893, 'order_steps': 4.9}\n",
      "340000: val: {'episode_reward': -0.1124721584491402, 'episode_steps': 9.3, 'order_profits': -0.1127118105663498, 'order_steps': 3.85}\n",
      "Episode 53700: reward=-0, steps=4, speed=34.0 f/s, elapsed=2:50:51\n",
      "Episode 53800: reward=-0, steps=9, speed=33.9 f/s, elapsed=2:51:14\n",
      "Episode 53900: reward=-0, steps=12, speed=33.8 f/s, elapsed=2:51:42\n",
      "Episode 54000: reward=-0, steps=4, speed=33.8 f/s, elapsed=2:52:03\n",
      "Episode 54100: reward=-0, steps=11, speed=33.8 f/s, elapsed=2:52:19\n",
      "Episode 54200: reward=-0, steps=2, speed=33.8 f/s, elapsed=2:52:44\n",
      "Episode 54300: reward=-0, steps=4, speed=33.7 f/s, elapsed=2:53:06\n",
      "Episode 54400: reward=0, steps=6, speed=33.6 f/s, elapsed=2:53:27\n",
      "Episode 54500: reward=0, steps=13, speed=33.6 f/s, elapsed=2:53:47\n",
      "Episode 54600: reward=-0, steps=5, speed=33.7 f/s, elapsed=2:54:04\n",
      "Episode 54700: reward=-0, steps=7, speed=33.7 f/s, elapsed=2:54:22\n",
      "Episode 54800: reward=-0, steps=2, speed=33.7 f/s, elapsed=2:54:45\n",
      "Episode 54900: reward=-1, steps=8, speed=33.7 f/s, elapsed=2:55:05\n",
      "Episode 55000: reward=-1, steps=8, speed=33.7 f/s, elapsed=2:55:25\n",
      "Episode 55100: reward=-0, steps=17, speed=33.7 f/s, elapsed=2:55:49\n",
      "350000: tst: {'episode_reward': -0.16140403700054623, 'episode_steps': 15.04, 'order_profits': -0.1618934376551337, 'order_steps': 9.04}\n",
      "350000: val: {'episode_reward': -0.031963372440365116, 'episode_steps': 8.26, 'order_profits': -0.03262257257765961, 'order_steps': 4.6}\n",
      "Episode 55200: reward=-1, steps=9, speed=33.6 f/s, elapsed=2:56:13\n",
      "Episode 55300: reward=-0, steps=10, speed=33.7 f/s, elapsed=2:56:33\n",
      "Episode 55400: reward=-0, steps=9, speed=33.6 f/s, elapsed=2:56:54\n",
      "Episode 55500: reward=0, steps=6, speed=33.7 f/s, elapsed=2:57:17\n",
      "Episode 55600: reward=-0, steps=9, speed=33.7 f/s, elapsed=2:57:36\n",
      "Episode 55700: reward=-0, steps=2, speed=33.8 f/s, elapsed=2:57:52\n",
      "Episode 55800: reward=-0, steps=2, speed=33.8 f/s, elapsed=2:58:12\n",
      "Episode 55900: reward=-0, steps=14, speed=33.9 f/s, elapsed=2:58:31\n",
      "Episode 56000: reward=0, steps=5, speed=33.9 f/s, elapsed=2:58:51\n",
      "Episode 56100: reward=-0, steps=2, speed=33.9 f/s, elapsed=2:59:08\n",
      "Episode 56200: reward=-0, steps=4, speed=33.9 f/s, elapsed=2:59:24\n",
      "Episode 56300: reward=-1, steps=9, speed=34.0 f/s, elapsed=2:59:43\n",
      "Episode 56400: reward=-0, steps=6, speed=34.0 f/s, elapsed=3:00:04\n",
      "Episode 56500: reward=-1, steps=7, speed=34.0 f/s, elapsed=3:00:23\n",
      "360000: tst: {'episode_reward': -0.062307064182144434, 'episode_steps': 12.87, 'order_profits': -0.06345523455349333, 'order_steps': 5.31}\n",
      "360000: val: {'episode_reward': -0.038616395263663825, 'episode_steps': 8.03, 'order_profits': -0.03953894622589684, 'order_steps': 4.01}\n",
      "Episode 56600: reward=-1, steps=8, speed=33.9 f/s, elapsed=3:00:43\n",
      "Episode 56700: reward=-0, steps=7, speed=34.0 f/s, elapsed=3:01:02\n",
      "Episode 56800: reward=-1, steps=4, speed=34.0 f/s, elapsed=3:01:20\n",
      "Episode 56900: reward=-0, steps=3, speed=34.0 f/s, elapsed=3:01:38\n",
      "Episode 57000: reward=-0, steps=16, speed=34.0 f/s, elapsed=3:01:58\n",
      "Episode 57100: reward=-1, steps=12, speed=34.1 f/s, elapsed=3:02:18\n",
      "Episode 57200: reward=-1, steps=8, speed=34.1 f/s, elapsed=3:02:36\n",
      "Episode 57300: reward=-0, steps=2, speed=34.1 f/s, elapsed=3:02:57\n",
      "Episode 57400: reward=-2, steps=11, speed=34.2 f/s, elapsed=3:03:16\n",
      "Episode 57500: reward=-1, steps=7, speed=34.2 f/s, elapsed=3:03:37\n",
      "Episode 57600: reward=-0, steps=12, speed=34.2 f/s, elapsed=3:03:59\n",
      "Episode 57700: reward=-0, steps=5, speed=34.2 f/s, elapsed=3:04:20\n",
      "Episode 57800: reward=-0, steps=6, speed=34.2 f/s, elapsed=3:04:41\n",
      "Episode 57900: reward=-0, steps=10, speed=34.3 f/s, elapsed=3:05:02\n",
      "Episode 58000: reward=-0, steps=7, speed=34.3 f/s, elapsed=3:05:23\n",
      "370000: tst: {'episode_reward': -0.04696188549176809, 'episode_steps': 19.83, 'order_profits': -0.04715389614811553, 'order_steps': 2.05}\n",
      "370000: val: {'episode_reward': -0.12071857695095396, 'episode_steps': 12.93, 'order_profits': -0.12076312657503811, 'order_steps': 2.23}\n",
      "Episode 58100: reward=-1, steps=2, speed=34.2 f/s, elapsed=3:05:45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 58200: reward=-0, steps=4, speed=34.2 f/s, elapsed=3:06:06\n",
      "Episode 58300: reward=-0, steps=5, speed=34.2 f/s, elapsed=3:06:23\n",
      "Episode 58400: reward=-0, steps=2, speed=34.3 f/s, elapsed=3:06:42\n",
      "Episode 58500: reward=-0, steps=4, speed=34.3 f/s, elapsed=3:07:00\n",
      "Episode 58600: reward=2, steps=9, speed=34.3 f/s, elapsed=3:07:22\n",
      "Episode 58700: reward=-0, steps=4, speed=34.3 f/s, elapsed=3:07:42\n",
      "Episode 58800: reward=-0, steps=7, speed=34.3 f/s, elapsed=3:08:02\n",
      "Episode 58900: reward=0, steps=9, speed=34.4 f/s, elapsed=3:08:22\n",
      "Episode 59000: reward=-0, steps=6, speed=34.4 f/s, elapsed=3:08:44\n",
      "Episode 59100: reward=-1, steps=7, speed=34.4 f/s, elapsed=3:09:05\n",
      "Episode 59200: reward=0, steps=4, speed=34.4 f/s, elapsed=3:09:27\n",
      "Episode 59300: reward=-0, steps=2, speed=34.4 f/s, elapsed=3:09:49\n",
      "380000: tst: {'episode_reward': -0.11199564744554008, 'episode_steps': 19.03, 'order_profits': -0.11347955399122694, 'order_steps': 11.23}\n",
      "380000: val: {'episode_reward': -0.13742018577272447, 'episode_steps': 11.47, 'order_profits': -0.1378457303960876, 'order_steps': 6.49}\n",
      "Episode 59400: reward=-0, steps=27, speed=34.3 f/s, elapsed=3:10:16\n",
      "Episode 59500: reward=-0, steps=7, speed=34.3 f/s, elapsed=3:10:38\n",
      "Episode 59600: reward=-0, steps=2, speed=34.3 f/s, elapsed=3:10:59\n",
      "Episode 59700: reward=-0, steps=8, speed=34.3 f/s, elapsed=3:11:22\n",
      "Episode 59800: reward=0, steps=3, speed=34.4 f/s, elapsed=3:11:41\n",
      "Episode 59900: reward=-0, steps=9, speed=34.5 f/s, elapsed=3:12:02\n",
      "Episode 60000: reward=-0, steps=5, speed=34.4 f/s, elapsed=3:12:23\n",
      "Episode 60100: reward=0, steps=15, speed=34.5 f/s, elapsed=3:12:45\n",
      "Episode 60200: reward=-0, steps=3, speed=34.5 f/s, elapsed=3:13:06\n",
      "Episode 60300: reward=-0, steps=8, speed=34.5 f/s, elapsed=3:13:27\n",
      "Episode 60400: reward=-0, steps=7, speed=34.5 f/s, elapsed=3:13:49\n",
      "Episode 60500: reward=-0, steps=18, speed=34.5 f/s, elapsed=3:14:10\n",
      "Episode 60600: reward=-0, steps=7, speed=34.5 f/s, elapsed=3:14:29\n",
      "Episode 60700: reward=0, steps=2, speed=34.5 f/s, elapsed=3:14:50\n",
      "390000: tst: {'episode_reward': -0.07100904230988819, 'episode_steps': 12.19, 'order_profits': -0.0717036882004234, 'order_steps': 8.01}\n",
      "390000: val: {'episode_reward': -0.004405699399436625, 'episode_steps': 8.06, 'order_profits': -0.0050382267923558335, 'order_steps': 4.45}\n",
      "Episode 60800: reward=0, steps=8, speed=34.4 f/s, elapsed=3:15:13\n",
      "Episode 60900: reward=-1, steps=2, speed=34.4 f/s, elapsed=3:15:34\n",
      "Episode 61000: reward=-1, steps=3, speed=34.4 f/s, elapsed=3:15:56\n",
      "Episode 61100: reward=-0, steps=6, speed=34.4 f/s, elapsed=3:16:17\n",
      "Episode 61200: reward=0, steps=6, speed=34.4 f/s, elapsed=3:16:39\n",
      "Episode 61300: reward=-1, steps=5, speed=34.4 f/s, elapsed=3:16:57\n",
      "Episode 61400: reward=-0, steps=3, speed=34.4 f/s, elapsed=3:17:18\n",
      "Episode 61500: reward=0, steps=15, speed=34.5 f/s, elapsed=3:17:40\n",
      "Episode 61600: reward=0, steps=4, speed=34.5 f/s, elapsed=3:18:01\n",
      "Episode 61700: reward=-0, steps=4, speed=34.5 f/s, elapsed=3:18:21\n",
      "Episode 61800: reward=-0, steps=5, speed=34.6 f/s, elapsed=3:18:40\n",
      "Episode 61900: reward=-0, steps=4, speed=34.7 f/s, elapsed=3:18:58\n",
      "Episode 62000: reward=0, steps=9, speed=34.7 f/s, elapsed=3:19:18\n",
      "Episode 62100: reward=-1, steps=17, speed=34.7 f/s, elapsed=3:19:37\n",
      "400000: tst: {'episode_reward': -0.08430707713086041, 'episode_steps': 7.06, 'order_profits': -0.08459107688024367, 'order_steps': 1.94}\n",
      "400000: val: {'episode_reward': 0.007434905746928337, 'episode_steps': 5.92, 'order_profits': 0.0070823481496990436, 'order_steps': 2.06}\n",
      "Episode 62200: reward=0, steps=8, speed=34.7 f/s, elapsed=3:19:56\n",
      "Episode 62300: reward=0, steps=8, speed=34.7 f/s, elapsed=3:20:15\n",
      "Episode 62400: reward=0, steps=12, speed=34.8 f/s, elapsed=3:20:33\n",
      "Episode 62500: reward=-0, steps=4, speed=34.8 f/s, elapsed=3:20:52\n",
      "Episode 62600: reward=-0, steps=8, speed=34.9 f/s, elapsed=3:21:09\n",
      "Episode 62700: reward=-0, steps=14, speed=35.0 f/s, elapsed=3:21:28\n",
      "Episode 62800: reward=-0, steps=4, speed=35.1 f/s, elapsed=3:21:46\n",
      "Episode 62900: reward=-0, steps=5, speed=35.2 f/s, elapsed=3:22:02\n",
      "Episode 63000: reward=-0, steps=2, speed=35.2 f/s, elapsed=3:22:20\n",
      "Episode 63100: reward=-0, steps=3, speed=35.2 f/s, elapsed=3:22:36\n",
      "Episode 63200: reward=-0, steps=10, speed=35.2 f/s, elapsed=3:22:56\n",
      "Episode 63300: reward=-0, steps=6, speed=35.3 f/s, elapsed=3:23:18\n",
      "Episode 63400: reward=0, steps=3, speed=35.2 f/s, elapsed=3:23:41\n",
      "Episode 63500: reward=-0, steps=8, speed=35.2 f/s, elapsed=3:24:02\n",
      "410000: tst: {'episode_reward': -0.06960833545168138, 'episode_steps': 16.67, 'order_profits': -0.07060588223483681, 'order_steps': 7.33}\n",
      "410000: val: {'episode_reward': 0.022702260547157235, 'episode_steps': 12.36, 'order_profits': 0.020985250131747454, 'order_steps': 6.04}\n",
      "Episode 63600: reward=0, steps=4, speed=35.1 f/s, elapsed=3:24:26\n",
      "Episode 63700: reward=0, steps=4, speed=35.2 f/s, elapsed=3:24:48\n",
      "Episode 63800: reward=-0, steps=2, speed=35.2 f/s, elapsed=3:25:11\n",
      "Episode 63900: reward=0, steps=15, speed=35.2 f/s, elapsed=3:25:34\n",
      "Episode 64000: reward=-0, steps=17, speed=35.2 f/s, elapsed=3:25:56\n",
      "Episode 64100: reward=0, steps=6, speed=35.2 f/s, elapsed=3:26:17\n",
      "Episode 64200: reward=-0, steps=4, speed=35.2 f/s, elapsed=3:26:38\n",
      "Episode 64300: reward=-0, steps=13, speed=35.2 f/s, elapsed=3:27:01\n",
      "Episode 64400: reward=0, steps=8, speed=35.2 f/s, elapsed=3:27:22\n",
      "Episode 64500: reward=-1, steps=5, speed=35.2 f/s, elapsed=3:27:41\n",
      "Episode 64600: reward=0, steps=4, speed=35.2 f/s, elapsed=3:28:04\n",
      "Episode 64700: reward=-0, steps=7, speed=35.2 f/s, elapsed=3:28:24\n",
      "Episode 64800: reward=-0, steps=7, speed=35.2 f/s, elapsed=3:28:43\n",
      "420000: tst: {'episode_reward': -0.12225873747007858, 'episode_steps': 17.11, 'order_profits': -0.12102584230109653, 'order_steps': 7.7}\n",
      "420000: val: {'episode_reward': -0.009215693767860968, 'episode_steps': 9.46, 'order_profits': -0.010412346134098022, 'order_steps': 3.41}\n",
      "Episode 64900: reward=-0, steps=9, speed=35.1 f/s, elapsed=3:29:08\n",
      "Episode 65000: reward=0, steps=8, speed=35.1 f/s, elapsed=3:29:31\n",
      "Episode 65100: reward=-0, steps=4, speed=35.2 f/s, elapsed=3:29:53\n",
      "Episode 65200: reward=-0, steps=7, speed=35.2 f/s, elapsed=3:30:13\n",
      "Episode 65300: reward=-0, steps=2, speed=35.2 f/s, elapsed=3:30:35\n",
      "Episode 65400: reward=-0, steps=12, speed=35.2 f/s, elapsed=3:31:00\n",
      "Episode 65500: reward=-1, steps=3, speed=35.2 f/s, elapsed=3:31:23\n",
      "Episode 65600: reward=-1, steps=11, speed=35.2 f/s, elapsed=3:31:43\n",
      "Episode 65700: reward=-2, steps=6, speed=35.2 f/s, elapsed=3:32:06\n",
      "Episode 65800: reward=-0, steps=10, speed=35.2 f/s, elapsed=3:32:30\n",
      "Episode 65900: reward=-0, steps=9, speed=35.2 f/s, elapsed=3:32:50\n",
      "Episode 66000: reward=0, steps=4, speed=35.2 f/s, elapsed=3:33:13\n",
      "Episode 66100: reward=-1, steps=5, speed=35.2 f/s, elapsed=3:33:35\n",
      "430000: tst: {'episode_reward': -0.036168390022227756, 'episode_steps': 11.13, 'order_profits': -0.03696015121887264, 'order_steps': 5.36}\n",
      "430000: val: {'episode_reward': -0.06679771397355021, 'episode_steps': 9.95, 'order_profits': -0.06746560910765201, 'order_steps': 5.96}\n",
      "Episode 66200: reward=-0, steps=6, speed=35.1 f/s, elapsed=3:33:59\n",
      "Episode 66300: reward=0, steps=4, speed=35.1 f/s, elapsed=3:34:23\n",
      "Episode 66400: reward=-0, steps=7, speed=35.2 f/s, elapsed=3:34:45\n",
      "Episode 66500: reward=-1, steps=29, speed=35.2 f/s, elapsed=3:35:09\n",
      "Episode 66600: reward=-0, steps=3, speed=35.2 f/s, elapsed=3:35:31\n",
      "Episode 66700: reward=1, steps=9, speed=35.2 f/s, elapsed=3:35:53\n",
      "Episode 66800: reward=-0, steps=20, speed=35.2 f/s, elapsed=3:36:16\n",
      "Episode 66900: reward=-0, steps=4, speed=35.2 f/s, elapsed=3:36:40\n",
      "Episode 67000: reward=-0, steps=4, speed=35.2 f/s, elapsed=3:37:00\n",
      "Episode 67100: reward=0, steps=10, speed=35.2 f/s, elapsed=3:37:21\n",
      "Episode 67200: reward=-0, steps=7, speed=35.2 f/s, elapsed=3:37:46\n",
      "Episode 67300: reward=-0, steps=4, speed=35.2 f/s, elapsed=3:38:08\n",
      "Episode 67400: reward=-0, steps=9, speed=35.3 f/s, elapsed=3:38:28\n",
      "440000: tst: {'episode_reward': -0.0963387155950856, 'episode_steps': 11.72, 'order_profits': -0.0968621118834178, 'order_steps': 4.77}\n",
      "440000: val: {'episode_reward': -0.13456575290344502, 'episode_steps': 8.7, 'order_profits': -0.1346259670123934, 'order_steps': 3.24}\n",
      "Episode 67500: reward=0, steps=13, speed=35.2 f/s, elapsed=3:38:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 67600: reward=0, steps=12, speed=35.2 f/s, elapsed=3:39:13\n",
      "Episode 67700: reward=0, steps=2, speed=35.2 f/s, elapsed=3:39:33\n",
      "Episode 67800: reward=0, steps=5, speed=35.3 f/s, elapsed=3:39:55\n",
      "Episode 67900: reward=-0, steps=3, speed=35.3 f/s, elapsed=3:40:17\n",
      "Episode 68000: reward=-1, steps=17, speed=35.3 f/s, elapsed=3:40:41\n",
      "Episode 68100: reward=0, steps=5, speed=35.2 f/s, elapsed=3:41:05\n",
      "Episode 68200: reward=0, steps=13, speed=35.2 f/s, elapsed=3:41:25\n",
      "Episode 68300: reward=0, steps=8, speed=35.2 f/s, elapsed=3:41:45\n",
      "Episode 68400: reward=-0, steps=14, speed=35.3 f/s, elapsed=3:42:07\n",
      "Episode 68500: reward=-0, steps=7, speed=35.3 f/s, elapsed=3:42:28\n",
      "Episode 68600: reward=-0, steps=12, speed=35.3 f/s, elapsed=3:42:51\n",
      "Episode 68700: reward=-0, steps=3, speed=35.3 f/s, elapsed=3:43:11\n",
      "450000: tst: {'episode_reward': -0.013165386357452964, 'episode_steps': 14.1, 'order_profits': -0.013612858998874457, 'order_steps': 5.63}\n",
      "450000: val: {'episode_reward': -0.12452477584810558, 'episode_steps': 11.05, 'order_profits': -0.12504343322785683, 'order_steps': 4.38}\n",
      "Episode 68800: reward=-0, steps=3, speed=35.2 f/s, elapsed=3:43:32\n",
      "Episode 68900: reward=0, steps=2, speed=35.2 f/s, elapsed=3:43:54\n",
      "Episode 69000: reward=0, steps=2, speed=35.2 f/s, elapsed=3:44:16\n",
      "Episode 69100: reward=-1, steps=7, speed=35.2 f/s, elapsed=3:44:37\n",
      "Episode 69200: reward=-0, steps=18, speed=35.2 f/s, elapsed=3:44:59\n",
      "Episode 69300: reward=0, steps=10, speed=35.3 f/s, elapsed=3:45:21\n",
      "Episode 69400: reward=-1, steps=6, speed=35.3 f/s, elapsed=3:45:43\n",
      "Episode 69500: reward=-0, steps=4, speed=35.3 f/s, elapsed=3:46:03\n",
      "Episode 69600: reward=-0, steps=5, speed=35.3 f/s, elapsed=3:46:26\n",
      "Episode 69700: reward=0, steps=4, speed=35.3 f/s, elapsed=3:46:46\n",
      "Episode 69800: reward=-1, steps=4, speed=35.3 f/s, elapsed=3:47:09\n",
      "Episode 69900: reward=0, steps=10, speed=35.3 f/s, elapsed=3:47:30\n",
      "Episode 70000: reward=-0, steps=14, speed=35.3 f/s, elapsed=3:47:50\n",
      "460000: tst: {'episode_reward': -0.07239282932942363, 'episode_steps': 8.77, 'order_profits': -0.0728302081405873, 'order_steps': 4.89}\n",
      "460000: val: {'episode_reward': -0.046165843895866684, 'episode_steps': 7.0, 'order_profits': -0.046535468729023706, 'order_steps': 2.91}\n",
      "Episode 70100: reward=-0, steps=5, speed=35.3 f/s, elapsed=3:48:14\n",
      "Episode 70200: reward=-1, steps=4, speed=35.3 f/s, elapsed=3:48:37\n",
      "Episode 70300: reward=-0, steps=4, speed=35.3 f/s, elapsed=3:48:58\n",
      "Episode 70400: reward=1, steps=22, speed=35.3 f/s, elapsed=3:49:20\n",
      "Episode 70500: reward=-0, steps=5, speed=35.3 f/s, elapsed=3:49:44\n",
      "Episode 70600: reward=-1, steps=10, speed=35.3 f/s, elapsed=3:50:07\n",
      "Episode 70700: reward=-0, steps=7, speed=35.3 f/s, elapsed=3:50:32\n",
      "Episode 70800: reward=-0, steps=22, speed=35.3 f/s, elapsed=3:50:57\n",
      "Episode 70900: reward=-1, steps=4, speed=35.3 f/s, elapsed=3:51:21\n",
      "Episode 71000: reward=-1, steps=22, speed=35.3 f/s, elapsed=3:51:43\n",
      "Episode 71100: reward=-1, steps=12, speed=35.3 f/s, elapsed=3:52:06\n",
      "Episode 71200: reward=-2, steps=18, speed=35.4 f/s, elapsed=3:52:25\n",
      "470000: tst: {'episode_reward': 0.04171318944979921, 'episode_steps': 15.52, 'order_profits': 0.04071590913713078, 'order_steps': 6.8}\n",
      "470000: val: {'episode_reward': 0.02021922704348517, 'episode_steps': 10.68, 'order_profits': 0.01992551056564275, 'order_steps': 4.56}\n",
      "Episode 71300: reward=-0, steps=5, speed=35.4 f/s, elapsed=3:52:47\n",
      "Episode 71400: reward=-1, steps=4, speed=35.5 f/s, elapsed=3:53:07\n",
      "Episode 71500: reward=0, steps=13, speed=35.6 f/s, elapsed=3:53:26\n",
      "Episode 71600: reward=-1, steps=12, speed=35.7 f/s, elapsed=3:53:47\n",
      "Episode 71700: reward=-0, steps=2, speed=35.7 f/s, elapsed=3:54:10\n",
      "Episode 71800: reward=-0, steps=9, speed=35.8 f/s, elapsed=3:54:30\n",
      "Episode 71900: reward=0, steps=4, speed=35.9 f/s, elapsed=3:54:50\n",
      "Episode 72000: reward=0, steps=4, speed=36.0 f/s, elapsed=3:55:10\n",
      "Episode 72100: reward=-0, steps=14, speed=36.0 f/s, elapsed=3:55:33\n",
      "Episode 72200: reward=-0, steps=12, speed=36.1 f/s, elapsed=3:55:54\n",
      "Episode 72300: reward=0, steps=7, speed=36.2 f/s, elapsed=3:56:11\n",
      "Episode 72400: reward=0, steps=5, speed=36.2 f/s, elapsed=3:56:33\n",
      "480000: tst: {'episode_reward': -0.054731103873296744, 'episode_steps': 16.29, 'order_profits': -0.05510877021247215, 'order_steps': 2.41}\n",
      "480000: val: {'episode_reward': -0.053137863612732164, 'episode_steps': 9.66, 'order_profits': -0.05339021951140041, 'order_steps': 2.37}\n",
      "Episode 72500: reward=-1, steps=12, speed=36.2 f/s, elapsed=3:56:57\n",
      "Episode 72600: reward=-0, steps=8, speed=36.3 f/s, elapsed=3:57:21\n",
      "Episode 72700: reward=0, steps=6, speed=36.4 f/s, elapsed=3:57:40\n",
      "Episode 72800: reward=-0, steps=14, speed=36.4 f/s, elapsed=3:58:03\n",
      "Episode 72900: reward=-0, steps=4, speed=36.5 f/s, elapsed=3:58:24\n",
      "Episode 73000: reward=-0, steps=16, speed=36.5 f/s, elapsed=3:58:48\n",
      "Episode 73100: reward=-0, steps=11, speed=36.6 f/s, elapsed=3:59:09\n",
      "Episode 73200: reward=-0, steps=6, speed=36.7 f/s, elapsed=3:59:29\n",
      "Episode 73300: reward=-1, steps=21, speed=36.7 f/s, elapsed=3:59:51\n",
      "Episode 73400: reward=-0, steps=11, speed=36.8 f/s, elapsed=4:00:14\n",
      "Episode 73500: reward=0, steps=13, speed=36.8 f/s, elapsed=4:00:33\n",
      "Episode 73600: reward=-1, steps=13, speed=36.8 f/s, elapsed=4:00:53\n",
      "490000: tst: {'episode_reward': -0.09086932709228694, 'episode_steps': 13.0, 'order_profits': -0.09129151101301684, 'order_steps': 4.48}\n",
      "490000: val: {'episode_reward': 0.026674153012948024, 'episode_steps': 8.69, 'order_profits': 0.02600267892417456, 'order_steps': 3.13}\n",
      "Episode 73700: reward=-0, steps=7, speed=36.7 f/s, elapsed=4:01:16\n",
      "Episode 73800: reward=-0, steps=11, speed=36.7 f/s, elapsed=4:01:38\n",
      "Episode 73900: reward=-0, steps=12, speed=36.7 f/s, elapsed=4:01:57\n",
      "Episode 74000: reward=-0, steps=8, speed=36.7 f/s, elapsed=4:02:18\n",
      "Episode 74100: reward=-0, steps=4, speed=36.7 f/s, elapsed=4:02:39\n",
      "Episode 74200: reward=-0, steps=7, speed=36.7 f/s, elapsed=4:03:02\n",
      "Episode 74300: reward=-0, steps=9, speed=36.7 f/s, elapsed=4:03:24\n",
      "Episode 74400: reward=-0, steps=5, speed=36.6 f/s, elapsed=4:03:45\n",
      "Episode 74500: reward=0, steps=3, speed=36.6 f/s, elapsed=4:04:04\n",
      "Episode 74600: reward=-0, steps=9, speed=36.6 f/s, elapsed=4:04:23\n",
      "Episode 74700: reward=-0, steps=6, speed=36.6 f/s, elapsed=4:04:47\n",
      "Episode 74800: reward=-1, steps=10, speed=36.6 f/s, elapsed=4:05:07\n",
      "Episode 74900: reward=-0, steps=7, speed=36.6 f/s, elapsed=4:05:31\n",
      "500000: tst: {'episode_reward': -0.03181900392996957, 'episode_steps': 22.49, 'order_profits': -0.03425675647488436, 'order_steps': 14.76}\n",
      "500000: val: {'episode_reward': -0.009072093858385285, 'episode_steps': 12.42, 'order_profits': -0.010288743427844041, 'order_steps': 7.42}\n",
      "Episode 75000: reward=-0, steps=15, speed=36.4 f/s, elapsed=4:06:01\n",
      "Episode 75100: reward=-0, steps=7, speed=36.4 f/s, elapsed=4:06:26\n",
      "Episode 75200: reward=-0, steps=4, speed=36.4 f/s, elapsed=4:06:49\n",
      "Episode 75300: reward=-0, steps=4, speed=36.4 f/s, elapsed=4:07:13\n",
      "Episode 75400: reward=-0, steps=3, speed=36.4 f/s, elapsed=4:07:36\n",
      "Episode 75500: reward=-0, steps=4, speed=36.3 f/s, elapsed=4:07:59\n",
      "Episode 75600: reward=-0, steps=5, speed=36.3 f/s, elapsed=4:08:22\n",
      "Episode 75700: reward=-1, steps=11, speed=36.3 f/s, elapsed=4:08:45\n",
      "Episode 75800: reward=-0, steps=4, speed=36.3 f/s, elapsed=4:09:08\n",
      "Episode 75900: reward=-4, steps=15, speed=36.3 f/s, elapsed=4:09:33\n",
      "Episode 76000: reward=-0, steps=11, speed=36.2 f/s, elapsed=4:09:55\n",
      "Episode 76100: reward=-1, steps=5, speed=36.2 f/s, elapsed=4:10:17\n",
      "510000: tst: {'episode_reward': 0.0058599528023537085, 'episode_steps': 21.93, 'order_profits': 0.00457613705050996, 'order_steps': 5.09}\n",
      "510000: val: {'episode_reward': -0.18542554160314714, 'episode_steps': 21.3, 'order_profits': -0.186858955092052, 'order_steps': 11.58}\n",
      "Episode 76200: reward=-0, steps=10, speed=36.1 f/s, elapsed=4:10:46\n",
      "Episode 76300: reward=-0, steps=5, speed=36.1 f/s, elapsed=4:11:06\n",
      "Episode 76400: reward=-0, steps=3, speed=36.1 f/s, elapsed=4:11:31\n",
      "Episode 76500: reward=0, steps=5, speed=36.0 f/s, elapsed=4:11:53\n",
      "Episode 76600: reward=0, steps=5, speed=36.0 f/s, elapsed=4:12:13\n",
      "Episode 76700: reward=-0, steps=5, speed=36.0 f/s, elapsed=4:12:36\n",
      "Episode 76800: reward=0, steps=6, speed=36.1 f/s, elapsed=4:12:56\n",
      "Episode 76900: reward=-0, steps=3, speed=36.1 f/s, elapsed=4:13:14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 77000: reward=-1, steps=9, speed=36.1 f/s, elapsed=4:13:36\n",
      "Episode 77100: reward=-0, steps=7, speed=36.1 f/s, elapsed=4:13:58\n",
      "Episode 77200: reward=-1, steps=12, speed=36.1 f/s, elapsed=4:14:24\n",
      "Episode 77300: reward=-0, steps=5, speed=36.1 f/s, elapsed=4:14:46\n",
      "Episode 77400: reward=-0, steps=3, speed=36.2 f/s, elapsed=4:15:02\n",
      "520000: tst: {'episode_reward': -0.04956092626996725, 'episode_steps': 11.16, 'order_profits': -0.0501558336843849, 'order_steps': 5.01}\n",
      "520000: val: {'episode_reward': 0.0037194583262562665, 'episode_steps': 7.44, 'order_profits': 0.0026525529439120656, 'order_steps': 3.4}\n",
      "Episode 77500: reward=-1, steps=10, speed=36.2 f/s, elapsed=4:15:23\n",
      "Episode 77600: reward=-0, steps=10, speed=36.3 f/s, elapsed=4:15:42\n",
      "Episode 77700: reward=0, steps=5, speed=36.3 f/s, elapsed=4:16:05\n",
      "Episode 77800: reward=-0, steps=9, speed=36.3 f/s, elapsed=4:16:32\n",
      "Episode 77900: reward=-0, steps=8, speed=36.3 f/s, elapsed=4:16:54\n",
      "Episode 78000: reward=-0, steps=5, speed=36.3 f/s, elapsed=4:17:17\n",
      "Episode 78100: reward=-0, steps=11, speed=36.4 f/s, elapsed=4:17:40\n",
      "Episode 78200: reward=-1, steps=7, speed=36.4 f/s, elapsed=4:18:04\n",
      "Episode 78300: reward=-0, steps=9, speed=36.5 f/s, elapsed=4:18:27\n",
      "Episode 78400: reward=0, steps=10, speed=36.6 f/s, elapsed=4:18:47\n",
      "Episode 78500: reward=-1, steps=13, speed=36.6 f/s, elapsed=4:19:10\n",
      "Episode 78600: reward=0, steps=23, speed=36.6 f/s, elapsed=4:19:37\n",
      "530000: tst: {'episode_reward': -0.036856943010243476, 'episode_steps': 13.75, 'order_profits': -0.038061430582901436, 'order_steps': 8.21}\n",
      "530000: val: {'episode_reward': 0.025456829936364302, 'episode_steps': 9.4, 'order_profits': 0.02419254186580001, 'order_steps': 5.02}\n",
      "Episode 78700: reward=0, steps=12, speed=36.5 f/s, elapsed=4:20:04\n",
      "Episode 78800: reward=-0, steps=7, speed=36.5 f/s, elapsed=4:20:30\n",
      "Episode 78900: reward=-0, steps=9, speed=36.5 f/s, elapsed=4:20:53\n",
      "533000: Best mean value updated -0.057 -> -0.057\n",
      "Episode 79000: reward=-1, steps=16, speed=36.4 f/s, elapsed=4:21:21\n",
      "534000: Best mean value updated -0.057 -> -0.053\n",
      "Episode 79100: reward=-0, steps=2, speed=36.4 f/s, elapsed=4:21:48\n",
      "Episode 79200: reward=0, steps=3, speed=36.4 f/s, elapsed=4:22:13\n",
      "Episode 79300: reward=-1, steps=32, speed=36.2 f/s, elapsed=4:22:54\n",
      "Episode 79400: reward=1, steps=17, speed=36.2 f/s, elapsed=4:23:20\n",
      "Episode 79500: reward=-0, steps=3, speed=36.1 f/s, elapsed=4:23:51\n",
      "Episode 79600: reward=-0, steps=7, speed=36.2 f/s, elapsed=4:24:18\n",
      "540000: tst: {'episode_reward': -0.028509396782051857, 'episode_steps': 61.69, 'order_profits': -0.035370364189288225, 'order_steps': 54.15}\n",
      "540000: val: {'episode_reward': 0.290124857559465, 'episode_steps': 31.56, 'order_profits': 0.28756018007568823, 'order_steps': 24.71}\n",
      "Episode 79700: reward=0, steps=8, speed=36.0 f/s, elapsed=4:24:54\n",
      "Episode 79800: reward=-0, steps=23, speed=36.0 f/s, elapsed=4:25:24\n",
      "Episode 79900: reward=0, steps=6, speed=36.1 f/s, elapsed=4:25:52\n",
      "Episode 80000: reward=-0, steps=5, speed=36.1 f/s, elapsed=4:26:17\n",
      "Episode 80100: reward=-0, steps=4, speed=36.1 f/s, elapsed=5:00:13\n",
      "Episode 80200: reward=0, steps=12, speed=36.2 f/s, elapsed=5:00:42\n",
      "Episode 80300: reward=1, steps=28, speed=36.1 f/s, elapsed=5:02:51\n",
      "Episode 80400: reward=-0, steps=13, speed=36.0 f/s, elapsed=5:03:24\n",
      "Episode 80500: reward=-1, steps=5, speed=36.0 f/s, elapsed=5:03:50\n",
      "Episode 80600: reward=-0, steps=6, speed=36.0 f/s, elapsed=5:04:20\n",
      "550000: tst: {'episode_reward': -0.13699092264987053, 'episode_steps': 19.33, 'order_profits': -0.13701158364421503, 'order_steps': 11.58}\n",
      "550000: val: {'episode_reward': -0.14595701288588253, 'episode_steps': 14.61, 'order_profits': -0.1440820489797005, 'order_steps': 10.17}\n",
      "Episode 80700: reward=-0, steps=8, speed=35.9 f/s, elapsed=5:04:49\n",
      "Episode 80800: reward=-0, steps=15, speed=35.9 f/s, elapsed=5:05:14\n",
      "Episode 80900: reward=0, steps=7, speed=35.9 f/s, elapsed=5:05:39\n",
      "Episode 81000: reward=-0, steps=7, speed=35.9 f/s, elapsed=5:05:58\n",
      "Episode 81100: reward=-0, steps=4, speed=36.0 f/s, elapsed=5:06:21\n",
      "Episode 81200: reward=-0, steps=8, speed=36.0 f/s, elapsed=5:06:45\n",
      "Episode 81300: reward=0, steps=9, speed=36.0 f/s, elapsed=5:07:08\n",
      "Episode 81400: reward=0, steps=3, speed=36.0 f/s, elapsed=5:07:34\n",
      "Episode 81500: reward=-0, steps=18, speed=36.1 f/s, elapsed=5:07:59\n",
      "Episode 81600: reward=0, steps=18, speed=36.1 f/s, elapsed=5:08:25\n",
      "Episode 81700: reward=-0, steps=13, speed=36.1 f/s, elapsed=5:08:49\n",
      "560000: tst: {'episode_reward': -0.10949979746518228, 'episode_steps': 34.4, 'order_profits': -0.11248567663508059, 'order_steps': 20.55}\n",
      "560000: val: {'episode_reward': 0.005053974526121171, 'episode_steps': 20.66, 'order_profits': 0.003090807827296798, 'order_steps': 10.02}\n",
      "Episode 81800: reward=-0, steps=14, speed=36.0 f/s, elapsed=5:09:19\n",
      "Episode 81900: reward=-0, steps=13, speed=36.0 f/s, elapsed=5:09:44\n",
      "Episode 82000: reward=0, steps=4, speed=36.0 f/s, elapsed=5:10:07\n",
      "Episode 82100: reward=1, steps=6, speed=36.0 f/s, elapsed=5:10:28\n",
      "Episode 82200: reward=-0, steps=3, speed=36.1 f/s, elapsed=5:10:54\n",
      "Episode 82300: reward=0, steps=7, speed=36.1 f/s, elapsed=5:11:21\n",
      "Episode 82400: reward=0, steps=9, speed=36.1 f/s, elapsed=5:11:46\n",
      "Episode 82500: reward=0, steps=3, speed=36.1 f/s, elapsed=5:12:11\n",
      "Episode 82600: reward=0, steps=3, speed=36.1 f/s, elapsed=5:12:37\n",
      "Episode 82700: reward=0, steps=5, speed=36.1 f/s, elapsed=5:13:02\n",
      "Episode 82800: reward=-0, steps=4, speed=36.2 f/s, elapsed=5:13:23\n",
      "570000: tst: {'episode_reward': 0.10886350138647508, 'episode_steps': 26.81, 'order_profits': 0.10569051492737927, 'order_steps': 22.53}\n",
      "570000: val: {'episode_reward': -0.008298549979992022, 'episode_steps': 22.89, 'order_profits': -0.01074556058131024, 'order_steps': 18.9}\n",
      "Episode 82900: reward=-0, steps=10, speed=36.0 f/s, elapsed=5:13:53\n",
      "Episode 83000: reward=-1, steps=6, speed=36.1 f/s, elapsed=5:14:17\n",
      "Episode 83100: reward=0, steps=10, speed=36.1 f/s, elapsed=5:14:41\n",
      "Episode 83200: reward=-0, steps=13, speed=36.1 f/s, elapsed=5:15:09\n",
      "Episode 83300: reward=-0, steps=13, speed=36.1 f/s, elapsed=5:15:41\n",
      "Episode 83400: reward=-1, steps=17, speed=36.1 f/s, elapsed=5:16:11\n",
      "576000: Best mean value updated -0.053 -> -0.051\n",
      "Episode 83500: reward=0, steps=9, speed=36.1 f/s, elapsed=5:16:41\n",
      "Episode 83600: reward=-1, steps=12, speed=36.1 f/s, elapsed=5:17:08\n",
      "578000: Best mean value updated -0.051 -> -0.037\n",
      "Episode 83700: reward=-0, steps=8, speed=36.1 f/s, elapsed=5:17:39\n",
      "Episode 83800: reward=-0, steps=6, speed=36.1 f/s, elapsed=5:18:12\n",
      "580000: tst: {'episode_reward': 0.15273528183872967, 'episode_steps': 60.61, 'order_profits': 0.14774093571697772, 'order_steps': 48.74}\n",
      "580000: val: {'episode_reward': 0.0756895427029433, 'episode_steps': 29.26, 'order_profits': 0.07195655393937454, 'order_steps': 22.65}\n",
      "Episode 83900: reward=-0, steps=13, speed=36.0 f/s, elapsed=5:18:51\n",
      "Episode 84000: reward=0, steps=4, speed=36.0 f/s, elapsed=5:19:16\n",
      "Episode 84100: reward=-0, steps=17, speed=36.0 f/s, elapsed=5:19:43\n",
      "Episode 84200: reward=1, steps=16, speed=36.1 f/s, elapsed=5:20:07\n",
      "Episode 84300: reward=0, steps=6, speed=36.1 f/s, elapsed=5:20:31\n",
      "Episode 84400: reward=-0, steps=4, speed=36.1 f/s, elapsed=5:20:57\n",
      "Episode 84500: reward=-0, steps=4, speed=36.1 f/s, elapsed=5:21:21\n",
      "Episode 84600: reward=-0, steps=2, speed=36.2 f/s, elapsed=5:21:44\n",
      "Episode 84700: reward=-0, steps=19, speed=36.2 f/s, elapsed=5:22:09\n",
      "Episode 84800: reward=-0, steps=7, speed=36.2 f/s, elapsed=5:22:32\n",
      "Episode 84900: reward=-0, steps=19, speed=36.2 f/s, elapsed=5:22:58\n",
      "590000: tst: {'episode_reward': -0.1431618772090653, 'episode_steps': 16.85, 'order_profits': -0.14299998126080798, 'order_steps': 11.44}\n",
      "590000: val: {'episode_reward': -0.055748022740489525, 'episode_steps': 11.6, 'order_profits': -0.058326734400687696, 'order_steps': 7.07}\n",
      "Episode 85000: reward=1, steps=6, speed=36.1 f/s, elapsed=5:23:24\n",
      "Episode 85100: reward=1, steps=9, speed=36.1 f/s, elapsed=5:23:47\n",
      "Episode 85200: reward=-0, steps=8, speed=36.1 f/s, elapsed=5:24:11\n",
      "Episode 85300: reward=-0, steps=2, speed=36.1 f/s, elapsed=5:24:37\n",
      "Episode 85400: reward=0, steps=2, speed=36.2 f/s, elapsed=5:25:03\n",
      "Episode 85500: reward=0, steps=9, speed=36.2 f/s, elapsed=5:25:29\n",
      "Episode 85600: reward=0, steps=16, speed=36.2 f/s, elapsed=5:25:55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 85700: reward=-1, steps=19, speed=36.2 f/s, elapsed=5:26:22\n",
      "Episode 85800: reward=-0, steps=7, speed=36.3 f/s, elapsed=5:26:46\n",
      "Episode 85900: reward=-1, steps=10, speed=36.3 f/s, elapsed=5:27:12\n",
      "600000: tst: {'episode_reward': 0.002313408359763427, 'episode_steps': 57.1, 'order_profits': -0.0030654171059745837, 'order_steps': 51.99}\n",
      "600000: val: {'episode_reward': 0.08294072644404606, 'episode_steps': 35.07, 'order_profits': 0.07638875560914027, 'order_steps': 31.22}\n",
      "Episode 86000: reward=-0, steps=8, speed=36.2 f/s, elapsed=5:27:46\n",
      "Episode 86100: reward=-0, steps=7, speed=36.2 f/s, elapsed=5:28:17\n",
      "Episode 86200: reward=-0, steps=8, speed=36.2 f/s, elapsed=5:28:43\n",
      "Episode 86300: reward=0, steps=7, speed=36.2 f/s, elapsed=5:29:15\n",
      "Episode 86400: reward=-0, steps=6, speed=36.2 f/s, elapsed=5:29:41\n",
      "Episode 86500: reward=-0, steps=11, speed=36.3 f/s, elapsed=5:30:06\n",
      "Episode 86600: reward=-1, steps=4, speed=36.3 f/s, elapsed=5:30:36\n",
      "Episode 86700: reward=0, steps=45, speed=36.3 f/s, elapsed=5:31:06\n",
      "Episode 86800: reward=-0, steps=22, speed=36.3 f/s, elapsed=5:31:39\n",
      "Episode 86900: reward=-0, steps=11, speed=36.3 f/s, elapsed=5:32:11\n",
      "610000: Best mean value updated -0.037 -> -0.036\n",
      "610000: tst: {'episode_reward': -0.14842986261368363, 'episode_steps': 51.39, 'order_profits': -0.15162869439155313, 'order_steps': 39.37}\n",
      "610000: val: {'episode_reward': 0.09572007152398881, 'episode_steps': 40.12, 'order_profits': 0.09977487475101579, 'order_steps': 30.82}\n",
      "611000: Best mean value updated -0.036 -> -0.029\n",
      "Episode 87000: reward=-0, steps=12, speed=36.1 f/s, elapsed=5:32:52\n",
      "Episode 87100: reward=-0, steps=7, speed=36.2 f/s, elapsed=5:33:20\n",
      "613000: Best mean value updated -0.029 -> -0.022\n",
      "Episode 87200: reward=0, steps=8, speed=36.2 f/s, elapsed=5:33:54\n",
      "Episode 87300: reward=-0, steps=6, speed=36.2 f/s, elapsed=5:34:24\n",
      "Episode 87400: reward=0, steps=9, speed=36.2 f/s, elapsed=5:34:59\n",
      "Episode 87500: reward=-1, steps=17, speed=36.2 f/s, elapsed=5:35:37\n",
      "Episode 87600: reward=-0, steps=8, speed=36.2 f/s, elapsed=5:36:08\n",
      "Episode 87700: reward=0, steps=10, speed=36.2 f/s, elapsed=5:36:38\n",
      "620000: tst: {'episode_reward': 0.11596654020381644, 'episode_steps': 18.36, 'order_profits': 0.11420625604659816, 'order_steps': 12.33}\n",
      "620000: val: {'episode_reward': -0.09312586796292172, 'episode_steps': 12.75, 'order_profits': -0.09501765950823308, 'order_steps': 8.65}\n",
      "Episode 87800: reward=1, steps=15, speed=36.2 f/s, elapsed=5:37:12\n",
      "Episode 87900: reward=0, steps=11, speed=36.2 f/s, elapsed=5:37:41\n",
      "Episode 88000: reward=-0, steps=25, speed=36.2 f/s, elapsed=5:38:10\n",
      "Episode 88100: reward=-0, steps=6, speed=36.2 f/s, elapsed=5:38:40\n",
      "Episode 88200: reward=-1, steps=8, speed=36.2 f/s, elapsed=5:39:10\n",
      "Episode 88300: reward=-0, steps=9, speed=36.2 f/s, elapsed=5:39:42\n",
      "Episode 88400: reward=0, steps=14, speed=36.2 f/s, elapsed=5:40:17\n",
      "Episode 88500: reward=-0, steps=17, speed=36.2 f/s, elapsed=5:40:50\n",
      "Episode 88600: reward=-1, steps=5, speed=36.2 f/s, elapsed=5:41:17\n",
      "630000: tst: {'episode_reward': -0.055917231718984495, 'episode_steps': 10.99, 'order_profits': -0.05634139410440868, 'order_steps': 3.59}\n",
      "630000: val: {'episode_reward': -0.057881811277190096, 'episode_steps': 9.0, 'order_profits': -0.05871658600989016, 'order_steps': 2.83}\n",
      "Episode 88700: reward=0, steps=17, speed=36.2 f/s, elapsed=5:41:45\n",
      "Episode 88800: reward=0, steps=3, speed=36.2 f/s, elapsed=5:42:11\n",
      "Episode 88900: reward=0, steps=3, speed=36.2 f/s, elapsed=5:42:37\n",
      "Episode 89000: reward=-2, steps=55, speed=36.3 f/s, elapsed=5:43:04\n",
      "Episode 89100: reward=0, steps=2, speed=36.3 f/s, elapsed=5:43:26\n",
      "Episode 89200: reward=-0, steps=5, speed=36.3 f/s, elapsed=5:43:47\n",
      "Episode 89300: reward=0, steps=7, speed=36.3 f/s, elapsed=5:44:15\n",
      "Episode 89400: reward=-0, steps=3, speed=36.3 f/s, elapsed=5:44:35\n",
      "Episode 89500: reward=-0, steps=3, speed=36.3 f/s, elapsed=5:44:56\n",
      "Episode 89600: reward=0, steps=5, speed=36.3 f/s, elapsed=5:45:21\n",
      "Episode 89700: reward=-1, steps=48, speed=36.3 f/s, elapsed=5:45:52\n",
      "640000: tst: {'episode_reward': -0.0762785648740691, 'episode_steps': 22.48, 'order_profits': -0.0778064057318071, 'order_steps': 13.36}\n",
      "640000: val: {'episode_reward': -0.04308949664749525, 'episode_steps': 15.78, 'order_profits': -0.04304773908888383, 'order_steps': 8.71}\n",
      "Episode 89800: reward=0, steps=4, speed=36.2 f/s, elapsed=5:46:24\n",
      "Episode 89900: reward=-0, steps=20, speed=36.2 f/s, elapsed=5:46:55\n",
      "Episode 90000: reward=1, steps=3, speed=36.2 f/s, elapsed=5:47:23\n",
      "Episode 90100: reward=0, steps=17, speed=36.2 f/s, elapsed=5:47:52\n",
      "Episode 90200: reward=-1, steps=11, speed=36.2 f/s, elapsed=5:48:19\n",
      "Episode 90300: reward=0, steps=11, speed=36.2 f/s, elapsed=5:48:54\n",
      "Episode 90400: reward=0, steps=10, speed=36.2 f/s, elapsed=5:49:23\n",
      "Episode 90500: reward=0, steps=6, speed=36.2 f/s, elapsed=5:49:52\n",
      "Episode 90600: reward=-0, steps=7, speed=36.2 f/s, elapsed=5:50:17\n",
      "Episode 90700: reward=0, steps=5, speed=36.2 f/s, elapsed=5:50:44\n",
      "650000: tst: {'episode_reward': -0.06333192061981122, 'episode_steps': 18.85, 'order_profits': -0.06495385290990281, 'order_steps': 7.93}\n",
      "650000: val: {'episode_reward': -0.03258844123067943, 'episode_steps': 13.17, 'order_profits': -0.0336262390389827, 'order_steps': 4.38}\n",
      "Episode 90800: reward=-0, steps=11, speed=36.1 f/s, elapsed=5:51:13\n",
      "Episode 90900: reward=-1, steps=9, speed=36.1 f/s, elapsed=5:51:40\n",
      "Episode 91000: reward=0, steps=11, speed=36.1 f/s, elapsed=5:52:13\n",
      "Episode 91100: reward=-1, steps=7, speed=36.1 f/s, elapsed=5:52:43\n",
      "Episode 91200: reward=-0, steps=6, speed=36.1 f/s, elapsed=5:53:11\n",
      "Episode 91300: reward=0, steps=3, speed=36.0 f/s, elapsed=5:53:50\n",
      "Episode 91400: reward=0, steps=6, speed=36.0 f/s, elapsed=5:54:22\n",
      "Episode 91500: reward=-0, steps=4, speed=36.1 f/s, elapsed=5:54:50\n",
      "Episode 91600: reward=-0, steps=7, speed=36.1 f/s, elapsed=5:55:21\n",
      "660000: tst: {'episode_reward': -0.04850516571248181, 'episode_steps': 20.27, 'order_profits': -0.05098861061585952, 'order_steps': 13.5}\n",
      "660000: val: {'episode_reward': -0.07924282400982027, 'episode_steps': 14.47, 'order_profits': -0.08124542838696643, 'order_steps': 8.88}\n",
      "Episode 91700: reward=-0, steps=4, speed=36.0 f/s, elapsed=5:55:58\n",
      "Episode 91800: reward=-0, steps=57, speed=36.0 f/s, elapsed=5:56:31\n",
      "Episode 91900: reward=-0, steps=8, speed=36.0 f/s, elapsed=5:57:03\n",
      "Episode 92000: reward=-0, steps=7, speed=36.0 f/s, elapsed=5:57:37\n",
      "Episode 92100: reward=-0, steps=3, speed=36.1 f/s, elapsed=5:58:12\n",
      "Episode 92200: reward=-0, steps=10, speed=36.0 f/s, elapsed=5:58:53\n",
      "Episode 92300: reward=-0, steps=7, speed=36.0 f/s, elapsed=5:59:28\n",
      "Episode 92400: reward=-0, steps=7, speed=36.0 f/s, elapsed=6:00:01\n",
      "670000: tst: {'episode_reward': 0.006742965547089261, 'episode_steps': 20.43, 'order_profits': 0.0046199271027582214, 'order_steps': 13.74}\n",
      "670000: val: {'episode_reward': 0.028187578790621746, 'episode_steps': 16.69, 'order_profits': 0.02436032511715672, 'order_steps': 12.22}\n",
      "Episode 92500: reward=-1, steps=3, speed=36.0 f/s, elapsed=6:00:39\n",
      "Episode 92600: reward=0, steps=32, speed=35.9 f/s, elapsed=6:01:18\n",
      "Episode 92700: reward=-1, steps=8, speed=35.9 f/s, elapsed=6:01:53\n",
      "Episode 92800: reward=-0, steps=20, speed=35.8 f/s, elapsed=6:02:34\n",
      "Episode 92900: reward=0, steps=7, speed=35.9 f/s, elapsed=6:03:12\n",
      "Episode 93000: reward=1, steps=6, speed=35.9 f/s, elapsed=6:03:48\n",
      "Episode 93100: reward=-0, steps=9, speed=35.9 f/s, elapsed=6:04:25\n",
      "680000: tst: {'episode_reward': 0.26408986653601274, 'episode_steps': 89.53, 'order_profits': 0.2649607332350209, 'order_steps': 48.03}\n",
      "680000: val: {'episode_reward': -0.03408597191688481, 'episode_steps': 67.28, 'order_profits': -0.03306780179145989, 'order_steps': 47.49}\n",
      "Episode 93200: reward=0, steps=20, speed=35.7 f/s, elapsed=6:05:22\n",
      "Episode 93300: reward=-0, steps=15, speed=35.7 f/s, elapsed=6:06:05\n",
      "Episode 93400: reward=-0, steps=26, speed=35.7 f/s, elapsed=6:06:47\n",
      "Episode 93500: reward=1, steps=11, speed=35.7 f/s, elapsed=6:07:22\n",
      "Episode 93600: reward=-1, steps=9, speed=35.7 f/s, elapsed=6:08:04\n",
      "Episode 93700: reward=-0, steps=16, speed=35.7 f/s, elapsed=6:08:43\n",
      "Episode 93800: reward=1, steps=31, speed=35.7 f/s, elapsed=6:09:27\n",
      "690000: tst: {'episode_reward': 0.04190113524367653, 'episode_steps': 31.0, 'order_profits': 0.0405961074962548, 'order_steps': 5.93}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "690000: val: {'episode_reward': -0.007669903562203915, 'episode_steps': 19.99, 'order_profits': -0.007904272724344132, 'order_steps': 6.32}\n",
      "Episode 93900: reward=0, steps=5, speed=35.6 f/s, elapsed=6:10:11\n",
      "Episode 94000: reward=-1, steps=15, speed=35.6 f/s, elapsed=6:10:48\n",
      "Episode 94100: reward=-1, steps=10, speed=35.6 f/s, elapsed=6:11:22\n",
      "Episode 94200: reward=-0, steps=2, speed=35.6 f/s, elapsed=6:11:57\n",
      "Episode 94300: reward=0, steps=20, speed=35.6 f/s, elapsed=6:12:31\n",
      "Episode 94400: reward=-0, steps=7, speed=35.6 f/s, elapsed=6:13:01\n",
      "Episode 94500: reward=-0, steps=4, speed=35.6 f/s, elapsed=6:13:33\n",
      "Episode 94600: reward=-1, steps=29, speed=35.7 f/s, elapsed=6:14:07\n",
      "700000: tst: {'episode_reward': -0.09356010700276145, 'episode_steps': 19.07, 'order_profits': -0.09438654645209638, 'order_steps': 10.16}\n",
      "700000: val: {'episode_reward': 0.0519911441467967, 'episode_steps': 10.85, 'order_profits': 0.050527127862113816, 'order_steps': 4.25}\n",
      "Episode 94700: reward=-0, steps=19, speed=35.6 f/s, elapsed=6:14:42\n",
      "Episode 94800: reward=-0, steps=2, speed=35.6 f/s, elapsed=6:15:10\n",
      "Episode 94900: reward=0, steps=59, speed=35.7 f/s, elapsed=6:15:40\n",
      "Episode 95000: reward=-0, steps=14, speed=35.7 f/s, elapsed=6:16:12\n",
      "Episode 95100: reward=-0, steps=22, speed=35.7 f/s, elapsed=6:16:43\n",
      "Episode 95200: reward=-0, steps=33, speed=35.7 f/s, elapsed=6:17:17\n",
      "Episode 95300: reward=-0, steps=7, speed=35.7 f/s, elapsed=6:17:49\n",
      "Episode 95400: reward=-0, steps=8, speed=35.7 f/s, elapsed=6:18:30\n",
      "Episode 95500: reward=0, steps=10, speed=35.7 f/s, elapsed=6:19:14\n",
      "710000: tst: {'episode_reward': 0.021106013989944314, 'episode_steps': 51.94, 'order_profits': 0.017471253202034534, 'order_steps': 42.0}\n",
      "710000: val: {'episode_reward': 0.03590260591372318, 'episode_steps': 24.7, 'order_profits': 0.03284311300257604, 'order_steps': 16.11}\n",
      "Episode 95600: reward=-0, steps=23, speed=35.6 f/s, elapsed=6:19:54\n",
      "Episode 95700: reward=0, steps=7, speed=35.6 f/s, elapsed=6:20:28\n",
      "Episode 95800: reward=0, steps=21, speed=35.6 f/s, elapsed=6:21:10\n",
      "Episode 95900: reward=-0, steps=4, speed=35.6 f/s, elapsed=6:21:54\n",
      "Episode 96000: reward=-0, steps=7, speed=35.6 f/s, elapsed=6:22:32\n",
      "Episode 96100: reward=0, steps=15, speed=35.6 f/s, elapsed=6:23:09\n",
      "Episode 96200: reward=-1, steps=10, speed=35.6 f/s, elapsed=6:23:42\n",
      "720000: tst: {'episode_reward': 0.0511839592701458, 'episode_steps': 32.24, 'order_profits': 0.04884143916358252, 'order_steps': 13.7}\n",
      "720000: val: {'episode_reward': 0.0067623665024585475, 'episode_steps': 21.33, 'order_profits': 0.0024705289852410437, 'order_steps': 10.78}\n",
      "Episode 96300: reward=0, steps=11, speed=35.6 f/s, elapsed=6:24:28\n",
      "Episode 96400: reward=-0, steps=28, speed=35.6 f/s, elapsed=6:25:05\n",
      "Episode 96500: reward=1, steps=8, speed=35.6 f/s, elapsed=6:25:41\n",
      "Episode 96600: reward=0, steps=22, speed=35.6 f/s, elapsed=6:26:22\n",
      "Episode 96700: reward=-2, steps=22, speed=35.6 f/s, elapsed=6:27:02\n",
      "Episode 96800: reward=-0, steps=10, speed=35.6 f/s, elapsed=6:27:41\n",
      "Episode 96900: reward=-0, steps=10, speed=35.6 f/s, elapsed=6:28:21\n",
      "730000: tst: {'episode_reward': 0.05360929165940988, 'episode_steps': 57.15, 'order_profits': 0.04902086360377636, 'order_steps': 45.33}\n",
      "730000: val: {'episode_reward': 0.0649144846673513, 'episode_steps': 39.83, 'order_profits': 0.058409814047654975, 'order_steps': 29.5}\n",
      "Episode 97000: reward=-0, steps=5, speed=35.5 f/s, elapsed=6:29:11\n",
      "Episode 97100: reward=-1, steps=35, speed=35.5 f/s, elapsed=6:29:56\n",
      "Episode 97200: reward=1, steps=35, speed=35.5 f/s, elapsed=6:30:39\n",
      "Episode 97300: reward=-0, steps=2, speed=35.4 f/s, elapsed=6:31:32\n",
      "Episode 97400: reward=0, steps=5, speed=35.4 f/s, elapsed=6:32:12\n",
      "Episode 97500: reward=-0, steps=22, speed=35.4 f/s, elapsed=6:32:57\n",
      "740000: tst: {'episode_reward': -0.10119795018962197, 'episode_steps': 88.16, 'order_profits': -0.10767629142615162, 'order_steps': 68.96}\n",
      "740000: val: {'episode_reward': -0.08364449401760148, 'episode_steps': 59.84, 'order_profits': -0.0921120644297023, 'order_steps': 46.08}\n",
      "Episode 97600: reward=1, steps=28, speed=35.2 f/s, elapsed=6:33:57\n",
      "Episode 97700: reward=0, steps=10, speed=35.2 f/s, elapsed=6:34:51\n",
      "Episode 97800: reward=0, steps=8, speed=35.2 f/s, elapsed=6:35:40\n",
      "Episode 97900: reward=1, steps=31, speed=35.2 f/s, elapsed=6:36:22\n",
      "Episode 98000: reward=0, steps=6, speed=35.2 f/s, elapsed=6:36:59\n",
      "Episode 98100: reward=-1, steps=12, speed=35.2 f/s, elapsed=6:37:35\n",
      "Episode 98200: reward=0, steps=5, speed=35.3 f/s, elapsed=6:38:13\n",
      "750000: tst: {'episode_reward': -0.028753096007380664, 'episode_steps': 18.0, 'order_profits': -0.02872486465552011, 'order_steps': 4.6}\n",
      "750000: val: {'episode_reward': 0.0760941258301577, 'episode_steps': 9.75, 'order_profits': 0.07438348072150777, 'order_steps': 3.03}\n",
      "Episode 98300: reward=0, steps=8, speed=35.2 f/s, elapsed=6:38:49\n",
      "Episode 98400: reward=-0, steps=23, speed=35.2 f/s, elapsed=6:39:26\n",
      "Episode 98500: reward=0, steps=11, speed=35.2 f/s, elapsed=6:40:21\n",
      "Episode 98600: reward=1, steps=6, speed=35.2 f/s, elapsed=6:41:13\n",
      "Episode 98700: reward=-0, steps=20, speed=35.2 f/s, elapsed=6:41:55\n",
      "Episode 98800: reward=3, steps=7, speed=35.3 f/s, elapsed=6:42:40\n",
      "759000: Best mean value updated -0.022 -> -0.007\n",
      "760000: tst: {'episode_reward': -0.28281720551718803, 'episode_steps': 114.49, 'order_profits': -0.29298655712187566, 'order_steps': 106.89}\n",
      "760000: val: {'episode_reward': -0.36181829804596916, 'episode_steps': 69.02, 'order_profits': -0.3699669336218603, 'order_steps': 61.96}\n",
      "Episode 98900: reward=0, steps=18, speed=35.1 f/s, elapsed=6:43:51\n",
      "Episode 99000: reward=0, steps=26, speed=35.1 f/s, elapsed=6:44:44\n",
      "Episode 99100: reward=-1, steps=13, speed=35.0 f/s, elapsed=6:45:39\n",
      "Episode 99200: reward=0, steps=11, speed=35.0 f/s, elapsed=6:46:32\n",
      "Episode 99300: reward=-1, steps=13, speed=35.0 f/s, elapsed=6:47:24\n",
      "Episode 99400: reward=-1, steps=5, speed=35.0 f/s, elapsed=6:48:32\n",
      "770000: tst: {'episode_reward': 0.02270951903095412, 'episode_steps': 80.44, 'order_profits': 0.022952308392089903, 'order_steps': 66.92}\n",
      "770000: val: {'episode_reward': -0.19693370474882382, 'episode_steps': 61.82, 'order_profits': -0.2051197036385188, 'order_steps': 54.38}\n",
      "Episode 99500: reward=-0, steps=16, speed=34.7 f/s, elapsed=6:49:53\n",
      "Episode 99600: reward=-0, steps=18, speed=34.8 f/s, elapsed=6:50:47\n",
      "Episode 99700: reward=-0, steps=11, speed=34.8 f/s, elapsed=6:51:37\n",
      "Episode 99800: reward=0, steps=60, speed=34.8 f/s, elapsed=6:52:29\n",
      "Episode 99900: reward=-2, steps=21, speed=34.7 f/s, elapsed=6:53:30\n",
      "780000: tst: {'episode_reward': 0.001463319524982299, 'episode_steps': 39.24, 'order_profits': 0.0008659544737812242, 'order_steps': 7.32}\n",
      "780000: val: {'episode_reward': 0.0025704515166889343, 'episode_steps': 21.22, 'order_profits': 0.0019349279199997428, 'order_steps': 4.94}\n",
      "Episode 100000: reward=-0, steps=15, speed=34.6 f/s, elapsed=6:54:23\n",
      "Episode 100100: reward=1, steps=23, speed=34.7 f/s, elapsed=6:55:00\n",
      "Episode 100200: reward=-0, steps=2, speed=34.7 f/s, elapsed=6:55:44\n",
      "Episode 100300: reward=-0, steps=15, speed=34.7 f/s, elapsed=6:56:25\n",
      "Episode 100400: reward=0, steps=2, speed=34.7 f/s, elapsed=6:57:13\n",
      "Episode 100500: reward=-0, steps=5, speed=34.8 f/s, elapsed=6:57:51\n",
      "Episode 100600: reward=-0, steps=2, speed=34.8 f/s, elapsed=6:58:29\n",
      "790000: tst: {'episode_reward': 0.06087454119624533, 'episode_steps': 25.66, 'order_profits': 0.059744590054650965, 'order_steps': 7.65}\n",
      "790000: val: {'episode_reward': -0.12160180258761258, 'episode_steps': 21.18, 'order_profits': -0.12240745679036145, 'order_steps': 6.79}\n",
      "Episode 100700: reward=0, steps=3, speed=34.7 f/s, elapsed=6:59:12\n",
      "Episode 100800: reward=-0, steps=17, speed=34.7 f/s, elapsed=6:59:56\n",
      "Episode 100900: reward=-0, steps=10, speed=34.7 f/s, elapsed=7:00:36\n",
      "Episode 101000: reward=-0, steps=13, speed=34.7 f/s, elapsed=7:01:12\n",
      "Episode 101100: reward=0, steps=6, speed=34.7 f/s, elapsed=7:01:51\n",
      "Episode 101200: reward=0, steps=4, speed=34.7 f/s, elapsed=7:02:29\n",
      "Episode 101300: reward=0, steps=32, speed=34.7 f/s, elapsed=7:03:11\n",
      "800000: tst: {'episode_reward': 0.02709767029758309, 'episode_steps': 18.32, 'order_profits': 0.02625145775069637, 'order_steps': 7.69}\n",
      "800000: val: {'episode_reward': -0.004363367628884409, 'episode_steps': 12.44, 'order_profits': -0.004893124018753178, 'order_steps': 4.69}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 101400: reward=-0, steps=28, speed=34.7 f/s, elapsed=7:03:55\n",
      "Episode 101500: reward=0, steps=9, speed=34.7 f/s, elapsed=7:04:45\n",
      "Episode 101600: reward=-0, steps=2, speed=34.7 f/s, elapsed=7:05:38\n",
      "Episode 101700: reward=0, steps=7, speed=34.7 f/s, elapsed=7:06:20\n",
      "Episode 101800: reward=0, steps=15, speed=34.7 f/s, elapsed=7:07:10\n",
      "Episode 101900: reward=-0, steps=15, speed=34.6 f/s, elapsed=7:46:10\n",
      "810000: tst: {'episode_reward': -0.06847994401602567, 'episode_steps': 51.26, 'order_profits': -0.07258846788413703, 'order_steps': 40.45}\n",
      "810000: val: {'episode_reward': -0.2057063352002053, 'episode_steps': 27.26, 'order_profits': -0.20817638319151166, 'order_steps': 19.08}\n",
      "Episode 102000: reward=0, steps=8, speed=34.5 f/s, elapsed=7:47:19\n",
      "Episode 102100: reward=1, steps=41, speed=34.5 f/s, elapsed=7:48:09\n",
      "Episode 102200: reward=0, steps=9, speed=34.5 f/s, elapsed=7:48:57\n",
      "Episode 102300: reward=-2, steps=29, speed=34.4 f/s, elapsed=7:49:42\n",
      "Episode 102400: reward=1, steps=33, speed=34.4 f/s, elapsed=7:50:34\n",
      "Episode 102500: reward=-1, steps=22, speed=34.4 f/s, elapsed=7:51:19\n",
      "820000: tst: {'episode_reward': -0.010658799559200191, 'episode_steps': 38.07, 'order_profits': -0.011890139059652883, 'order_steps': 10.66}\n",
      "820000: val: {'episode_reward': -0.08428116901971298, 'episode_steps': 23.11, 'order_profits': -0.08428990675597905, 'order_steps': 4.83}\n",
      "Episode 102600: reward=0, steps=4, speed=34.3 f/s, elapsed=7:52:07\n",
      "Episode 102700: reward=0, steps=38, speed=34.3 f/s, elapsed=7:52:53\n",
      "Episode 102800: reward=-3, steps=8, speed=34.3 f/s, elapsed=7:53:36\n",
      "Episode 102900: reward=0, steps=10, speed=34.2 f/s, elapsed=7:54:19\n",
      "Episode 103000: reward=-0, steps=5, speed=34.2 f/s, elapsed=7:55:01\n",
      "Episode 103100: reward=-0, steps=29, speed=34.2 f/s, elapsed=7:55:59\n",
      "830000: tst: {'episode_reward': 0.23918551041019342, 'episode_steps': 47.96, 'order_profits': 0.2366526586618265, 'order_steps': 29.98}\n",
      "830000: val: {'episode_reward': 0.019856212218180236, 'episode_steps': 24.67, 'order_profits': 0.01731937852530923, 'order_steps': 15.64}\n",
      "Episode 103200: reward=-1, steps=8, speed=34.1 f/s, elapsed=7:57:15\n",
      "Episode 103300: reward=0, steps=20, speed=34.1 f/s, elapsed=7:58:11\n",
      "Episode 103400: reward=0, steps=22, speed=34.0 f/s, elapsed=7:59:29\n",
      "Episode 103500: reward=0, steps=10, speed=34.0 f/s, elapsed=8:00:28\n",
      "Episode 103600: reward=0, steps=12, speed=34.0 f/s, elapsed=8:01:23\n",
      "840000: tst: {'episode_reward': 0.09092831164127971, 'episode_steps': 42.19, 'order_profits': 0.08924127535201933, 'order_steps': 22.61}\n",
      "840000: val: {'episode_reward': -0.06390653093286945, 'episode_steps': 31.24, 'order_profits': -0.06618192236812474, 'order_steps': 15.63}\n",
      "Episode 103700: reward=0, steps=15, speed=33.9 f/s, elapsed=8:02:40\n",
      "Episode 103800: reward=2, steps=26, speed=33.9 f/s, elapsed=8:03:51\n",
      "Episode 103900: reward=0, steps=10, speed=33.9 f/s, elapsed=8:05:09\n",
      "Episode 104000: reward=0, steps=12, speed=33.9 f/s, elapsed=8:06:22\n",
      "850000: tst: {'episode_reward': 0.031176180867516824, 'episode_steps': 35.49, 'order_profits': 0.0287478704180986, 'order_steps': 17.61}\n",
      "850000: val: {'episode_reward': 0.04755303927764868, 'episode_steps': 19.93, 'order_profits': 0.04635695243794211, 'order_steps': 10.02}\n",
      "Episode 104100: reward=-0, steps=14, speed=33.8 f/s, elapsed=8:07:30\n",
      "Episode 104200: reward=-0, steps=18, speed=33.8 f/s, elapsed=8:08:35\n",
      "Episode 104300: reward=0, steps=7, speed=33.7 f/s, elapsed=8:09:54\n",
      "Episode 104400: reward=0, steps=16, speed=33.7 f/s, elapsed=8:10:48\n",
      "Episode 104500: reward=-0, steps=15, speed=33.7 f/s, elapsed=8:11:33\n",
      "860000: tst: {'episode_reward': -0.11618158982157738, 'episode_steps': 19.44, 'order_profits': -0.11665244729432031, 'order_steps': 5.78}\n",
      "860000: val: {'episode_reward': -0.025259532880139754, 'episode_steps': 15.4, 'order_profits': -0.025930510829277678, 'order_steps': 4.49}\n",
      "Episode 104600: reward=-0, steps=14, speed=33.6 f/s, elapsed=8:12:40\n",
      "Episode 104700: reward=0, steps=3, speed=33.5 f/s, elapsed=8:13:43\n",
      "Episode 104800: reward=-0, steps=2, speed=33.5 f/s, elapsed=8:14:47\n",
      "Episode 104900: reward=-0, steps=28, speed=33.5 f/s, elapsed=8:15:42\n",
      "Episode 105000: reward=-1, steps=30, speed=33.5 f/s, elapsed=8:16:47\n",
      "870000: tst: {'episode_reward': 0.07506516355075231, 'episode_steps': 40.36, 'order_profits': 0.0740003885435765, 'order_steps': 26.49}\n",
      "870000: val: {'episode_reward': -0.07521731450006994, 'episode_steps': 25.44, 'order_profits': -0.079097789453377, 'order_steps': 15.75}\n",
      "Episode 105100: reward=1, steps=12, speed=33.4 f/s, elapsed=8:18:00\n",
      "Episode 105200: reward=-0, steps=3, speed=33.4 f/s, elapsed=8:19:06\n",
      "Episode 105300: reward=0, steps=5, speed=33.3 f/s, elapsed=8:20:27\n",
      "Episode 105400: reward=0, steps=37, speed=33.3 f/s, elapsed=8:21:58\n",
      "Episode 105500: reward=0, steps=13, speed=33.2 f/s, elapsed=8:22:58\n",
      "880000: tst: {'episode_reward': -0.06163614321023325, 'episode_steps': 52.31, 'order_profits': -0.06404525679494143, 'order_steps': 22.050505050505052}\n",
      "880000: val: {'episode_reward': -0.2569547150062418, 'episode_steps': 30.81, 'order_profits': -0.2576155592455309, 'order_steps': 10.39}\n",
      "Episode 105600: reward=-0, steps=6, speed=33.1 f/s, elapsed=8:24:34\n",
      "Episode 105700: reward=-1, steps=21, speed=32.9 f/s, elapsed=8:26:13\n",
      "Episode 105800: reward=-0, steps=41, speed=32.8 f/s, elapsed=8:27:55\n",
      "890000: tst: {'episode_reward': 0.17931671437757693, 'episode_steps': 28.59, 'order_profits': 0.17822959966380034, 'order_steps': 16.4}\n",
      "890000: val: {'episode_reward': 0.06222541980973104, 'episode_steps': 20.68, 'order_profits': 0.0603573941109655, 'order_steps': 11.84}\n",
      "Episode 105900: reward=1, steps=79, speed=32.8 f/s, elapsed=8:29:27\n",
      "Episode 106000: reward=2, steps=53, speed=32.7 f/s, elapsed=8:31:02\n",
      "Episode 106100: reward=-0, steps=14, speed=32.8 f/s, elapsed=8:32:17\n",
      "Episode 106200: reward=-0, steps=3, speed=32.7 f/s, elapsed=8:33:37\n",
      "900000: tst: {'episode_reward': -0.07210531510764326, 'episode_steps': 62.15, 'order_profits': -0.07513456538901685, 'order_steps': 38.28}\n",
      "900000: val: {'episode_reward': -0.02595334921890037, 'episode_steps': 35.63, 'order_profits': -0.029205307543562604, 'order_steps': 21.66}\n",
      "Episode 106300: reward=0, steps=5, speed=32.6 f/s, elapsed=8:35:39\n",
      "Episode 106400: reward=-0, steps=9, speed=32.5 f/s, elapsed=8:37:19\n",
      "Episode 106500: reward=0, steps=23, speed=32.5 f/s, elapsed=8:38:44\n",
      "Episode 106600: reward=-1, steps=9, speed=32.5 f/s, elapsed=8:40:16\n",
      "910000: tst: {'episode_reward': 0.09834128622908611, 'episode_steps': 54.72, 'order_profits': 0.09274926181082016, 'order_steps': 37.09}\n",
      "910000: val: {'episode_reward': -0.042892928339395404, 'episode_steps': 41.54, 'order_profits': -0.04845680652462663, 'order_steps': 26.45}\n",
      "911000: Best mean value updated -0.007 -> 0.000\n",
      "913000: Best mean value updated 0.000 -> 0.021\n",
      "Episode 106700: reward=-1, steps=7, speed=32.4 f/s, elapsed=8:42:15\n",
      "914000: Best mean value updated 0.021 -> 0.030\n",
      "915000: Best mean value updated 0.030 -> 0.046\n",
      "Episode 106800: reward=-2, steps=131, speed=32.4 f/s, elapsed=8:44:14\n",
      "920000: tst: {'episode_reward': 0.1531308771211873, 'episode_steps': 132.74, 'order_profits': 0.14738296932854278, 'order_steps': 95.08}\n",
      "920000: val: {'episode_reward': 0.05730594087498739, 'episode_steps': 87.75, 'order_profits': 0.04488151995058983, 'order_steps': 60.54}\n",
      "Episode 106900: reward=2, steps=28, speed=32.3 f/s, elapsed=8:46:49\n",
      "Episode 107000: reward=-1, steps=10, speed=32.3 f/s, elapsed=8:48:35\n",
      "Episode 107100: reward=-1, steps=17, speed=32.4 f/s, elapsed=8:50:34\n",
      "930000: tst: {'episode_reward': 0.13014562439432903, 'episode_steps': 82.29, 'order_profits': 0.12090712552013178, 'order_steps': 70.63}\n",
      "930000: val: {'episode_reward': -0.04684019632556418, 'episode_steps': 55.48, 'order_profits': -0.050955359024939396, 'order_steps': 46.58}\n",
      "Episode 107200: reward=-1, steps=17, speed=32.3 f/s, elapsed=8:52:39\n",
      "Episode 107300: reward=-1, steps=38, speed=32.5 f/s, elapsed=8:54:14\n",
      "Episode 107400: reward=1, steps=15, speed=32.6 f/s, elapsed=8:55:45\n",
      "940000: tst: {'episode_reward': -0.014566980552162084, 'episode_steps': 82.23, 'order_profits': -0.01982621851342482, 'order_steps': 49.44}\n",
      "940000: val: {'episode_reward': 0.0001291636565444565, 'episode_steps': 57.36, 'order_profits': -0.007407310990431225, 'order_steps': 39.58}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 107500: reward=0, steps=14, speed=32.6 f/s, elapsed=8:57:33\n",
      "Episode 107600: reward=-1, steps=37, speed=32.7 f/s, elapsed=8:59:11\n",
      "950000: tst: {'episode_reward': 0.05924275910151502, 'episode_steps': 62.02, 'order_profits': 0.05488487475532985, 'order_steps': 37.22222222222222}\n",
      "950000: val: {'episode_reward': -0.12495185550290261, 'episode_steps': 37.96, 'order_profits': -0.1270155651748269, 'order_steps': 22.49}\n",
      "Episode 107700: reward=0, steps=24, speed=32.8 f/s, elapsed=9:00:51\n",
      "Episode 107800: reward=-0, steps=12, speed=32.9 f/s, elapsed=9:02:25\n",
      "Episode 107900: reward=-1, steps=26, speed=33.0 f/s, elapsed=9:38:14\n",
      "960000: tst: {'episode_reward': 0.0540840258067015, 'episode_steps': 73.28, 'order_profits': 0.050892932867146384, 'order_steps': 34.98}\n",
      "960000: val: {'episode_reward': 0.10679937172385104, 'episode_steps': 40.69, 'order_profits': 0.1000055311244093, 'order_steps': 24.85}\n",
      "Episode 108000: reward=-0, steps=24, speed=33.0 f/s, elapsed=10:26:58\n",
      "Episode 108100: reward=1, steps=159, speed=33.1 f/s, elapsed=10:45:18\n",
      "Episode 108200: reward=-0, steps=10, speed=33.1 f/s, elapsed=11:08:31\n",
      "970000: tst: {'episode_reward': -0.2006280017600692, 'episode_steps': 102.07, 'order_profits': -0.21019560945023363, 'order_steps': 86.63}\n",
      "970000: val: {'episode_reward': -0.21957766520522656, 'episode_steps': 79.44, 'order_profits': -0.23441001190482255, 'order_steps': 68.33}\n",
      "Episode 108300: reward=1, steps=16, speed=33.1 f/s, elapsed=16:31:25\n",
      "975000: Best mean value updated 0.046 -> 0.058\n",
      "Episode 108400: reward=1, steps=16, speed=33.1 f/s, elapsed=16:45:06\n",
      "Episode 108500: reward=0, steps=17, speed=33.2 f/s, elapsed=17:22:42\n",
      "980000: tst: {'episode_reward': 0.16658747167936044, 'episode_steps': 94.33, 'order_profits': 0.15419162005700784, 'order_steps': 77.76}\n",
      "980000: val: {'episode_reward': -0.17862204096274503, 'episode_steps': 62.85, 'order_profits': -0.19021620008003867, 'order_steps': 53.72}\n",
      "Episode 108600: reward=0, steps=12, speed=33.2 f/s, elapsed=17:55:45\n",
      "Episode 108700: reward=0, steps=13, speed=33.3 f/s, elapsed=18:28:51\n",
      "990000: tst: {'episode_reward': -0.09542587667924562, 'episode_steps': 93.72, 'order_profits': -0.09781264305414238, 'order_steps': 72.77}\n",
      "990000: val: {'episode_reward': -0.10598942821549956, 'episode_steps': 75.42, 'order_profits': -0.12264512410850642, 'order_steps': 65.26}\n",
      "Episode 108800: reward=1, steps=21, speed=33.2 f/s, elapsed=19:28:31\n",
      "991000: Best mean value updated 0.058 -> 0.063\n",
      "992000: Best mean value updated 0.063 -> 0.075\n",
      "Episode 108900: reward=0, steps=10, speed=33.3 f/s, elapsed=19:50:40\n",
      "994000: Best mean value updated 0.075 -> 0.080\n",
      "995000: Best mean value updated 0.080 -> 0.081\n",
      "996000: Best mean value updated 0.081 -> 0.090\n",
      "997000: Best mean value updated 0.090 -> 0.096\n",
      "Episode 109000: reward=-0, steps=10, speed=33.4 f/s, elapsed=20:38:24\n",
      "1000000: tst: {'episode_reward': -0.046220658893927354, 'episode_steps': 125.4, 'order_profits': -0.050093117948895344, 'order_steps': 87.81}\n",
      "1000000: val: {'episode_reward': -0.03208191377956058, 'episode_steps': 84.21, 'order_profits': -0.04579800487939176, 'order_steps': 59.37}\n",
      "Episode 109100: reward=-0, steps=27, speed=33.3 f/s, elapsed=21:46:49\n",
      "Episode 109200: reward=1, steps=22, speed=33.3 f/s, elapsed=22:06:31\n",
      "1010000: tst: {'episode_reward': 0.050479144777139454, 'episode_steps': 101.91, 'order_profits': 0.045865353530163605, 'order_steps': 71.68}\n",
      "1010000: val: {'episode_reward': -0.08441958356754112, 'episode_steps': 67.72, 'order_profits': -0.09398171112223075, 'order_steps': 50.99}\n",
      "Episode 109300: reward=0, steps=10, speed=33.1 f/s, elapsed=22:09:14\n",
      "Episode 109400: reward=1, steps=16, speed=33.0 f/s, elapsed=22:11:29\n",
      "Episode 109500: reward=0, steps=27, speed=33.0 f/s, elapsed=22:13:31\n",
      "1020000: tst: {'episode_reward': 0.1652789864585717, 'episode_steps': 74.28, 'order_profits': 0.1607718309901951, 'order_steps': 58.34}\n",
      "1020000: val: {'episode_reward': -0.11099672143845098, 'episode_steps': 57.89, 'order_profits': -0.11976129258437335, 'order_steps': 43.91}\n",
      "Episode 109600: reward=1, steps=35, speed=32.8 f/s, elapsed=22:15:57\n",
      "Episode 109700: reward=-0, steps=99, speed=32.7 f/s, elapsed=22:17:52\n",
      "Episode 109800: reward=-0, steps=9, speed=32.6 f/s, elapsed=22:19:55\n",
      "1030000: tst: {'episode_reward': 0.5457552055229542, 'episode_steps': 75.03, 'order_profits': 0.5410158499045793, 'order_steps': 48.6}\n",
      "1030000: val: {'episode_reward': -0.011582496488435677, 'episode_steps': 66.3, 'order_profits': -0.017818879632714547, 'order_steps': 46.73}\n",
      "Episode 109900: reward=-0, steps=10, speed=32.4 f/s, elapsed=22:22:35\n",
      "Episode 110000: reward=-0, steps=29, speed=32.3 f/s, elapsed=22:25:04\n",
      "Episode 110100: reward=-1, steps=27, speed=32.2 f/s, elapsed=22:27:31\n",
      "1040000: tst: {'episode_reward': 0.05216758027761999, 'episode_steps': 110.67, 'order_profits': 0.044889756243878656, 'order_steps': 83.44}\n",
      "1040000: val: {'episode_reward': 0.14773819242925684, 'episode_steps': 66.47, 'order_profits': 0.13681490618575967, 'order_steps': 49.73}\n",
      "Episode 110200: reward=0, steps=7, speed=32.1 f/s, elapsed=22:29:37\n",
      "Episode 110300: reward=0, steps=9, speed=32.0 f/s, elapsed=22:31:43\n",
      "1050000: tst: {'episode_reward': 0.22620333267789636, 'episode_steps': 89.64, 'order_profits': 0.22012010685239264, 'order_steps': 64.79}\n",
      "1050000: val: {'episode_reward': -0.3602967565183714, 'episode_steps': 75.34, 'order_profits': -0.365721554631844, 'order_steps': 60.13}\n",
      "Episode 110400: reward=-1, steps=33, speed=31.9 f/s, elapsed=22:33:49\n",
      "Episode 110500: reward=3, steps=15, speed=32.1 f/s, elapsed=22:35:36\n",
      "Episode 110600: reward=1, steps=14, speed=32.2 f/s, elapsed=22:37:36\n",
      "1060000: tst: {'episode_reward': 0.1255458444431257, 'episode_steps': 129.52, 'order_profits': 0.12412250681193698, 'order_steps': 84.19}\n",
      "1060000: val: {'episode_reward': 0.07829584435913879, 'episode_steps': 74.57, 'order_profits': 0.06583222604981075, 'order_steps': 47.56}\n",
      "Episode 110700: reward=-1, steps=69, speed=32.2 f/s, elapsed=23:10:01\n",
      "Episode 110800: reward=-0, steps=74, speed=32.3 f/s, elapsed=23:12:56\n",
      "Episode 110900: reward=-1, steps=11, speed=32.2 f/s, elapsed=23:17:13\n",
      "1070000: tst: {'episode_reward': -0.04632661100586901, 'episode_steps': 93.29, 'order_profits': -0.052282554278763375, 'order_steps': 80.08}\n",
      "1070000: val: {'episode_reward': -0.4182234972234429, 'episode_steps': 73.51, 'order_profits': -0.4299993597635903, 'order_steps': 64.78}\n",
      "1071000: Best mean value updated 0.096 -> 0.097\n",
      "Episode 111000: reward=-1, steps=46, speed=32.1 f/s, elapsed=23:19:43\n",
      "Episode 111100: reward=1, steps=124, speed=32.0 f/s, elapsed=23:22:18\n",
      "1080000: tst: {'episode_reward': 0.045824198144181434, 'episode_steps': 95.3, 'order_profits': 0.035442670660874906, 'order_steps': 76.32}\n",
      "1080000: val: {'episode_reward': -0.324088179505508, 'episode_steps': 76.19, 'order_profits': -0.3288280243663562, 'order_steps': 63.66}\n",
      "Episode 111200: reward=0, steps=39, speed=31.7 f/s, elapsed=23:25:48\n",
      "Episode 111300: reward=-2, steps=56, speed=31.6 f/s, elapsed=23:28:27\n",
      "Episode 111400: reward=-1, steps=8, speed=31.4 f/s, elapsed=23:30:38\n",
      "1090000: tst: {'episode_reward': 0.2606363632975289, 'episode_steps': 120.61, 'order_profits': 0.24971073081774608, 'order_steps': 95.26}\n",
      "1090000: val: {'episode_reward': -0.19367754454939187, 'episode_steps': 91.61, 'order_profits': -0.19971217405731376, 'order_steps': 73.13}\n",
      "Episode 111500: reward=-2, steps=50, speed=31.2 f/s, elapsed=23:33:34\n",
      "Episode 111600: reward=-1, steps=84, speed=31.2 f/s, elapsed=23:35:55\n",
      "1100000: tst: {'episode_reward': 0.08751210825402848, 'episode_steps': 132.0, 'order_profits': 0.07935607401792305, 'order_steps': 87.2}\n",
      "1100000: val: {'episode_reward': 0.07404174995358655, 'episode_steps': 72.45, 'order_profits': 0.0659679566159975, 'order_steps': 50.35}\n",
      "Episode 111700: reward=-0, steps=59, speed=31.0 f/s, elapsed=23:38:56\n",
      "Episode 111800: reward=-0, steps=33, speed=31.0 f/s, elapsed=23:41:08\n",
      "Episode 111900: reward=-1, steps=71, speed=30.9 f/s, elapsed=23:43:43\n",
      "1110000: tst: {'episode_reward': 0.011537548978527545, 'episode_steps': 90.04, 'order_profits': 0.008014036084969802, 'order_steps': 53.57}\n",
      "1110000: val: {'episode_reward': -0.3021242280027339, 'episode_steps': 63.1, 'order_profits': -0.3054143391499336, 'order_steps': 37.84}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 112000: reward=0, steps=64, speed=30.8 f/s, elapsed=23:45:55\n",
      "Episode 112100: reward=-1, steps=49, speed=30.8 f/s, elapsed=23:48:16\n",
      "Episode 112200: reward=1, steps=33, speed=30.7 f/s, elapsed=23:50:42\n",
      "1120000: tst: {'episode_reward': -0.368650213776004, 'episode_steps': 94.54, 'order_profits': -0.371705089821604, 'order_steps': 77.88}\n",
      "1120000: val: {'episode_reward': 0.12844150243537483, 'episode_steps': 61.56, 'order_profits': 0.11730285141377741, 'order_steps': 52.72}\n",
      "Episode 112300: reward=0, steps=91, speed=30.6 f/s, elapsed=23:53:54\n",
      "Episode 112400: reward=1, steps=28, speed=30.6 f/s, elapsed=23:55:59\n",
      "1130000: tst: {'episode_reward': 0.08322463770177403, 'episode_steps': 79.18, 'order_profits': 0.07696247940701074, 'order_steps': 52.98}\n",
      "1130000: val: {'episode_reward': 0.05272128229088062, 'episode_steps': 41.12, 'order_profits': 0.046558346594485156, 'order_steps': 29.15}\n",
      "Episode 112500: reward=0, steps=20, speed=30.5 f/s, elapsed=23:58:12\n",
      "Episode 112600: reward=-0, steps=12, speed=30.4 f/s, elapsed=1 day, 0:00:14\n",
      "Episode 112700: reward=0, steps=21, speed=30.3 f/s, elapsed=1 day, 0:02:59\n",
      "1140000: tst: {'episode_reward': 0.023817234358163467, 'episode_steps': 79.12, 'order_profits': 0.024665146875777852, 'order_steps': 65.53}\n",
      "1140000: val: {'episode_reward': -0.08579377420176833, 'episode_steps': 61.25, 'order_profits': -0.09445879052782924, 'order_steps': 51.44}\n",
      "Episode 112800: reward=-0, steps=24, speed=30.2 f/s, elapsed=1 day, 0:05:41\n",
      "Episode 112900: reward=-0, steps=77, speed=30.2 f/s, elapsed=1 day, 0:07:37\n",
      "1150000: tst: {'episode_reward': 0.0732047987990288, 'episode_steps': 84.8, 'order_profits': 0.06916366327722828, 'order_steps': 51.3}\n",
      "1150000: val: {'episode_reward': 0.03151841943669343, 'episode_steps': 50.74, 'order_profits': 0.02582071844704551, 'order_steps': 33.48}\n",
      "Episode 113000: reward=-0, steps=42, speed=30.1 f/s, elapsed=1 day, 0:10:11\n",
      "Episode 113100: reward=0, steps=36, speed=30.1 f/s, elapsed=1 day, 0:11:52\n",
      "Episode 113200: reward=-0, steps=24, speed=30.0 f/s, elapsed=1 day, 0:14:05\n",
      "1160000: tst: {'episode_reward': 0.17048644172149946, 'episode_steps': 68.36, 'order_profits': 0.16941075339130002, 'order_steps': 32.58163265306123}\n",
      "1160000: val: {'episode_reward': 0.052144964049330694, 'episode_steps': 52.8, 'order_profits': 0.04570974586136743, 'order_steps': 26.54}\n",
      "Episode 113300: reward=0, steps=15, speed=29.9 f/s, elapsed=1 day, 0:16:47\n",
      "Episode 113400: reward=0, steps=107, speed=29.9 f/s, elapsed=1 day, 0:18:45\n",
      "Episode 113500: reward=-0, steps=45, speed=29.9 f/s, elapsed=1 day, 0:20:56\n",
      "1170000: tst: {'episode_reward': 0.03717736858829113, 'episode_steps': 51.88, 'order_profits': 0.034623842348083536, 'order_steps': 19.626262626262626}\n",
      "1170000: val: {'episode_reward': -0.04586663126953068, 'episode_steps': 33.7, 'order_profits': -0.04877964616822327, 'order_steps': 13.06}\n",
      "Episode 113600: reward=-1, steps=85, speed=29.7 f/s, elapsed=1 day, 0:22:48\n",
      "Episode 113700: reward=-0, steps=35, speed=29.7 f/s, elapsed=1 day, 0:24:24\n",
      "Episode 113800: reward=0, steps=18, speed=29.7 f/s, elapsed=1 day, 0:26:15\n",
      "1180000: tst: {'episode_reward': 0.13242772692459834, 'episode_steps': 110.66, 'order_profits': 0.12191886101580347, 'order_steps': 82.8080808080808}\n",
      "1180000: val: {'episode_reward': 0.07272142636313435, 'episode_steps': 61.26, 'order_profits': 0.0639288510979796, 'order_steps': 44.33}\n",
      "Episode 113900: reward=-0, steps=12, speed=29.7 f/s, elapsed=1 day, 0:28:38\n",
      "Episode 114000: reward=2, steps=60, speed=29.8 f/s, elapsed=1 day, 0:30:41\n",
      "Episode 114100: reward=-0, steps=8, speed=29.9 f/s, elapsed=1 day, 0:32:37\n",
      "1190000: tst: {'episode_reward': 0.009594479130365024, 'episode_steps': 82.3, 'order_profits': 0.00030252588450259947, 'order_steps': 58.12}\n",
      "1190000: val: {'episode_reward': 0.10717410175031318, 'episode_steps': 60.01, 'order_profits': 0.09970050309340682, 'order_steps': 43.69}\n",
      "Episode 114200: reward=0, steps=14, speed=29.9 f/s, elapsed=1 day, 0:34:38\n",
      "Episode 114300: reward=1, steps=26, speed=30.0 f/s, elapsed=1 day, 0:36:33\n",
      "1200000: tst: {'episode_reward': -0.04109098784953393, 'episode_steps': 109.21, 'order_profits': -0.0428672118904176, 'order_steps': 70.11111111111111}\n",
      "1200000: val: {'episode_reward': -0.025173847461920928, 'episode_steps': 66.43, 'order_profits': -0.03136030913330199, 'order_steps': 41.96}\n",
      "Episode 114400: reward=1, steps=19, speed=30.0 f/s, elapsed=1 day, 0:38:39\n",
      "Episode 114500: reward=1, steps=47, speed=30.1 f/s, elapsed=1 day, 0:40:28\n",
      "Episode 114600: reward=-0, steps=17, speed=30.2 f/s, elapsed=1 day, 0:42:17\n",
      "1210000: tst: {'episode_reward': 0.05763098958107232, 'episode_steps': 98.25, 'order_profits': 0.04421045405543185, 'order_steps': 89.13}\n",
      "1210000: val: {'episode_reward': -0.14688867590291532, 'episode_steps': 70.48, 'order_profits': -0.15884068918720987, 'order_steps': 62.49}\n",
      "Episode 114700: reward=0, steps=60, speed=30.3 f/s, elapsed=1 day, 0:44:08\n",
      "Episode 114800: reward=1, steps=49, speed=30.5 f/s, elapsed=1 day, 0:45:53\n",
      "Episode 114900: reward=1, steps=48, speed=30.6 f/s, elapsed=1 day, 0:47:37\n",
      "1220000: tst: {'episode_reward': 0.04668357071896363, 'episode_steps': 101.97, 'order_profits': 0.04093955142671976, 'order_steps': 67.73}\n",
      "1220000: val: {'episode_reward': -0.04220385890896515, 'episode_steps': 66.17, 'order_profits': -0.04755705054716555, 'order_steps': 43.44}\n",
      "Episode 115000: reward=1, steps=10, speed=30.6 f/s, elapsed=1 day, 0:49:27\n",
      "Episode 115100: reward=-0, steps=8, speed=30.7 f/s, elapsed=1 day, 0:51:21\n",
      "1230000: tst: {'episode_reward': 0.40022197929621944, 'episode_steps': 117.51, 'order_profits': 0.3904136240364437, 'order_steps': 87.99}\n",
      "1230000: val: {'episode_reward': -0.16513499229444284, 'episode_steps': 68.76, 'order_profits': -0.17082232967655792, 'order_steps': 53.22}\n",
      "Episode 115200: reward=1, steps=24, speed=30.7 f/s, elapsed=1 day, 0:53:26\n",
      "Episode 115300: reward=2, steps=33, speed=30.8 f/s, elapsed=1 day, 0:55:02\n",
      "Episode 115400: reward=-0, steps=17, speed=30.9 f/s, elapsed=1 day, 0:57:02\n",
      "1240000: tst: {'episode_reward': 0.05704677053396214, 'episode_steps': 80.29, 'order_profits': 0.05023761089792252, 'order_steps': 66.13}\n",
      "1240000: val: {'episode_reward': 0.02201040751634975, 'episode_steps': 58.24, 'order_profits': 0.014908527932200356, 'order_steps': 49.42}\n",
      "Episode 115500: reward=-0, steps=18, speed=30.9 f/s, elapsed=1 day, 0:59:10\n",
      "Episode 115600: reward=0, steps=8, speed=31.1 f/s, elapsed=1 day, 1:00:53\n",
      "1250000: Best mean value updated 0.097 -> 0.101\n",
      "1250000: tst: {'episode_reward': -0.23483428213205143, 'episode_steps': 103.96, 'order_profits': -0.24883988933598564, 'order_steps': 96.34}\n",
      "1250000: val: {'episode_reward': -0.03949075392334866, 'episode_steps': 70.35, 'order_profits': -0.05477301691741742, 'order_steps': 62.95}\n",
      "Episode 115700: reward=1, steps=50, speed=31.1 f/s, elapsed=1 day, 1:02:55\n",
      "1251000: Best mean value updated 0.101 -> 0.109\n",
      "Episode 115800: reward=-1, steps=41, speed=31.2 f/s, elapsed=1 day, 1:04:39\n",
      "Episode 115900: reward=0, steps=48, speed=31.4 f/s, elapsed=1 day, 1:06:22\n",
      "1260000: tst: {'episode_reward': 0.556370734843831, 'episode_steps': 136.4, 'order_profits': 0.5412772437065729, 'order_steps': 74.55}\n",
      "1260000: val: {'episode_reward': -0.23577643224081538, 'episode_steps': 81.05, 'order_profits': -0.23995533778554956, 'order_steps': 55.42}\n",
      "Episode 116000: reward=0, steps=26, speed=31.4 f/s, elapsed=1 day, 1:08:33\n",
      "Episode 116100: reward=-0, steps=9, speed=31.5 f/s, elapsed=1 day, 1:11:37\n",
      "1270000: tst: {'episode_reward': 0.4003537782172357, 'episode_steps': 100.82, 'order_profits': 0.39282337853490423, 'order_steps': 65.03}\n",
      "1270000: val: {'episode_reward': -0.4155327551840221, 'episode_steps': 84.15, 'order_profits': -0.4255540395927941, 'order_steps': 66.48}\n",
      "Episode 116200: reward=-0, steps=91, speed=31.5 f/s, elapsed=1 day, 1:15:20\n",
      "Episode 116300: reward=0, steps=13, speed=31.6 f/s, elapsed=1 day, 1:17:13\n",
      "1280000: tst: {'episode_reward': -0.10538040470759169, 'episode_steps': 115.4, 'order_profits': -0.11437591831437559, 'order_steps': 94.74}\n",
      "1280000: val: {'episode_reward': 0.10317053550919641, 'episode_steps': 71.23, 'order_profits': 0.0888750331941663, 'order_steps': 59.48}\n",
      "Episode 116400: reward=1, steps=38, speed=31.5 f/s, elapsed=1 day, 1:19:46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 116500: reward=-0, steps=10, speed=31.6 f/s, elapsed=1 day, 1:21:31\n",
      "Episode 116600: reward=-0, steps=13, speed=31.7 f/s, elapsed=1 day, 1:23:12\n",
      "1290000: tst: {'episode_reward': 0.2740342231037586, 'episode_steps': 78.1, 'order_profits': 0.26405030406869257, 'order_steps': 71.59}\n",
      "1290000: val: {'episode_reward': -0.07880559211274445, 'episode_steps': 64.36, 'order_profits': -0.0898939157330029, 'order_steps': 58.31}\n",
      "Episode 116700: reward=-1, steps=19, speed=31.7 f/s, elapsed=1 day, 1:25:13\n",
      "Episode 116800: reward=-0, steps=36, speed=31.7 f/s, elapsed=1 day, 1:26:51\n",
      "Episode 116900: reward=0, steps=25, speed=31.8 f/s, elapsed=1 day, 1:28:35\n",
      "1300000: tst: {'episode_reward': 0.23823301440808817, 'episode_steps': 84.97, 'order_profits': 0.22765580703972138, 'order_steps': 72.72}\n",
      "1300000: val: {'episode_reward': -0.11208853220365865, 'episode_steps': 74.98, 'order_profits': -0.11165291232348885, 'order_steps': 65.3}\n",
      "Episode 117000: reward=0, steps=5, speed=31.8 f/s, elapsed=1 day, 1:30:39\n",
      "Episode 117100: reward=-2, steps=38, speed=31.9 f/s, elapsed=1 day, 1:32:36\n",
      "1310000: tst: {'episode_reward': 0.34880402927722626, 'episode_steps': 71.49, 'order_profits': 0.34397268123194835, 'order_steps': 41.39}\n",
      "1310000: val: {'episode_reward': -0.0102332925770934, 'episode_steps': 71.82, 'order_profits': -0.015644657309057827, 'order_steps': 51.95}\n",
      "Episode 117200: reward=-0, steps=39, speed=31.9 f/s, elapsed=1 day, 1:34:34\n",
      "Episode 117300: reward=-0, steps=40, speed=32.0 f/s, elapsed=1 day, 1:36:36\n",
      "Episode 117400: reward=1, steps=44, speed=32.1 f/s, elapsed=1 day, 1:38:30\n",
      "1320000: tst: {'episode_reward': -0.06711068541711793, 'episode_steps': 111.27, 'order_profits': -0.07645126667230008, 'order_steps': 84.99}\n",
      "1320000: val: {'episode_reward': -0.10919664079587152, 'episode_steps': 71.16, 'order_profits': -0.11439108484396086, 'order_steps': 57.7}\n",
      "Episode 117500: reward=-0, steps=21, speed=32.0 f/s, elapsed=1 day, 1:40:34\n",
      "Episode 117600: reward=-0, steps=55, speed=32.1 f/s, elapsed=1 day, 1:42:29\n",
      "1330000: tst: {'episode_reward': 0.03871132693886425, 'episode_steps': 112.95, 'order_profits': 0.028596773826895787, 'order_steps': 75.01}\n",
      "1330000: val: {'episode_reward': -0.44063344474645894, 'episode_steps': 74.7, 'order_profits': -0.4468074608926538, 'order_steps': 55.81}\n",
      "Episode 117700: reward=-2, steps=102, speed=32.1 f/s, elapsed=1 day, 1:44:53\n",
      "Episode 117800: reward=0, steps=5, speed=32.1 f/s, elapsed=1 day, 1:46:48\n",
      "Episode 117900: reward=0, steps=31, speed=32.1 f/s, elapsed=1 day, 1:48:41\n",
      "1340000: tst: {'episode_reward': 0.31206779538564733, 'episode_steps': 78.52, 'order_profits': 0.3072360171978974, 'order_steps': 47.39}\n",
      "1340000: val: {'episode_reward': -0.2852152964767263, 'episode_steps': 64.42, 'order_profits': -0.291106075158172, 'order_steps': 45.6}\n",
      "Episode 118000: reward=0, steps=53, speed=32.1 f/s, elapsed=1 day, 1:50:39\n",
      "Episode 118100: reward=0, steps=23, speed=32.1 f/s, elapsed=1 day, 1:52:27\n",
      "Episode 118200: reward=0, steps=15, speed=32.2 f/s, elapsed=1 day, 1:53:50\n",
      "1350000: tst: {'episode_reward': 0.151081398171626, 'episode_steps': 115.05, 'order_profits': 0.13775404161295882, 'order_steps': 105.36}\n",
      "1350000: val: {'episode_reward': -0.05129036236357095, 'episode_steps': 83.57, 'order_profits': -0.05719771784556493, 'order_steps': 75.36}\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Stock path\n",
    "    data = STOCKS\n",
    "    \n",
    "    # Year to train on\n",
    "    year = None\n",
    "    \n",
    "    # Validation path\n",
    "    val = VAL_STOCKS\n",
    "    \n",
    "    # Run name\n",
    "    run_name = \"run_name\"\n",
    "    \n",
    "    # Get the device type\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Saving path\n",
    "    saves_path = SAVES_DIR / f\"conv-{run_name}\"\n",
    "    \n",
    "    # Make a directory for saving\n",
    "    saves_path.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    # Get all the subdirectories for data path\n",
    "    data_path = pathlib.Path(data)\n",
    "    \n",
    "    # Get all the subdirectories for validation path\n",
    "    val_path = pathlib.Path(val)\n",
    "\n",
    "    # If year is defined OR data_path exists\n",
    "    if (year is not None) or data_path.is_file():\n",
    "        \n",
    "        # If year is defined\n",
    "        if (year is not None):\n",
    "            \n",
    "            # Load the training data\n",
    "            stock_data = data_loader.load_year_data(year)\n",
    "            \n",
    "        # If year is NOT defined\n",
    "        else:\n",
    "            \n",
    "            # Load the training data\n",
    "            stock_data = {\"YNDX\": data_loader.load_relative(data_path)}\n",
    "            \n",
    "        # Instantiate the training environment\n",
    "        env = environ.StocksEnv(stock_data, bars_count=BARS_COUNT, state_1d=True)\n",
    "        \n",
    "        # Instantiate the testing environment\n",
    "        env_tst = environ.StocksEnv(stock_data, bars_count=BARS_COUNT, state_1d=True)\n",
    "        \n",
    "    # If data_path exists\n",
    "    elif data_path.is_dir():\n",
    "        \n",
    "        # Instantiate the training environment\n",
    "        env = environ.StocksEnv.from_dir(data_path, bars_count = BARS_COUNT, state_1d = True)\n",
    "        \n",
    "        # Instantiate the testing environment\n",
    "        env_tst = environ.StocksEnv.from_dir(data_path, bars_count = BARS_COUNT, state_1d = True)\n",
    "        \n",
    "    # If year is not defined OR data_path does not exists\n",
    "    else:\n",
    "        \n",
    "        # Raise error\n",
    "        raise RuntimeError(\"No data to train on\")\n",
    "\n",
    "    # Wrap the environment with time limit\n",
    "    env = gym.wrappers.TimeLimit(env, max_episode_steps = 1000)\n",
    "    \n",
    "    # Load the validation data\n",
    "    val_data = {\"YNDX\": data_loader.load_relative(val_path)}\n",
    "    \n",
    "    # Instantiate the validation environment\n",
    "    env_val = environ.StocksEnv(val_data, bars_count = BARS_COUNT, state_1d = True)\n",
    "\n",
    "    # Instantiate the source network\n",
    "    net = models.DQNConv1D(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    \n",
    "    # Instantiate the target network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "    # Instantiate the action selector (epsilon-greedy)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(EPS_START)\n",
    "    \n",
    "    # Instantiate the epsilon tracker\n",
    "    eps_tracker = ptan.actions.EpsilonTracker(selector, EPS_START, EPS_FINAL, EPS_STEPS)\n",
    "    \n",
    "    # Instantiate the DQN agent\n",
    "    agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "    \n",
    "    # Instantiate the experience source (first-last)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, GAMMA, steps_count=REWARD_STEPS)\n",
    "    \n",
    "    # Instantiate the experience replay buffer\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_SIZE)\n",
    "    \n",
    "    # Instantiate the Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Function for processing each batch\n",
    "    def process_batch(engine, batch):\n",
    "        \n",
    "        # Reset the optimizer's weight to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss_v = common.calc_loss(batch, net, tgt_net.target_model, gamma = GAMMA ** REWARD_STEPS, device = device)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss_v.backward()\n",
    "        \n",
    "        # Do optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track the epsilon\n",
    "        eps_tracker.frame(engine.state.iteration)\n",
    "\n",
    "        # \n",
    "        if getattr(engine.state, \"eval_states\", None) is None:\n",
    "            \n",
    "            # Sample from buffer\n",
    "            eval_states = buffer.sample(STATES_TO_EVALUATE)\n",
    "            \n",
    "            # Convert each state into a numpy array\n",
    "            eval_states = [np.array(transition.state, copy = False) for transition in eval_states]\n",
    "            \n",
    "            # Convert the whole states into a numpy array\n",
    "            engine.state.eval_states = np.array(eval_states, copy = False)\n",
    "\n",
    "        return {\"loss\": loss_v.item(), \"epsilon\": selector.epsilon,}\n",
    "\n",
    "    # Instantiate the engine\n",
    "    engine = Engine(process_batch)\n",
    "    \n",
    "    # Setup the ignite\n",
    "    tb = common.setup_ignite(engine, exp_source, f\"conv-{run_name}\", extra_metrics = ('values_mean',))\n",
    "\n",
    "    # \n",
    "    @engine.on(ptan.ignite.PeriodEvents.ITERS_1000_COMPLETED)\n",
    "    \n",
    "    # Function for synching\n",
    "    def sync_eval(engine: Engine):\n",
    "        \n",
    "        # Synch the source network's weight with target network\n",
    "        tgt_net.sync()\n",
    "\n",
    "        # Get the mean of values\n",
    "        mean_val = common.calc_values_of_states(engine.state.eval_states, net, device = device)\n",
    "        \n",
    "        # Update the metrics\n",
    "        engine.state.metrics[\"values_mean\"] = mean_val\n",
    "        \n",
    "        # \n",
    "        if getattr(engine.state, \"best_mean_val\", None) is None:\n",
    "            \n",
    "            # Assign mean_val to best_mean_val\n",
    "            engine.state.best_mean_val = mean_val\n",
    "            \n",
    "        # If mean_val is larger than best_mean_val\n",
    "        if engine.state.best_mean_val < mean_val:\n",
    "            \n",
    "            # Report\n",
    "            print(\"%d: Best mean value updated %.3f -> %.3f\" % (engine.state.iteration, \n",
    "                                                                engine.state.best_mean_val, \n",
    "                                                                mean_val))\n",
    "            \n",
    "            # Get the path\n",
    "            path = saves_path / (\"mean_value-%.3f.data\" % mean_val)\n",
    "            \n",
    "            # Save the weights\n",
    "            torch.save(net.state_dict(), path)\n",
    "            \n",
    "            # Update the best_mean_val in engine\n",
    "            engine.state.best_mean_val = mean_val\n",
    "\n",
    "    # \n",
    "    @engine.on(ptan.ignite.PeriodEvents.ITERS_10000_COMPLETED)\n",
    "    \n",
    "    # Function for testing\n",
    "    def validate(engine: Engine):\n",
    "        \n",
    "        # Test the model on testset\n",
    "        res = validation.validation_run(env_tst, net, device=device)\n",
    "        \n",
    "        # Report\n",
    "        print(\"%d: tst: %s\" % (engine.state.iteration, res))\n",
    "        \n",
    "        # Loop over keys and values\n",
    "        for key, val in res.items():\n",
    "            \n",
    "            # Update the metrics\n",
    "            engine.state.metrics[key + \"_tst\"] = val\n",
    "            \n",
    "        # Test the model on validation set\n",
    "        res = validation.validation_run(env_val, net, device=device)\n",
    "        \n",
    "        # Report\n",
    "        print(\"%d: val: %s\" % (engine.state.iteration, res))\n",
    "        \n",
    "        # Loop over keys and values\n",
    "        for key, val in res.items():\n",
    "            \n",
    "            # Update the metrics\n",
    "            engine.state.metrics[key + \"_val\"] = val\n",
    "            \n",
    "        # Get validation reward\n",
    "        val_reward = res['episode_reward']\n",
    "        \n",
    "        # \n",
    "        if getattr(engine.state, \"best_val_reward\", None) is None:\n",
    "            \n",
    "            # Assign val_reward to best_val_reward\n",
    "            engine.state.best_val_reward = val_reward\n",
    "            \n",
    "        # If val_reward is greater than best_val_reward\n",
    "        if engine.state.best_val_reward < val_reward:\n",
    "            \n",
    "            # Report\n",
    "            print(\"Best validation reward updated: %.3f -> %.3f, model saved\" % (engine.state.best_val_reward, \n",
    "                                                                                 val_reward))\n",
    "            \n",
    "            # Assign val_reward to best_val_reward\n",
    "            engine.state.best_val_reward = val_reward\n",
    "            \n",
    "            # Saving path\n",
    "            path = saves_path / (\"val_reward-%.3f.data\" % val_reward)\n",
    "            \n",
    "            # Save the weights\n",
    "            torch.save(net.state_dict(), path)\n",
    "\n",
    "    # Instantiate the period event (if 1000 iteration got completed)\n",
    "    event = ptan.ignite.PeriodEvents.ITERS_10000_COMPLETED\n",
    "    \n",
    "    # Get all the test metrics \n",
    "    tst_metrics = [m + \"_tst\" for m in validation.METRICS]\n",
    "    \n",
    "    # Test handler\n",
    "    tst_handler = tb_logger.OutputHandler(tag = \"test\", metric_names = tst_metrics)\n",
    "    \n",
    "    # \n",
    "    tb.attach(engine, log_handler=tst_handler, event_name=event)\n",
    "\n",
    "    # Get all the validation metrics\n",
    "    val_metrics = [m + \"_val\" for m in validation.METRICS]\n",
    "    \n",
    "    # Validation handler\n",
    "    val_handler = tb_logger.OutputHandler(tag = \"validation\", metric_names = val_metrics)\n",
    "    \n",
    "    # \n",
    "    tb.attach(engine, log_handler=val_handler, event_name=event)\n",
    "\n",
    "    # Run the engine\n",
    "    engine.run(common.batch_generator(buffer, REPLAY_INITIAL, BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 07. Testing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from lib import environ, data_loader, models\n",
    "\n",
    "mpl.use(\"Agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "EPSILON = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data path\n",
    "STOCKS = \"data/YNDX_160101_161231.csv\"\n",
    "\n",
    "# Validation data path\n",
    "VAL_STOCKS = \"data/YNDX_150101_151231.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/YNDX_160101_161231.csv\n",
      "Read done, got 131542 rows, 99752 filtered, 0 open prices adjusted\n",
      "100: reward=-0.255\n",
      "200: reward=-0.536\n",
      "300: reward=-1.143\n",
      "400: reward=-1.729\n",
      "500: reward=-2.138\n",
      "600: reward=-2.138\n",
      "700: reward=-2.138\n",
      "800: reward=-2.529\n",
      "900: reward=-2.529\n",
      "1000: reward=-2.529\n",
      "1100: reward=-2.529\n",
      "1200: reward=-2.811\n",
      "1300: reward=-2.811\n",
      "1400: reward=-3.367\n",
      "1500: reward=-3.362\n",
      "1600: reward=-3.362\n",
      "1700: reward=-3.362\n",
      "1800: reward=-3.362\n",
      "1900: reward=-3.362\n",
      "2000: reward=-3.383\n",
      "2100: reward=-3.383\n",
      "2200: reward=-3.383\n",
      "2300: reward=-3.612\n",
      "2400: reward=-4.106\n",
      "2500: reward=-4.106\n",
      "2600: reward=-4.106\n",
      "2700: reward=-4.106\n",
      "2800: reward=-4.106\n",
      "2900: reward=-4.106\n",
      "3000: reward=-4.106\n",
      "3100: reward=-4.106\n",
      "3200: reward=-4.008\n",
      "3300: reward=-4.874\n",
      "3400: reward=-5.322\n",
      "3500: reward=-5.807\n",
      "3600: reward=-5.818\n",
      "3700: reward=-6.167\n",
      "3800: reward=-6.167\n",
      "3900: reward=-6.123\n",
      "4000: reward=-6.470\n",
      "4100: reward=-6.470\n",
      "4200: reward=-6.470\n",
      "4300: reward=-6.470\n",
      "4400: reward=-6.670\n",
      "4500: reward=-6.670\n",
      "4600: reward=-6.670\n",
      "4700: reward=-6.670\n",
      "4800: reward=-6.670\n",
      "4900: reward=-6.670\n",
      "5000: reward=-7.170\n",
      "5100: reward=-7.667\n",
      "5200: reward=-7.667\n",
      "5300: reward=-7.820\n",
      "5400: reward=-7.820\n",
      "5500: reward=-7.820\n",
      "5600: reward=-7.820\n",
      "5700: reward=-7.921\n",
      "5800: reward=-7.921\n",
      "5900: reward=-7.921\n",
      "6000: reward=-7.972\n",
      "6100: reward=-7.972\n",
      "6200: reward=-7.972\n",
      "6300: reward=-7.972\n",
      "6400: reward=-7.972\n",
      "6500: reward=-8.073\n",
      "6600: reward=-8.073\n",
      "6700: reward=-8.324\n",
      "6800: reward=-8.324\n",
      "6900: reward=-8.574\n",
      "7000: reward=-8.574\n",
      "7100: reward=-8.574\n",
      "7200: reward=-8.822\n",
      "7300: reward=-8.822\n",
      "7400: reward=-8.822\n",
      "7500: reward=-8.822\n",
      "7600: reward=-9.672\n",
      "7700: reward=-9.669\n",
      "7800: reward=-9.919\n",
      "7900: reward=-10.917\n",
      "8000: reward=-10.917\n",
      "8100: reward=-11.312\n",
      "8200: reward=-11.662\n",
      "8300: reward=-11.862\n",
      "8400: reward=-11.862\n",
      "8500: reward=-11.862\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # CSV file with quotes to run the model\n",
    "    data = STOCKS\n",
    "    \n",
    "    # Model file to load\n",
    "    model = None\n",
    "    \n",
    "    # Count of bars to feed into the model\n",
    "    bars = 50\n",
    "    \n",
    "    # Name to use in output images\n",
    "    name = \"output_image\"\n",
    "    \n",
    "    # Commission size in percent\n",
    "    commission = 0.1\n",
    "    \n",
    "    # Use convolution model instead of FF\n",
    "    conv = True\n",
    "\n",
    "    # Load the dataset\n",
    "    prices = data_loader.load_relative(data)\n",
    "    \n",
    "    # Instantiate the environment\n",
    "    env = environ.StocksEnv({\"TEST\": prices}, \n",
    "                            bars_count = bars, \n",
    "                            reset_on_close = False, \n",
    "                            commission = commission,\n",
    "                            state_1d = conv, \n",
    "                            random_ofs_on_reset = False, \n",
    "                            reward_on_close = False, \n",
    "                            volumes = False)\n",
    "    \n",
    "    # If \"conv\" is true\n",
    "    if conv:\n",
    "        \n",
    "        # Instantiate the CNN network\n",
    "        net = models.DQNConv1D(env.observation_space.shape, env.action_space.n)\n",
    "        \n",
    "    # If \"conv\" is false\n",
    "    else:\n",
    "        \n",
    "        # Instantiate the ANN network\n",
    "        net = models.SimpleFFDQN(env.observation_space.shape[0], env.action_space.n)\n",
    "\n",
    "    # Load the weights\n",
    "    net.load_state_dict(torch.load(model, map_location = lambda storage, loc: storage))\n",
    "\n",
    "    # Reset the environment and get the observations\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Get the start prices\n",
    "    start_price = env._state._cur_close()\n",
    "\n",
    "    # Initialize the total reward with zero\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    # Initialize the step index with zero\n",
    "    step_idx = 0\n",
    "    \n",
    "    # Initialize the rewards with a empty list\n",
    "    rewards = []\n",
    "\n",
    "    # Infinite loop\n",
    "    while True:\n",
    "        \n",
    "        # Increment the step index\n",
    "        step_idx += 1\n",
    "        \n",
    "        # Convert observations to torch tensor\n",
    "        obs_v = torch.tensor([obs])\n",
    "        \n",
    "        # Feedforward\n",
    "        out_v = net(obs_v)\n",
    "        \n",
    "        # Get the action index\n",
    "        action_idx = out_v.max(dim = 1)[1].item()\n",
    "        \n",
    "        # If epsilon is higher than the random number\n",
    "        if np.random.random() < EPSILON:\n",
    "            \n",
    "            # Set action index randomely\n",
    "            action_idx = env.action_space.sample()\n",
    "            \n",
    "        # Get the actual action to take\n",
    "        action = environ.Actions(action_idx)\n",
    "\n",
    "        # Take action and get the S, R, and done mask\n",
    "        obs, reward, done, _ = env.step(action_idx)\n",
    "        \n",
    "        # Add reward to total reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Append reward to list\n",
    "        rewards.append(total_reward)\n",
    "        \n",
    "        # Every 100 times\n",
    "        if step_idx % 100 == 0:\n",
    "            \n",
    "            # Reprt\n",
    "            print(\"%d: reward=%.3f\" % (step_idx, total_reward))\n",
    "            \n",
    "        # If terminal state\n",
    "        if done:\n",
    "            \n",
    "            # Break the loop\n",
    "            break\n",
    "\n",
    "    # Visualization\n",
    "    plt.clf()\n",
    "    plt.plot(rewards)\n",
    "    plt.title(\"Total reward, data=%s\" % name)\n",
    "    plt.ylabel(\"Reward, %\")\n",
    "    plt.savefig(\"rewards-%s.png\" % name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 08. Results\n",
    "\n",
    "---\n",
    "\n",
    "Let's now take a look at the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 08.1. The feed-forward model\n",
    "\n",
    "---\n",
    "\n",
    "The convergence on Yandex data for one year requires about 10M training steps, which can take a while. (GTX 1080 Ti trains at a speed of 230-250 steps per second.) During the training, we have several charts in TensorBoard showing us what's going on.\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig10.3.png\">\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig10.4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two preceding charts show the reward for episodes played during the training and the reward obtained from testing (which is done on the same quotes, but with epsilon=0). From them, we see that our agent is learning how to increase the profit from its actions over time.\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig10.5.6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lengths of episodes also increased after 1M training iterations. The number of values predicted by the network is growing.\n",
    "\n",
    "<img width=\"700\" src=\"assets/fig10.7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 10.7 shows quite important information: the amount of reward obtained during the training on the validation set (which is quotes from 2015 by default). This reward doesn't have as obvious a trend as the reward on the training data. This might be an indication of overfitting of the agent, which starts after 3M training iterations. But still, the reward is above line –0.2% (which is a broker commission in our environment) and means that our agent is better than a random \"buying and selling monkey.\"\n",
    "\n",
    "During the training, our code saves models for later experiments. It does this every time the mean Q-values on our held-out states set update the maximum or when the reward on the validation sets beats the previous record. There is a tool that loads the model, trades on prices you've provided to it with the command-line option, and draws the plots with the profit change over time. The tool is called Chapter10/ run_model.py and it can be used as shown here:\n",
    "\n",
    "```\n",
    "$ ./run_model.py -d data/YNDX_160101_161231.csv -m saves/ff- YNDX16/mean_\n",
    "val-0.332.data -b 10 -n test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The options that the tool accepts are as follows:\n",
    "- `-d`: This is the path to the quotes to use. In the preceding example, we apply the model to the data that it was trained on.\n",
    "- `-m`: This is the path to the model file. By default, the training code saves it in the saves dir.\n",
    "- `-b`: This shows how many bars to pass to the model in the context. It has to match the count of bars used on training, which is 10 by default and can be changed in the training code.\n",
    "- `-n`: This is the suffix to be prepended to the images produced.\n",
    "- `--commission`: This allows you to redefine the broker's commission, which has a default of 0.1%.\n",
    "\n",
    "At the end, the tool creates a chart of the total profit dynamics (in percentages). The following is the reward chart on Yandex 2016 quotes (used for training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"600\" src=\"assets/fig10.8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result looks amazing: more than 200% profit in a year. However, let's look at what will happen with the 2015 data:\n",
    "\n",
    "<img width=\"600\" src=\"assets/fig10.9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is much worse, as we've seen from the validation plots in TensorBoard. To check that our system is profitable with zero commission, we must rerun on the same data with the --commission 0.0 option.\n",
    "\n",
    "<img width=\"600\" src=\"assets/fig10.10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some bad days with drawdown, but the overall results are good: without commission, our agent can be profitable. Of course, the commission is not the only issue. Our order simulation is very primitive and doesn't take into account real-life situations, such as price spread and a slip in order execution.\n",
    "\n",
    "If we take the model with the best reward on the validation set, the reward dynamics are a bit better. Profitability is lower, but the drawdown on unseen quotes is much less.\n",
    "\n",
    "<img width=\"600\" src=\"assets/fig10.11.12.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 08.2. The convolution model\n",
    "\n",
    "---\n",
    "\n",
    "The second model implemented in this example uses 1D convolution filters to extract features from the price data. This allows us to increase the number of bars in the context window that our agent sees on every step without a significant increase in the network size. By default, the convolution model example uses 50 bars of context. The training code is in Chapter10/train_model_conv.py, and it accepts the same set of command-line parameters as the feed-forward version.\n",
    "\n",
    "Training dynamics are almost identical, but the reward obtained on the validation set is slightly higher and starts to overfit later.\n",
    "\n",
    "<img width=\"600\" src=\"assets/fig10.13.png\">\n",
    "\n",
    "<img width=\"600\" src=\"assets/fig10.14.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 08. Things to try\n",
    "\n",
    "---\n",
    "\n",
    "As already mentioned, financial markets are large and complicated. The methods that we've tried are just the very beginning. Using RL to create a complete and profitable trading strategy is a large project, which can take several months of dedicated labor. However, there are things that we can try to get a better understanding of the topic:\n",
    "- Our data representation is definitely not perfect. We don't take into account significant price levels (support and resistance), round price values, and others. Incorporating them into the observation could be a challenging problem.\n",
    "- Market prices are usually analyzed at different timeframes. Low-level data like one-minute bars are noisy (as they include lots of small price movements caused by individual trades), and it is like looking at the market using a microscope. At larger scales, such as one-hour or one-day bars, you can see large, long trends in data movement, which could be extremely important for price prediction.\n",
    "- More training data is needed. One year of data for one stock is just 130k bars, which might be not enough to capture all market situations. Ideally, a real- life agent should be trained on a much larger dataset, such as the prices for hundreds of stocks for the past 10 years.\n",
    "- Experiment with the network architecture. The convolution model has shown much faster convergence than the feed-forward model, but there are a lot of things to optimize: the count of layers, kernel size, residual architecture, attention mechanism, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 09. Summary\n",
    "\n",
    "---\n",
    "In this chapter, we saw a practical example of RL and implemented the trading agent and custom Gym environment. We tried two different architectures: a feed-forward network with price history on input and a 1D convolution network. Both architectures used the DQN method, with some extensions described in Chapter 8, DQN Extensions.\n",
    "\n",
    "This is the last chapter in part two of this book. In part three, we will talk about a different family of RL methods: policy gradients. We've touched on this approach a bit, but in the upcoming chapters, we will go much deeper into the subject, covering the REINFORCE method and the best method in the family: A3C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reinforcement Learning",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
