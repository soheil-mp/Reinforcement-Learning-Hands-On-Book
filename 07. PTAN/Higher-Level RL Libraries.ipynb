{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center; font-size:28px;\">Higher-Level RL Libraries</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "In Chapter 6, Deep Q-Networks, we implemented the deep Q-network (DQN) model published by DeepMind in 2015 (https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning). This paper had a significant effect on the RL field by demonstrating that it's possible to use nonlinear approximators in RL.\n",
    "\n",
    "In this chapter, we will take another step and discuss the higher-level RL libraries, which will allow you to build your code from higher-level blocks. Most of the chapter will describe the __PyTorch Agent Net (PTAN)__ library. This library will be used in the rest of the book to avoid code repetition.\n",
    "\n",
    "We will cover:\n",
    "\n",
    "- The motivation for using high-level libraries, rather than reimplementing everything from scratch.\n",
    "- The PTAN library, with code examples.\n",
    "- Implementing DQN (on CartPol) using the PTAN library.\n",
    "- Other RL libraries that you might consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 1. Why RL libraries?\n",
    "---\n",
    "\n",
    "The implementation of DQN (in Chapter 6) wasn't very long and complicated. It was about 200 lines of training code + 120 lines in environment wrappers. \n",
    "\n",
    "When you are becoming familiar with RL methods, it is very useful to implement everything from scratch. However, the more involved you become in the field, the more often you will realize that you are writing the same code over and over again. Writing the same code over and over again is not very efficient, as bugs might be introduced every time, which will cost you time for debugging and understanding. In addition, carefully designed code that has been used in several projects usually has a higher quality in terms of performance, unit tests, readability, and documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 2. The PTAN library\n",
    "\n",
    "<hr>\n",
    "\n",
    "The PTAN library is available at https://github.com/Shmuma/ptan. All the subsequent examples were implemented using version 0.6 of PTAN, which can be installed by running the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptan==0.6 in /Users/soheilmohammadpour/opt/anaconda3/lib/python3.7/site-packages (0.6)\n",
      "Requirement already satisfied: numpy in /Users/soheilmohammadpour/opt/anaconda3/lib/python3.7/site-packages (from ptan==0.6) (1.18.1)\n",
      "Requirement already satisfied: atari-py in /Users/soheilmohammadpour/opt/anaconda3/lib/python3.7/site-packages (from ptan==0.6) (0.2.6)\n",
      "Requirement already satisfied: gym in /Users/soheilmohammadpour/opt/anaconda3/lib/python3.7/site-packages (from ptan==0.6) (0.17.1)\n",
      "Requirement already satisfied: torch==1.3.0 in /Users/soheilmohammadpour/opt/anaconda3/lib/python3.7/site-packages (from ptan==0.6) (1.3.0)\n",
      "Requirement already satisfied: opencv-python in /Users/soheilmohammadpour/opt/anaconda3/lib/python3.7/site-packages (from ptan==0.6) (4.2.0.32)\n",
      "Requirement already satisfied: six in /Users/soheilmohammadpour/opt/anaconda3/lib/python3.7/site-packages (from atari-py->ptan==0.6) (1.10.0)\n",
      "Requirement already satisfied: scipy in /Users/soheilmohammadpour/opt/anaconda3/lib/python3.7/site-packages (from gym->ptan==0.6) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /Users/soheilmohammadpour/opt/anaconda3/lib/python3.7/site-packages (from gym->ptan==0.6) (1.3.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /Users/soheilmohammadpour/opt/anaconda3/lib/python3.7/site-packages (from gym->ptan==0.6) (1.5.0)\n",
      "Requirement already satisfied: future in /Users/soheilmohammadpour/opt/anaconda3/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym->ptan==0.6) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ptan==0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original goal of PTAN was to simplify the RL experiments, and it tries to keep the balance between two extremes:\n",
    "- Import the library and then write one line of code for training (an example is the OpenAI Baselines project). This approch is very inflexible since we can't change little things in it.\n",
    "- Implement everything from scratch which is error-prone, boring, and inefficient.\n",
    "\n",
    "<br>\n",
    "\n",
    "At a high level, PTAN provides the following entities:\n",
    "- __Agent:__ This class can convert a batch of observations to a batch of actions. Also, it can contain an optional state (if you want to track some information between consequent actions in one episode). The library provides several agents for the most common RL cases, but you always can write your own subclass of BaseAgent as well.\n",
    "\n",
    "\n",
    "- __ActionSelector:__ For choosing the action from some output of the network. It works in tandem with Agent.\n",
    "\n",
    "\n",
    "- __ExperienceSource (and variations):__ Can provide information about the trajectory from episodes. In its simplest form, it is one single (a, r, s') transition at a time, but its functionality goes beyond this.\n",
    "\n",
    "\n",
    "- __ExperienceSourceBuffer (and friends):__ Replay buffers with various characteristics. They include a simple replay buffer and two versions of prioritized replay buffers.\n",
    "\n",
    "\n",
    "- __Various utility classes:__ Like TargetNet, and wrappers for time series preprocessing (used for tracking training progress in TensorBoard).\n",
    "\n",
    "\n",
    "- __Wrappers for Gym environments:__ Like wrappers for Atari games.\n",
    "\n",
    "\n",
    "- PyTorch Ignite helpers to integrate PTAN into the Ignite framework.\n",
    "\n",
    "That's basically it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 3. Action selectors\n",
    "\n",
    "---\n",
    "\n",
    "An action selector is an object that helps with going from network output to concrete action values. The most common cases include:\n",
    "\n",
    "- __Argmax:__ Commonly used by Q-value methods when the network predicts Q-values for a set of actions and the desired action is the action with the largest Q(s, a).\n",
    "\n",
    "\n",
    "- __Policy-based:__ The network outputs the probability distribution (in the form of logits or normalized distribution), and an action needs to be sampled from this distribution. The concrete classes provided by the library are:\n",
    "\n",
    "\n",
    "- __ArgmaxActionSelector:__ Applies the argmax on the second axis of a passed tensor. (It assumes a matrix with batch dimension along the first axis.)\n",
    "\n",
    "\n",
    "- __ProbabilityActionSelector:__ Samples from the probability distribution of a discrete set of actions.\n",
    "\n",
    "\n",
    "- __EpsilonGreedyActionSelector:__ Has the parameter epsilon, which specifies the probability of a random action to be taken.\n",
    "\n",
    "All the classes assume that NumPy arrays will be passed to them. The complete example from this section can be found in Chapter07/01_actions.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import ptan\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_vals\n",
      "[[ 1  2  3]\n",
      " [ 1 -1  0]]\n"
     ]
    }
   ],
   "source": [
    "# Create Q-values\n",
    "q_vals = np.array([[1, 2, 3], [1, -1, 0]])\n",
    "print(\"q_vals\")\n",
    "print(q_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "argmax: [2 0]\n"
     ]
    }
   ],
   "source": [
    "# Argmax action selector  (CHOOSE MAXIMUM)\n",
    "selector = ptan.actions.ArgmaxActionSelector()        # Returns indices of actions with the largest values\n",
    "print(\"argmax:\", selector(q_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Selector with Epsilon of 0.0: \t [2 0]\n"
     ]
    }
   ],
   "source": [
    "# Epsilon greedy action selector with epsilon of 0 (TOTALLY GREEDY)\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon = 0.0)\n",
    "print(\"Action Selector with Epsilon of 0.0: \\t\", selector(q_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Selector with Epsilon of 1.0: \t [2 1]\n"
     ]
    }
   ],
   "source": [
    "# Epsilon greedy action selector with epsilon of 1 (TOTALLY RANDOM)\n",
    "selector.epsilon = 1.0\n",
    "print(\"Action Selector with Epsilon of 1.0: \\t\", selector(q_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Selector with Epsilon of 0.5: \t [2 0]\n"
     ]
    }
   ],
   "source": [
    "# Epsilon greedy action selector with epsilon of 0.5 (HALF RANDOM)\n",
    "selector.epsilon = 0.5\n",
    "print(\"Action Selector with Epsilon of 0.5: \\t\", selector(q_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Selector with Epsilon of 0.1: \t [2 0]\n"
     ]
    }
   ],
   "source": [
    "# Epsilon greedy action selector with epsilon of 0.1 (10% RANDOM)\n",
    "selector.epsilon = 0.1\n",
    "print(\"Action Selector with Epsilon of 0.1: \\t\", selector(q_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with ProbabilityActionSelector is the same, but the input needs to be a normalized probability distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions sampled from three prob distributions:\n",
      "[1 2 1]\n",
      "[1 2 0]\n",
      "[1 2 0]\n",
      "[2 2 0]\n",
      "[1 2 0]\n",
      "[1 2 1]\n",
      "[2 2 0]\n",
      "[1 2 0]\n",
      "[1 2 1]\n",
      "[1 2 1]\n"
     ]
    }
   ],
   "source": [
    "# Probability action selector (BASED ON GIVEN PROBABILITIES)\n",
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "\n",
    "# Report\n",
    "print(\"Actions sampled from three prob distributions:\")\n",
    "for _ in range(10):\n",
    "    acts = selector(np.array([[0.1, 0.8, 0.1],\n",
    "                              [0.0, 0.0, 1.0],\n",
    "                              [0.5, 0.5, 0.0]]))\n",
    "    print(acts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding example, we sample from three distributions: in the first, the action with index 1 is chosen with probability 80%; in the second distribution, we always select action number 2; and in the third, actions 0 and 1 are equally likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 4. The agent\n",
    "\n",
    "---\n",
    "\n",
    "The agent entity bridges observations and action. There are 3 variants that the agent must have:\n",
    "\n",
    "1. So far, we've only used a simple DQN agent (that uses neural networks) to obtain action's value and behaving greedily on those values. We've used epsilon-greedy behavior to explore the environment. This could be more complicated. For example, instead of predicting the values of the actions, our agent could predict a probability distribution over the actions. Such agents are called policy agents.\n",
    "\n",
    "\n",
    "2. It's necessary for the agent to keep a state between observations. For example, very often one observation (or even the last k observation) is not enough to make a decision about the action, and we want to keep some memory in the agent to capture the necessary information. There is a whole subdomain of RL that tries to address this complication with partially observable Markov decision process (POMDP) formalism, which is not covered in the book.\n",
    "\n",
    "\n",
    "3. The third variant of the agent is very common in continuous control problems. In such cases, actions are not discrete anymore but some continuous value, and the agent needs to predict them from the observations.\n",
    "\n",
    "To capture all those variants and make the code flexible, the agent in PTAN is implemented as an extensible hierarchy of classes with the ptan.agent.BaseAgent abstract class at the top. From a high level, the agent needs to accept the batch of observations (in the form of a NumPy array) and return the batch of actions. The batch is used to make the processing more efficient, as processing several observations in one pass in a graphics processing unit (GPU) is frequently much faster than processing them individually. The abstract base class doesn't define the types of input and output, which makes it very flexible and easy to extend. For example, in the continuous domain, our actions will no longer be indices of discrete actions, but float values.\n",
    "\n",
    "PTAN provides two of the most common ways to convert observations into actions: DQNAgent and PolicyAgent. Let's check them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import ptan\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### DQN using PTAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Network\n",
    "class DQNNet(nn.Module):\n",
    "    \n",
    "    # Constructor function\n",
    "    def __init__(self, actions: int):\n",
    "        \n",
    "        # Inherite parent's constructor\n",
    "        super(DQNNet, self).__init__()\n",
    "        \n",
    "        # Initialize the action\n",
    "        self.actions = actions\n",
    " \n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Produce a diagonal tensor of shape (batch_size, actions)\n",
    "        output = torch.eye(x.size()[0], self.actions)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dqn_net: \n",
      " tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DQN network\n",
    "net = DQNNet(actions = 3)\n",
    "\n",
    "# Feedforward\n",
    "net_out = net(torch.zeros(2, 10))\n",
    "\n",
    "print(\"dqn_net: \\n\", net_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argmax: (array([0, 1]), [None, None])\n"
     ]
    }
   ],
   "source": [
    "### DQN Agent (using Argmax)\n",
    "\n",
    "# Argmax action selector\n",
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "\n",
    "# DQN Agent\n",
    "agent = ptan.agent.DQNAgent(dqn_model = net, action_selector = selector)\n",
    "\n",
    "# Output of the agent\n",
    "ag_out = agent(torch.zeros(2, 5))\n",
    "\n",
    "print(\"Argmax:\", ag_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps=1.0: [0 1 0 2 1 0 1 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "### DQN Agent (using Epsilon-Greedy)\n",
    "\n",
    "# Epsilon-greedy action selector\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon = 1.0)\n",
    "\n",
    "# DQN agent\n",
    "agent = ptan.agent.DQNAgent(dqn_model = net, action_selector = selector)\n",
    "\n",
    "# Output of the agent\n",
    "ag_out = agent(torch.zeros(10, 5))[0]\n",
    "\n",
    "# Report\n",
    "print(\"eps=1.0:\", ag_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps=0.5: [0 1 2 2 0 0 0 1 2 2]\n"
     ]
    }
   ],
   "source": [
    "### DQN Agent (using Epsilon-Greedy)\n",
    "\n",
    "# Set epsilon to 0.5\n",
    "selector.epsilon = 0.5\n",
    "\n",
    "# Output of the agent\n",
    "ag_out = agent(torch.zeros(10, 5))[0]\n",
    "\n",
    "# Report\n",
    "print(\"eps=0.5:\", ag_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps=0.1: [0 1 2 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "### DQN Agent (using Epsilon-Greedy)\n",
    "\n",
    "# Set epsilon to 0.5\n",
    "selector.epsilon = 0.1\n",
    "\n",
    "# Output of the agent\n",
    "ag_out = agent(torch.zeros(10, 5))[0]\n",
    "\n",
    "# Report\n",
    "print(\"eps=0.1:\", ag_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "##### PolicyNet using PTAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy network\n",
    "class PolicyNet(nn.Module):\n",
    "    \n",
    "    # Constructor function\n",
    "    def __init__(self, actions: int):\n",
    "        \n",
    "        # Inherite parent's constructor\n",
    "        super(PolicyNet, self).__init__()\n",
    "        \n",
    "        # Initialize the actions\n",
    "        self.actions = actions\n",
    "\n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Get the shape of output\n",
    "        shape = (x.size()[0], self.actions)\n",
    "        \n",
    "        # Initialize the output with zeros\n",
    "        output = torch.zeros(shape, dtype = torch.float32)\n",
    "        \n",
    "        # Update the output with first two actions having the same logit scores\n",
    "        output[:, 0] = 1\n",
    "        output[:, 1] = 1\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_net: \n",
      " tensor([[1., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the policy network\n",
    "net = PolicyNet(actions = 5)\n",
    "\n",
    "# Feedforward\n",
    "net_out = net(torch.zeros(6, 10))      # The first two column of this zero matrix will turn into one\n",
    "\n",
    "print(\"policy_net: \\n\", net_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Probability action selector\n",
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "\n",
    "# Policy agent\n",
    "agent = ptan.agent.PolicyAgent(model = net, action_selector = selector, apply_softmax = True)\n",
    "\n",
    "# Output of the agent\n",
    "ag_out = agent(torch.zeros(6, 5))[0]\n",
    "\n",
    "print(ag_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 5. DQNAgent\n",
    "\n",
    "---\n",
    "\n",
    "This class is applicable in Q-learning when the action space is not very large, which covers Atari games and lots of classical problems. This representation is not universal, and later in the book, you will see ways of dealing with that. DQNAgent takes a batch of observations on input (as a NumPy array), applies the network on them to get Q-values, and then uses the provided ActionSelector to convert Q-values to indices of actions.\n",
    "\n",
    "Let's consider a small example. For simplicity, our network always produces the same output for the input batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Network\n",
    "class DQNNet(nn.Module):\n",
    "    \n",
    "    # Constructor function\n",
    "    def __init__(self, actions: int):\n",
    "        \n",
    "        # Inherite parent's constructor\n",
    "        super(DQNNet, self).__init__()\n",
    "        \n",
    "        # Initialize the action\n",
    "        self.actions = actions\n",
    " \n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Produce a diagonal tensor of shape (batch_size, actions)\n",
    "        output = torch.eye(x.size()[0], self.actions)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined the above class, we can use it as a DQN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the DQN network\n",
    "net = DQNNet(actions=3)\n",
    "\n",
    "# Feedforward\n",
    "net(torch.zeros(2, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the simple argmax policy, so the agent will always return actions corresponding to 1s in the network output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), [None, None])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action selector (Argmax)\n",
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "\n",
    "# DQN Agent\n",
    "agent = ptan.agent.DQNAgent(dqn_model = net, action_selector = selector)\n",
    "\n",
    "# Pass a zero tensor to agent\n",
    "agent(torch.zeros(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the input, a batch of two observations, each having five values, was given, and on the output, the agent returned a tuple of two objects:\n",
    "\n",
    "- An array with actions to be executed for every batch. In our case, this is action 0 for the first batch sample and action 1 for the second.\n",
    "- A list with the agent's internal state. This is used for stateful agents and is a list of None in our case. As our agent is stateless so you can ignore it.\n",
    "\n",
    "Now let's make the agent with an epsilon-greedy exploration strategy. For this, we just need to pass a different action selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 2, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action selector (Epsilon-Greedy)\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon = 1.0)\n",
    "\n",
    "# DQN Agent\n",
    "agent = ptan.agent.DQNAgent(dqn_model = net, action_selector = selector)\n",
    "\n",
    "# Pass a tensor to agent\n",
    "agent(torch.zeros(10, 5))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As epsilon is 1.0, all the actions will be random, regardless of the network's output. But we can change the epsilon value on the fly, which is very handy during the training when we are supposed to anneal epsilon over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 2, 1, 2, 0, 2, 0, 0])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change epsilon\n",
    "selector.epsilon = 0.5\n",
    "\n",
    "# Pass a tensor to DQN agent\n",
    "agent(torch.zeros(10, 5))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 0, 1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change epsilon\n",
    "selector.epsilon = 0.1\n",
    "\n",
    "# Pass a tensor to DQN agent\n",
    "agent(torch.zeros(10, 5))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 6. PolicyAgent\n",
    "\n",
    "---\n",
    "\n",
    "PolicyAgent expects the network to produce policy distribution over a discrete set of actions. Policy distribution could be either logits (unnormalized) or a normalized distribution. In practice, you should always use logits to improve the numeric stability of the training process.\n",
    "\n",
    "Let's reimplement our previous example, but now the network will produce probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy network\n",
    "class PolicyNet(nn.Module):\n",
    "    \n",
    "    # Constructor function\n",
    "    def __init__(self, actions: int):\n",
    "        \n",
    "        # Inherite parent's constructor\n",
    "        super(PolicyNet, self).__init__()\n",
    "        \n",
    "        # Initialize the actions\n",
    "        self.actions = actions\n",
    "\n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Get the shape of output\n",
    "        shape = (x.size()[0], self.actions)\n",
    "        \n",
    "        # Initialize the output with zeros\n",
    "        output = torch.zeros(shape, dtype = torch.float32)\n",
    "        \n",
    "        # Update the output with first two actions having the same logit scores\n",
    "        output[:, 0] = 1\n",
    "        output[:, 1] = 1\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class above could be used to get the action logits for a batch of observations (which is ignored in our example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the PolicyNet\n",
    "net = PolicyNet(actions=5)\n",
    "\n",
    "# Pass a tensor into the PolicyNet\n",
    "net(torch.zeros(6, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use PolicyAgent in combination with ProbabilityActionSelector. As the latter expects normalized probabilities, we need to ask PolicyAgent to apply softmax to the network's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 1, 1, 4, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action selector\n",
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "\n",
    "# Policy Agent\n",
    "agent = ptan.agent.PolicyAgent(model = net, action_selector = selector, apply_softmax = True)\n",
    "\n",
    "# Pass a tensor into the agent\n",
    "agent(torch.zeros(6, 5))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the softmax operation produces non-zero probabilities for zero logits, so our agent can still select actions >1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3222, 0.3222, 0.1185, 0.1185, 0.1185]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add softmax at the end (for making probabulity distribution)\n",
    "torch.nn.functional.softmax(input = net(torch.zeros(1, 10)), \n",
    "                            dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 7. Experience source\n",
    "\n",
    "---\n",
    "\n",
    "The agent abstraction described in the previous section allows us to implement environment communications in a generic way. These communications happen in the form of trajectories, produced by applying the agent's actions to the Gym environment.\n",
    "\n",
    "At a high level, experience source classes take the agent instance and environment and provide you with step-by step data from the trajectories. The functionality of those classes includes:\n",
    "- Support of multiple environments being communicated at the same time. This allows efficient GPU utilization as a batch of observations is being processed by the agent at once.\n",
    "- A trajectory can be preprocessed and presented in a convenient form for further training. For example, there is an implementation of subtrajectory rollouts with accumulation of the reward. That preprocessing is convenient for DQN and n-step DQN, when we are not interested in individual intermediate steps in subtrajectories, so they can be dropped. This saves memory and reduces the amount of code we need to write.\n",
    "- Support of vectorized environments from OpenAI Universe. We will cover this in Chapter 17, Continuous Action Space, for web automation and MiniWoB environments. \n",
    "\n",
    "So, the experience source classes act as a \"magic black box\" to hide the environment interaction and trajectory handling complexities from the library user. But the overall PTAN philosophy is to be flexible and extensible, so if you want, you can subclass one of the existing classes or implement your own version as needed.\n",
    "\n",
    "There are three classes provided by the system:\n",
    "- __ExperienceSource:__ using the agent and the set of environments, it produces n-step subtrajectories with all intermediate steps\n",
    "- __ExperienceSourceFirstLast:__ this is the same as ExperienceSource, but instead of a full subtrajectory (with all steps), it keeps only the first and last steps, with proper reward accumulation in between. This can save a lot of memory in the case of n-step DQN or advantage actor-critic (A2C) rollouts\n",
    "- __ExperienceSourceRollouts:__ this follows the asynchronous advantage actor-critic (A3C) rollouts scheme described in Mnih's paper about Atari games (referenced in Chapter 12, The Actor-Critic Method)\n",
    "\n",
    "All the classes are written to be efficient both in terms of central processing unit (CPU) and memory, which is not very important for toy problems, but might become an issue when you want to solve Atari games and need to keep 10M samples in the replay buffer using commodity hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import ptan\n",
    "from typing import List, Optional, Tuple, Any\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "class ToyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment with observation 0-4 and actions 0-2. Observations are rotated sequentialy mod 5, reward is equal \n",
    "    to given action. Episodes are having fixed length of 10\n",
    "    \"\"\"\n",
    "\n",
    "    # Constructor function\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Inherite parent's constructor\n",
    "        super(ToyEnv, self).__init__()\n",
    "        \n",
    "        # Initialize the observation space\n",
    "        self.observation_space = gym.spaces.Discrete(n=5)\n",
    "        \n",
    "        # Initialize the action space\n",
    "        self.action_space = gym.spaces.Discrete(n=3)\n",
    "        \n",
    "        # Initialize the step index\n",
    "        self.step_index = 0\n",
    "\n",
    "        \n",
    "    # Reset funtion\n",
    "    def reset(self):\n",
    "        \n",
    "        # Reset the step index to zero\n",
    "        self.step_index = 0\n",
    "        \n",
    "        return self.step_index\n",
    "\n",
    "    \n",
    "    # Step function\n",
    "    def step(self, action):\n",
    "        \n",
    "        # Get is_done\n",
    "        is_done = self.step_index == 10\n",
    "        \n",
    "        # If step index is 10\n",
    "        if is_done:\n",
    "            \n",
    "            return self.step_index % self.observation_space.n, 0.0, is_done, {}\n",
    "        \n",
    "        # Increment the step index\n",
    "        self.step_index += 1\n",
    "        \n",
    "        return self.step_index % self.observation_space.n, float(action), self.step_index == 10, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.reset() : 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = ToyEnv()\n",
    "\n",
    "# Reset the environment\n",
    "s = env.reset()\n",
    "\n",
    "print(\"env.reset() : %s\" % s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.step(1) : (1, 1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# Take action 1\n",
    "s = env.step(1)\n",
    "print(\"env.step(1) : %s\" % str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.step(2) : (2, 2.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "# Take action 2\n",
    "s = env.step(2)\n",
    "print(\"env.step(2) : %s\" % str(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 0.0, False, {})\n",
      "(4, 0.0, False, {})\n",
      "(0, 0.0, False, {})\n",
      "(1, 0.0, False, {})\n",
      "(2, 0.0, False, {})\n",
      "(3, 0.0, False, {})\n",
      "(4, 0.0, False, {})\n",
      "(0, 0.0, True, {})\n",
      "(0, 0.0, True, {})\n",
      "(0, 0.0, True, {})\n"
     ]
    }
   ],
   "source": [
    "# Loop 10 times\n",
    "for _ in range(10):\n",
    "\n",
    "    # Take action 0\n",
    "    r = env.step(0)\n",
    "    \n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "class DullAgent(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent always returns the fixed action\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, action: int):\n",
    "        \n",
    "        # Initialize the action\n",
    "        self.action = action\n",
    "\n",
    "    # Call function\n",
    "    def __call__(self, observations: List[Any], state: Optional[List] = None) -> Tuple[List[int], Optional[List]]:\n",
    "        return [self.action for _ in observations], state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the agent\n",
    "agent = DullAgent(action = 1)\n",
    "\n",
    "print(\"agent:\", agent([1, 2])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=True))\n",
      "(Experience(state=4, action=1, reward=1.0, done=True),)\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "# Experience source (2 step count)\n",
    "exp_source = ptan.experience.ExperienceSource(env = env, agent = agent, steps_count = 2)\n",
    "\n",
    "# Print untill 15th index\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    if idx > 15:\n",
    "        break\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False),\n",
      " Experience(state=1, action=1, reward=1.0, done=False),\n",
      " Experience(state=2, action=1, reward=1.0, done=False),\n",
      " Experience(state=3, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "# Experience source (4 step count)\n",
    "exp_source = ptan.experience.ExperienceSource(env = env, agent = agent, steps_count = 4)\n",
    "\n",
    "pprint(next(iter(exp_source)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "# Experience source (multiple environment)\n",
    "exp_source = ptan.experience.ExperienceSource(env = [ToyEnv(), ToyEnv()], agent = agent, steps_count = 2)\n",
    "\n",
    "# Print untill 4th index\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    if idx > 4:\n",
    "        break\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n"
     ]
    }
   ],
   "source": [
    "# Experience source (first-last)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1.0, steps_count=1)\n",
    "\n",
    "# Print untill 4th index\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    print(exp)\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 8. Toy environment\n",
    "\n",
    "---\n",
    "\n",
    "For demonstration, we will implement a very simple Gym environment with a small predictable observation state to show how experience source classes work. This environment has integer observation, which increases from 0 to 4, integer action, and a reward equal to the action given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "from typing import List, Tuple, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "class ToyEnv(gym.Env):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Inherite parent's constructor\n",
    "        super(ToyEnv, self).__init__()\n",
    "        \n",
    "        # Observation space\n",
    "        self.observation_space = gym.spaces.Discrete(n=5)\n",
    "        \n",
    "        # Action space\n",
    "        self.action_space = gym.spaces.Discrete(n=3)\n",
    "        \n",
    "        # Step index\n",
    "        self.step_index = 0\n",
    "       \n",
    "    \n",
    "    # Reset function\n",
    "    def reset(self):\n",
    "        \n",
    "        # Set step_index to zero\n",
    "        self.step_index = 0\n",
    "        \n",
    "        return self.step_index\n",
    "    \n",
    "    \n",
    "    # Step function\n",
    "    def step(self, action):\n",
    "        \n",
    "        # Boolean if step_index is 10\n",
    "        is_done = self.step_index == 10\n",
    "        \n",
    "        # If step index is equal to 10\n",
    "        if is_done:\n",
    "            \n",
    "            return self.step_index % self.observation_space.n, 0.0, is_done, {}\n",
    "        \n",
    "        # Increment the step index\n",
    "        self.step_index += 1\n",
    "        \n",
    "        return self.step_index % self.observation_space.n, float(action), self.step_index == 10, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to this environment, we will use an agent that always generates fixed actions regardless of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "class DullAgent(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent always returns the fixed action\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, action: int):\n",
    "        \n",
    "        # Initialize the actions\n",
    "        self.action = action\n",
    "\n",
    "    # Call function that generates fixed actions regardless of observations\n",
    "    def __call__(self, observations: List[Any], state: Optional[List] = None) -> Tuple[List[int], Optional[List]]:\n",
    "        \n",
    "        return [self.action for _ in observations], state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 9. The ExperienceSource class\n",
    "\n",
    "---\n",
    "\n",
    "The first class is __ptan.experience.ExperienceSource__, which generates chunks of agent trajectories of the given length. The implementation automatically handles the end of episode situation (when the step() method in the environment returns is_done=True) and resets the environment.\n",
    "\n",
    "The constructor accepts several arguments:\n",
    "- The Gym environment to be used. Alternatively, it could be the list of environments.\n",
    "- The agent instance.\n",
    "- steps_count=2: the length of subtrajectories to be generated.\n",
    "- vectorized=False: if set to True, the environment needs to be an OpenAI Universe vectorized environment. We will discuss such environments in detail in Chapter 16, Web Navigation.\n",
    "\n",
    "The class instance provides the standard Python iterator interface, so you can just iterate over this to get subtrajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the environment\n",
    "env = ToyEnv()\n",
    "\n",
    "# Instantiate the agent\n",
    "agent = DullAgent(action=1)\n",
    "\n",
    "# Instantiate the experience source (2 steps count)\n",
    "exp_source = ptan.experience.ExperienceSource(env = env, agent = agent, steps_count = 2)\n",
    "\n",
    "# Get trajectories until 2nd index\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    if idx > 2:\n",
    "        break\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On every iteration, ExperienceSource returns a piece of the agent's trajectory\n",
    "in environment communication. It might look simple, but there are several things happening under the hood of our example:\n",
    "1. reset() was called in the environment to get the initial state\n",
    "2. The agent was asked to select the action to execute from the state returned\n",
    "3. The step() method was executed to get the reward and the next state\n",
    "4. This next state was passed to the agent for the next action\n",
    "5. Information about the transition from one state to the next state was returned\n",
    "6. The process iterated (from step 3) until it iterated over the experience source\n",
    "\n",
    "\n",
    "If the agent changes the way it generates actions (we can get this by updating the network weights, decreasing epsilon, or by some other means), it will immediately affect the experience trajectories that we get.\n",
    "\n",
    "The ExperienceSource instance returns tuples of length equal to or less than the argument step_count passed on construction. In our case, we asked for two-step subtrajectories, so tuples will be of length 2 or 1 (at the end of episodes). Every object in a tuple is an instance of the ptan.experience.Experience class, which is a namedtuple with the following fields:\n",
    "- state: the state we observed before taking the action\n",
    "- action: the action we completed\n",
    "- reward: the immediate reward we got from env\n",
    "- done: whether the episode was done\n",
    "\n",
    "If the episode reaches the end, the subtrajectory will be shorter and the underlying environment will be reset automatically, so we don't need to bother with this and can just keep iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=True))\n",
      "(Experience(state=4, action=1, reward=1.0, done=True),)\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "# Get trajectories until 15th index\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    if idx > 15:\n",
    "        break\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can ask ExperienceSource for subtrajectories of any length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience source (4 step count)\n",
    "exp_source = ptan.experience.ExperienceSource(env = env, agent = agent, steps_count = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Experience(state=0, action=1, reward=1.0, done=False),\n",
       " Experience(state=1, action=1, reward=1.0, done=False),\n",
       " Experience(state=2, action=1, reward=1.0, done=False),\n",
       " Experience(state=3, action=1, reward=1.0, done=False))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the next trajectory\n",
    "next(iter(exp_source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass it several instances of gym.Env. In that case, they will be used in round- robin fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience source (multiple environment)\n",
    "exp_source = ptan.experience.ExperienceSource(env = [env, env], agent = agent, steps_count = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=True))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=True))\n"
     ]
    }
   ],
   "source": [
    "# Get trajectories until 4th index\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    if idx > 4:\n",
    "        break\n",
    "    print(exp)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 10. ExperienceSourceFirstLast\n",
    "\n",
    "---\n",
    "\n",
    "The class ExperienceSource provides us with full subtrajectories of the given length as the list of (s, a, r) objects. The next state, s', is returned in the next tuple, which is not always convenient. For example, in DQN training, we want to have tuples (s, a, r, s') at once to do one-step Bellman approximation during the training. In addition, some extension of DQN, like n-step DQN, might want to collapse longer sequences of observations into (first-state, action, total-reward-for-n-steps, state-after-step-n).\n",
    "\n",
    "To support this in a generic way, a simple subclass of ExperienceSource is implemented: ExperienceSourceFirstLast. It accepts almost the same arguments in the constructor, but returns different data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n"
     ]
    }
   ],
   "source": [
    "# Experience source first-last (1 steps count)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma = 1.0, steps_count = 1)\n",
    "\n",
    "# Get trajectories until 10th index\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    print(exp)\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it returns a single object on every iteration, which is again a namedtuple with the following fields:\n",
    "- state: the state we used to decide on the action to take\n",
    "- action: the action we took at this step\n",
    "- reward: the partial accumulated reward for steps_count (in our case, steps_count=1, so it is equal to the immediate reward)\n",
    "- last_state: the state we got after executing the action. If our episode ends, we have None here\n",
    "\n",
    "This data is much more convenient for DQN training, as we can apply Bellman approximation directly to it.\n",
    "\n",
    "Let's check the result with a larger number of steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n",
      "ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3)\n",
      "ExperienceFirstLast(state=2, action=1, reward=2.0, last_state=4)\n",
      "ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=0)\n",
      "ExperienceFirstLast(state=4, action=1, reward=2.0, last_state=1)\n",
      "ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n",
      "ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3)\n",
      "ExperienceFirstLast(state=2, action=1, reward=2.0, last_state=4)\n",
      "ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=None)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
      "ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n",
      "ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3)\n"
     ]
    }
   ],
   "source": [
    "# Experience source first-last (2 steps count)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma = 1.0, steps_count = 2)\n",
    "\n",
    "# Get trajectories until 10th index\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    print(exp)\n",
    "    if idx > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we are collapsing two steps on every iteration and calculating the immediate reward (that's why reward=2.0 for most of the samples). More interesting samples are at the end of the episode:\n",
    "\n",
    "        ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=None)\n",
    "        ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
    "        \n",
    "As the episode ends, we have last_state=None in those samples, but additionally, we calculate the reward for the tail of the episode. Those tiny details are very easy to implement wrongly if you are doing all the trajectory handling yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 11. Experience replay buffers\n",
    "\n",
    "---\n",
    "\n",
    "In DQN, we rarely deal with immediate experience samples, as they are heavily correlated, which leads to instability in the training. Normally, we have large replay buffers, which are populated with experience pieces. Then the buffer is sampled (randomly or with priority weights) to get the training batch. The replay buffer normally has a maximum capacity, so old samples are pushed out when the replay buffer reaches the limit.\n",
    "\n",
    "There are several implementation tricks here, which become extremely important when you need to deal with large problems:\n",
    "\n",
    "- How to efficiently sample from a large buffer\n",
    "- How to push old samples from the buffer\n",
    "- In the case of a prioritized buffer, how priorities need to be maintained and handled in the most efficient way\n",
    "\n",
    "All this becomes a quite non-trivial task if you want to solve Atari, keeping 10-100M samples, where every sample is an image from the game. A small mistake can lead to a 10-100x memory increase and major slowdowns of the training process.\n",
    "\n",
    "PTAN provides several variants of replay buffers, which integrate simply with ExperienceSource and Agent machinery. Normally, what you need to do is ask the buffer to pull a new sample from the source and sample the training batch. The provided classes are:\n",
    "\n",
    "- __ExperienceReplayBuffer:__ a simple replay buffer of predefined size with uniform sampling.\n",
    "- __PrioReplayBufferNaive:__ a simple, but not very efficient, prioritized replay buffer implementation. The complexity of sampling is O(n), which might become an issue with large buffers. This version has the advantage over the optimized class, having much easier code.\n",
    "- __PrioritizedReplayBuffer:__ uses segment trees for sampling, which makes the code cryptic, but with O(log(n)) sampling complexity.\n",
    "The following shows how the replay buffer could be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import ptan\n",
    "from typing import List, Optional, Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "class ToyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment with observation 0-4 and actions 0-2. Observations are rotated sequentialy mod 5, reward is equal \n",
    "    to given action. Episodes are having fixed length of 10\n",
    "    \"\"\"\n",
    "\n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Inherite parent's constructor\n",
    "        super(ToyEnv, self).__init__()\n",
    "        \n",
    "        # Observation space\n",
    "        self.observation_space = gym.spaces.Discrete(n=5)\n",
    "        \n",
    "        # Action space\n",
    "        self.action_space = gym.spaces.Discrete(n=3)\n",
    "        \n",
    "        # Initialize the step index to zero\n",
    "        self.step_index = 0\n",
    "        \n",
    "\n",
    "    # Reset function\n",
    "    def reset(self):\n",
    "        \n",
    "        # Set step index back to \n",
    "        self.step_index = 0\n",
    "        \n",
    "        return self.step_index\n",
    "\n",
    "    \n",
    "    # Step function\n",
    "    def step(self, action):\n",
    "        \n",
    "        # Get is_done\n",
    "        is_done = self.step_index == 10\n",
    "        \n",
    "        # If terminal state\n",
    "        if is_done:\n",
    "            \n",
    "            return self.step_index % self.observation_space.n, 0.0, is_done, {}\n",
    "        \n",
    "        # Increment the step index\n",
    "        self.step_index += 1\n",
    "        \n",
    "        return self.step_index % self.observation_space.n, float(action), self.step_index == 10, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "class DullAgent(ptan.agent.BaseAgent):\n",
    "    \"\"\"\n",
    "    Agent always returns the fixed action\n",
    "    \"\"\"\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, action: int):\n",
    "        \n",
    "        # Initialize the action\n",
    "        self.action = action\n",
    "\n",
    "    # Call function\n",
    "    def __call__(self, observations: List[Any], state: Optional[List] = None) -> Tuple[List[int], Optional[List]]:\n",
    "        \n",
    "        return [self.action for _ in observations], state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "\n",
      "Train time, 4 batch samples:\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Instantiate the environment\n",
    "    env = ToyEnv()\n",
    "    \n",
    "    # Instantiate the agent\n",
    "    agent = DullAgent(action=1)\n",
    "    \n",
    "    # Instantiate the experience source\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma = 1.0, steps_count = 1)\n",
    "    \n",
    "    # Instantiate the experience buffer\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size = 100)\n",
    "\n",
    "    # Loop over 6 times\n",
    "    for step in range(6):\n",
    "        \n",
    "        # Populate the buffer\n",
    "        buffer.populate(1)\n",
    "        \n",
    "        # If buffer is less than 5\n",
    "        if len(buffer) < 5:\n",
    "            \n",
    "            # Start the loop again\n",
    "            continue\n",
    "            \n",
    "        # Sample batches\n",
    "        batch = buffer.sample(4)\n",
    "        \n",
    "        print(\"\\nTrain time, %d batch samples:\" % len(batch))\n",
    "        \n",
    "        # Iterate through batches\n",
    "        for s in batch:\n",
    "            \n",
    "            print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All replay buffers provide the following interface:\n",
    "- A Python iterator interface to walk over all the samples in the buffer\n",
    "- The method populate(N) to get N samples from the experience source and put them into the buffer\n",
    "- The method sample(N) to get the batch of N experience objects\n",
    "\n",
    "So, the normal training loop for DQN looks like an infinite repetition of the following steps:\n",
    "1. Call buffer.populate(1) to get a fresh sample from the environment\n",
    "2. batch = buffer.sample(BATCH_SIZE) to get the batch from the buffer\n",
    "3. Calculate the loss on the sampled batch\n",
    "4. Backpropagate\n",
    "5. Repeat until convergence (hopefully)\n",
    "\n",
    "All the rest happens automatically: resetting the environment, handling subtrajectories, buffer size maintenance, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 12. The TargetNet class\n",
    "\n",
    "---\n",
    "\n",
    "TargetNet is a small but useful class that allows us to synchronize two NNs of the same architecture. The purpose of this was described in the previous chapter: improving training stability. TargetNet supports two modes of such synchronization:\n",
    "\n",
    "- sync(): weights from the source network are copied into the target network.\n",
    "- alpha_sync(): the source network's weights are blended into the target network with some alpha weight (between 0 and 1).\n",
    "\n",
    "The first mode is the standard way to perform a target network sync in discrete action space problems, like Atari and CartPole. We did this in Chapter 6, Deep Q-Networks. The latter mode is used in continuous control problems, which will be described in several chapters in part four of the book. In such problems, the transition between two network's parameters should be smooth, so alhpha blending is used, given by the formula ${w_i = w_i \\alpha + s_i(1-\\alpha)}$, where ${w_i}$ is the target network's i<sup>th</sup> parameter and ${s_i}$ is the source network's weight. The following is a small example of how _TargetNet_ should be used in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import ptan\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Network\n",
    "class DQNNet(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Inherite the parent's constructor\n",
    "        super(DQNNet, self).__init__()\n",
    "        \n",
    "        # Initialize a linear layer\n",
    "        self.ff = nn.Linear(5, 3)\n",
    "\n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Source or Target Network: \n",
      " DQNNet(\n",
      "  (ff): Linear(in_features=5, out_features=3, bias=True)\n",
      ") \n",
      "\n",
      "* Source Network's Weight: \n",
      " Parameter containing:\n",
      "tensor([[-0.0047, -0.2140, -0.2696, -0.4327, -0.2010],\n",
      "        [ 0.0379, -0.1393,  0.0023, -0.3331, -0.3991],\n",
      "        [ 0.2437, -0.3072, -0.3382, -0.0287, -0.2591]], requires_grad=True) \n",
      "\n",
      "* Target Network's Weight: \n",
      " Parameter containing:\n",
      "tensor([[-0.0047, -0.2140, -0.2696, -0.4327, -0.2010],\n",
      "        [ 0.0379, -0.1393,  0.0023, -0.3331, -0.3991],\n",
      "        [ 0.2437, -0.3072, -0.3382, -0.0287, -0.2591]], requires_grad=True) \n",
      "\n",
      "* Source Network's Weight (After Adding 1 to Source Network): \n",
      " Parameter containing:\n",
      "tensor([[0.9953, 0.7860, 0.7304, 0.5673, 0.7990],\n",
      "        [1.0379, 0.8607, 1.0023, 0.6669, 0.6009],\n",
      "        [1.2437, 0.6928, 0.6618, 0.9713, 0.7409]], requires_grad=True) \n",
      "\n",
      "* Target Network's Weight (After Adding 1 to Source Network): \n",
      " Parameter containing:\n",
      "tensor([[-0.0047, -0.2140, -0.2696, -0.4327, -0.2010],\n",
      "        [ 0.0379, -0.1393,  0.0023, -0.3331, -0.3991],\n",
      "        [ 0.2437, -0.3072, -0.3382, -0.0287, -0.2591]], requires_grad=True) \n",
      "\n",
      "* Source Network's Weight (After Synching): \n",
      " Parameter containing:\n",
      "tensor([[0.9953, 0.7860, 0.7304, 0.5673, 0.7990],\n",
      "        [1.0379, 0.8607, 1.0023, 0.6669, 0.6009],\n",
      "        [1.2437, 0.6928, 0.6618, 0.9713, 0.7409]], requires_grad=True) \n",
      "\n",
      "* Target Network's Weight (After Synching): \n",
      " Parameter containing:\n",
      "tensor([[0.9953, 0.7860, 0.7304, 0.5673, 0.7990],\n",
      "        [1.0379, 0.8607, 1.0023, 0.6669, 0.6009],\n",
      "        [1.2437, 0.6928, 0.6618, 0.9713, 0.7409]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Instantiate the DQN network (source & target network)\n",
    "    net = DQNNet()\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    print(\"* Source or Target Network: \\n\", net, \"\\n\")\n",
    "    \n",
    "    # Print the weights (source & target network)\n",
    "    print(\"* Source Network's Weight: \\n\", net.ff.weight, \"\\n\")\n",
    "    print(\"* Target Network's Weight: \\n\", tgt_net.target_model.ff.weight, \"\\n\")\n",
    "    \n",
    "    # Add 1 to source network\n",
    "    net.ff.weight.data += 1.0\n",
    "    print(\"* Source Network's Weight (After Adding 1 to Source Network): \\n\", net.ff.weight, \"\\n\")\n",
    "    print(\"* Target Network's Weight (After Adding 1 to Source Network): \\n\", tgt_net.target_model.ff.weight, \"\\n\")\n",
    "    \n",
    "    # Sync the weights\n",
    "    tgt_net.sync()\n",
    "    print(\"* Source Network's Weight (After Synching): \\n\", net.ff.weight, \"\\n\")\n",
    "    print(\"* Target Network's Weight (After Synching): \\n\", tgt_net.target_model.ff.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 13. Ignite helpers\n",
    "\n",
    "---\n",
    "\n",
    "PyTorch Ignite was briefly discussed in Chapter 3, Deep Learning with PyTorch and it will be used in the rest of the book to reduce the amount of training loop code. PTAN provides several small helpers to simplify integration with Ignite, which reside in the ptan.ignite package:\n",
    "- __EndOfEpisodeHandler:__ attached to the ignite.Engine, it emits an EPISODE_COMPLETED event, and tracks the reward and number of steps in the event in the engine's metrics. It also can emit an event when the average reward for the last episodes reaches the predefined boundary, which is supposed to be used to stop the training on some goal reward.\n",
    "\n",
    "- __EpisodeFPSHandler:__ tracks the number of interactions between the agent and environment that are performed and calculates performance metrics as frames per second. It also keeps the number of seconds passed since the start of the training.\n",
    "\n",
    "- __PeriodicEvents:__ emits corresponding events every 10, 100, or 1,000 training iterations. It is useful for reducing the amount of data being written into TensorBoard.\n",
    "\n",
    "A detailed illustration of how the preceding classes can be used will be given in the next chapter, when we will use them to reimplement DQN training from Chapter 6, Deep Q-Networks, and then check several DQN extensions and tweaks to improve basic DQN convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 14. The PTAN CartPole solver\n",
    "\n",
    "---\n",
    "\n",
    "Let's now take the PTAN classes (without Ignite so far) and try to combine everything together to solve our first environment: CartPole. The complete code is in Chapter07/06_cartpole.py. I will show only the important parts of the code related to the material that we have just covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "import ptan\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "TGT_NET_SYNC = 10\n",
    "GAMMA = 0.9\n",
    "REPLAY_SIZE = 1000\n",
    "LR = 1e-3\n",
    "EPS_DECAY=0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        \n",
    "        # Inherite parent's constructor\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Feedforward the x into the network\n",
    "        output = self.net(x.float())\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all operations to have no gradient\n",
    "@torch.no_grad()\n",
    "\n",
    "# Unpacking the batches\n",
    "def unpack_batch(batch, net, gamma):\n",
    "    \n",
    "    # Initialize empty list for S, A, R, is_done, S'\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    done_masks = []\n",
    "    last_states = []\n",
    "    \n",
    "    # Loop over experiences in batch\n",
    "    for exp in batch:\n",
    "        \n",
    "        # Append the S, A, R, is_done, S' to list\n",
    "        states.append(exp.state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        done_masks.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            last_states.append(exp.state)\n",
    "        else:\n",
    "            last_states.append(exp.last_state)\n",
    "\n",
    "    # Convert S, A, R, S' to torch tensor\n",
    "    states_v = torch.tensor(states)\n",
    "    actions_v = torch.tensor(actions)\n",
    "    rewards_v = torch.tensor(rewards)\n",
    "    last_states_v = torch.tensor(last_states)\n",
    "    \n",
    "    # Feedforward the last state into network\n",
    "    last_state_q_v = net(last_states_v)\n",
    "    \n",
    "    # Get the maximum value\n",
    "    best_last_q_v = torch.max(last_state_q_v, dim=1)[0]\n",
    "    \n",
    "    # Set index (which are is_done) to zero\n",
    "    best_last_q_v[done_masks] = 0.0\n",
    "    \n",
    "    return states_v, actions_v, best_last_q_v * gamma + rewards_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: episode 1 done, reward=15.000, epsilon=1.00\n",
      "67: episode 2 done, reward=51.000, epsilon=0.70\n",
      "81: episode 3 done, reward=14.000, epsilon=0.61\n",
      "95: episode 4 done, reward=14.000, epsilon=0.53\n",
      "108: episode 5 done, reward=13.000, epsilon=0.47\n",
      "120: episode 6 done, reward=12.000, epsilon=0.41\n",
      "131: episode 7 done, reward=11.000, epsilon=0.37\n",
      "143: episode 8 done, reward=12.000, epsilon=0.33\n",
      "152: episode 9 done, reward=9.000, epsilon=0.30\n",
      "162: episode 10 done, reward=10.000, epsilon=0.27\n",
      "171: episode 11 done, reward=9.000, epsilon=0.25\n",
      "181: episode 12 done, reward=10.000, epsilon=0.22\n",
      "193: episode 13 done, reward=12.000, epsilon=0.20\n",
      "202: episode 14 done, reward=9.000, epsilon=0.18\n",
      "212: episode 15 done, reward=10.000, epsilon=0.16\n",
      "226: episode 16 done, reward=14.000, epsilon=0.14\n",
      "238: episode 17 done, reward=12.000, epsilon=0.13\n",
      "253: episode 18 done, reward=15.000, epsilon=0.11\n",
      "264: episode 19 done, reward=11.000, epsilon=0.10\n",
      "282: episode 20 done, reward=18.000, epsilon=0.08\n",
      "298: episode 21 done, reward=16.000, epsilon=0.07\n",
      "312: episode 22 done, reward=14.000, epsilon=0.06\n",
      "331: episode 23 done, reward=19.000, epsilon=0.05\n",
      "348: episode 24 done, reward=17.000, epsilon=0.04\n",
      "362: episode 25 done, reward=14.000, epsilon=0.04\n",
      "385: episode 26 done, reward=23.000, epsilon=0.03\n",
      "406: episode 27 done, reward=21.000, epsilon=0.02\n",
      "446: episode 28 done, reward=40.000, epsilon=0.02\n",
      "479: episode 29 done, reward=33.000, epsilon=0.01\n",
      "504: episode 30 done, reward=25.000, epsilon=0.01\n",
      "556: episode 31 done, reward=52.000, epsilon=0.01\n",
      "600: episode 32 done, reward=44.000, epsilon=0.00\n",
      "620: episode 33 done, reward=20.000, epsilon=0.00\n",
      "657: episode 34 done, reward=37.000, epsilon=0.00\n",
      "724: episode 35 done, reward=67.000, epsilon=0.00\n",
      "763: episode 36 done, reward=39.000, epsilon=0.00\n",
      "804: episode 37 done, reward=41.000, epsilon=0.00\n",
      "1004: episode 38 done, reward=200.000, epsilon=0.00\n",
      "Congrats!\n"
     ]
    }
   ],
   "source": [
    "# Start the program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Create the CartPole environment\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    \n",
    "    # Get the observation size\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    \n",
    "    # Get the number of actions\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    # Instantiate the source network\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    \n",
    "    # Instantiate the target network\n",
    "    tgt_net = ptan.agent.TargetNet(net)\n",
    "    \n",
    "    # Action selector (argmax)\n",
    "    selector = ptan.actions.ArgmaxActionSelector()\n",
    "    \n",
    "    # Action selector (epsilon greedy)\n",
    "    selector = ptan.actions.EpsilonGreedyActionSelector(epsilon = 1, selector = selector)\n",
    "    \n",
    "    # DQN Agent\n",
    "    agent = ptan.agent.DQNAgent(net, selector)\n",
    "    \n",
    "    # Experience source first-last\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma = GAMMA)\n",
    "    \n",
    "    # Experience buffer\n",
    "    buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size = REPLAY_SIZE)\n",
    "    optimizer = optim.Adam(net.parameters(), LR)\n",
    "\n",
    "    # Initialize the step number\n",
    "    step = 0\n",
    "    \n",
    "    # Initialize the episode number\n",
    "    episode = 0\n",
    "    \n",
    "    # Initialize the 'solved' to false\n",
    "    solved = False\n",
    "\n",
    "    # Infinite loop\n",
    "    while True:\n",
    "        \n",
    "        # Increment the step number\n",
    "        step += 1\n",
    "        \n",
    "        # Populate the buffer\n",
    "        buffer.populate(1)\n",
    "\n",
    "        # Iterate over reward and steps of current experience source\n",
    "        for reward, steps in exp_source.pop_rewards_steps():\n",
    "            \n",
    "            # Increment the episode number\n",
    "            episode += 1\n",
    "            \n",
    "            # Report\n",
    "            print(\"%d: episode %d done, reward=%.3f, epsilon=%.2f\" % (step, episode, reward, selector.epsilon))\n",
    "            \n",
    "            # If reward is higher than 150 then set 'solved' to true\n",
    "            solved = reward > 150\n",
    "            \n",
    "        # If solved\n",
    "        if solved:\n",
    "            \n",
    "            # Report\n",
    "            print(\"Congrats!\")\n",
    "            \n",
    "            # Break the loop\n",
    "            break\n",
    "\n",
    "        # If length of experience buffer is less than 2*batch sizes\n",
    "        if len(buffer) < 2*BATCH_SIZE:\n",
    "            \n",
    "            # Start the loop over\n",
    "            continue\n",
    "\n",
    "        # Sample from buffer\n",
    "        batch = buffer.sample(BATCH_SIZE)\n",
    "        \n",
    "        # Unpack the batch\n",
    "        states_v, actions_v, tgt_q_v = unpack_batch(batch, tgt_net.target_model, GAMMA)\n",
    "        \n",
    "        # Reset the gradients of optimizer to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feedforward\n",
    "        q_v = net(states_v)\n",
    "        \n",
    "        # Reshape the output so that it's suitable for DQN training\n",
    "        q_v = q_v.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # Calculate the MSE loss\n",
    "        loss_v = F.mse_loss(q_v, tgt_q_v)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss_v.backward()\n",
    "        \n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Decay the value of epsilon\n",
    "        selector.epsilon *= EPS_DECAY\n",
    "\n",
    "        # Every TGT_NET_SYNC time\n",
    "        if step % TGT_NET_SYNC == 0:\n",
    "            \n",
    "            # Sync the source network to target network\n",
    "            tgt_net.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 15. Other RL libraries\n",
    "\n",
    "---\n",
    "\n",
    "As we discussed earlier, there are several RL-specific libraries available. Overall, TensorFlow is more popular than PyTorch, as it is more widespread in the deep learning community. The following is my (very biased) list of libraries:\n",
    "- __Keras-RL:__ started by Matthias Plappert in 2016, this includes basic deep RL methods. As suggested by the name, this library was implemented using Keras, which is a higher-level wrapper around TensorFlow (https://github.com/keras-rl/keras-rl).\n",
    "- __Dopamine:__ a library from Google published in 2018. It is TensorFlow- specific, which is not surprising for a library from Google (https://github.com/google/dopamine).\n",
    "- __Ray:__ a library for distributed execution of machine learning code. It includes RL utilities as part of the library (https://github.com/ray-project/ray).\n",
    "- __TF-Agents:__ another library from Google published in 2018 (https://github.com/tensorflow/agents).\n",
    "- __ReAgent:__ a library from Facebook Research. It uses PyTorch internally and uses a declarative style of configuration (when you are creating a JSON file to describe your problem), which limits extensibility. But, of course, as it is open source, you can always extend the functionality (https://github.com/ facebookresearch/ReAgent).\n",
    "- __Catalyst.RL:__ a project started by Sergey Kolesnikov (one of this book's technical reviewers). It uses PyTorch as the backend (https://github.com/catalyst-team/catalyst).\n",
    "- __SLM Lab:__ another PyTorch RL library (https://github.com/kengz/SLM-Lab)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 16. Summary\n",
    "\n",
    "---\n",
    "\n",
    "In this chapter, we talked about higher-level RL libraries, their motivation, and their requirements. Then we took a deep look into the PTAN library, which will be used in the rest of the book to simplify example code.\n",
    "In the next chapter, we will return to DQN methods by exploring extensions that researchers and practitioners have discovered since the classic DQN introduction to improve the stability and performance of the method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Good Job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
