{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">The Cross-Entropy Method</h1>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this chapter, we will learn one of the RL methods called __cross-entropy__. Despite being less famous than other RL algorithms, this method has it own strength:\n",
    "\n",
    "- __Simplicity:__ This method is really simple, which makes it an intuitive method to follow. Its implementation on PyTorch is less than 100 lines of code.\n",
    "\n",
    "- __Good convergence:__ In simple environments, this method usually works very well. Lots of practical problems don't fall into this category, but sometimes they do. In such cases, cross-entropy (on its own or as a part of a larger system) can be the perfect fit.\n",
    "\n",
    "In the following sections, we will start from the practical side of cross-entropy, and then look at how it works in two environments in Gym. Then, at the end of the chapter, we will take a look at the theoretical background of the method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 1. Taxonomy of RL methods\n",
    "\n",
    "---\n",
    "\n",
    "All methods in RL can be classified into various aspects:\n",
    "\n",
    "1. Model-free or model-based\n",
    "2. Value-based or policy-based\n",
    "3. On-policy or off-policy\n",
    "\n",
    "For example, the cross-entropy method falls into the model-free and policy-based category of methods. These notions are new, so let's spend some time exploring them. There are other ways that you can taxonomize RL methods, but for now we're interested in the preceding three. \n",
    "\n",
    "<img width=\"800px\" src=\"./assets/rl_taxomy.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.1. Model-Free or Model-Based\n",
    "\n",
    "\n",
    "Let's define the model-free and model-based methods:\n",
    "\n",
    "- __Model-free__ means that the agent takes current observations and does some computation on them, and the result is the action that it should take. In other words, the method directly connects observations to actions (or values that are related to actions).\n",
    "\n",
    "\n",
    "- __Model-based__ means that the method predict the next observation and/or reward. Based on that, the agent choose the best action to take. \n",
    "\n",
    "<img width = \"600px\" src = \"assets/model_free_model_based.gif\">\n",
    "\n",
    "Both classes of methods have strong and weak sides, but usually pure __model-based__ methods are used in deterministic environments (such as board games with strict rules). On the other hand, __model-free__ methods are usually easier to train as it's hard to build good models of complex environments with rich observations. \n",
    "\n",
    "<img width = \"400px\" src = \"assets/model_free_model_based_2.png\">\n",
    "\n",
    "All of the methods in this book are model-free. Recently the researchers started to mix the benefits from both sides (for example, refer to DeepMind's papers on imagination in agents. This approach will be described in Chapter 17, Beyond Model-Free â€“ Imagination)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.2. Value-Based or Policy-Based\n",
    "\n",
    "__Policy-based__ methods directly approximate the policy of the agent (e.g. the actions that the agent should take at every step). Policy is usually represented by probability distribution over the available actions. \n",
    "\n",
    "In __value-based__ methods, the agent calculates the value of every possible action, and chooses the action with the best value. \n",
    "\n",
    "Both of these families of methods are equally popular and we'll discuss both of them later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### 1.3. On-policy or off-policy\n",
    "\n",
    "We'll discuss the distinction of on-policy and off-policy more in parts 2 and 3 of the book. The off-policy method is the ability of the method to learn on old historical data (obtained by a previous version of the agent or recorded by human demonstration or just seen by the same agent several episodes ago). While on-policy method requires fresh data obtained from the environment.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 2. Practical cross-entropy\n",
    "\n",
    "---\n",
    "\n",
    "The cross-entropy method is: \n",
    "- __Model-free:__ It doesn't build any model of the environment, it just says to the agent what to do at every step\n",
    "- __Policy-based:__ It approximates the policy of the agent\n",
    "- __On-policy:__ It requires fresh data obtained from the environment\n",
    "\n",
    "The central thing in RL is the agent which it tries to accumulate as much total reward as possible. In practice, all of the complications of the agent is replaced with some kind of nonlinear trainable function. This function maps the agent's input (observations from the environment) to some output. Since the cross-entropy method is policy-based, then our nonlinear function (neural network) produces policy, that says for every observation which action the agent should take.\n",
    "\n",
    "<img width=\"500px\" src=\"./assets/high_level_RL.png\">\n",
    "\n",
    "The policy is usually represented as a probability distribution over actions. This is similar to a classification problem in which the number of classes is equal to the number of actions. This makes our agent very simple; It passes an observation from the environment to the network. Then it takes the probability distribution over actions, and perform random sampling using probability distribution to take an action. This random sampling adds randomness to our agent, which is a good thing. At the beginning of the training when our weights are random, the agent behaves randomly. After the agent takes an action in the environment, it obtains the next observation and reward for the last action. Then the loop continues.\n",
    "\n",
    "\n",
    "The agent's experience is presented as __episodes__. An episode is represented as:\n",
    "1. A sequence of observations that the agent received from the environment.\n",
    "2. The actions it took.\n",
    "3. The rewards it received (from the action it took).\n",
    "\n",
    "In each episode, the total reward can be __discounted__ or not. For simplicity reasons, let's assume a discount factor of gamma = 1 (e.g. the sum of all local rewards for every episode). This total reward shows how good this episode was for the agent. Let's illustrate this with a diagram, which contains four episodes (note that different episodes have different values for O<sub>i</sub> , a<sub>i</sub> , and r<sub>i</sub> ):\n",
    "\n",
    "\n",
    "<img width=\"700px\" src=\"./assets/sample_episode.png\">\n",
    "\n",
    "Every cell represents the agent's step in the episode. Due to randomness in the environment and the way that the agent selects actions, some episodes will be better than others. The core of the cross-entropy method is to throw away bad episodes and train on better ones. \n",
    "\n",
    "__The steps for the cross-entropy method are as follows:__\n",
    "\n",
    "1. Play N number of episodes using our current model and environment.\n",
    "2. Calculate the total reward for every episode and decide on a reward boundary. Usually, we use some percentile of all rewards, such as 50th or 70th.\n",
    "3. Throw away all episodes with a reward below the boundary.\n",
    "4. Train on the remaining \"elite\" episodes using observations as the input and issued actions as the desired output.\n",
    "5. Repeat from step 1 until we become satisfied with the result.\n",
    "\n",
    "With the preceding procedure, our neural network learns how to repeat actions, which leads to a larger reward, constantly moving the boundary higher and higher. Despite the simplicity of this method, it works well in simple environments, it's easy to implement, and it's quite robust to hyperparameters changing, which makes it an ideal baseline method to try. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 3. Cross-entropy on CartPole\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we will apply the cross-entropy method to our CartPole environment. Our model's core is a one-hidden-layer neural network, with ReLU and 128 hidden neurons (which is absolutely arbitrary). Other hyperparameters are also set almost randomly and aren't tuned, as the method is robust and converges very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "HIDDEN_SIZE = 128     # Count of neurons in the hidden layer\n",
    "BATCH_SIZE = 16       # Count of episodes we play on every iteration\n",
    "PERCENTILE = 70       # The percentile of episodes' total rewards that we use for elite episode filtering. 70th percentile means that we'll leave the top 30% of episodes sorted by reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network in cross-entropy method, takes a single observation as an input, and outputs a number for every action. Since the output is a probability distribution over actions, it's a good idea to include softmax nonlinearity at the very end. However, in this network we don't use a softmax layer in order to increase the numerical stability of the training process. So, instead of calculating softmax and then calculating cross-entropy loss, we'll use the PyTorch class, __nn.CrossEntropyLoss__, which combines both softmax and cross-entropy in a single, more numerically stable expression. CrossEntropyLoss requires raw, unnormalized values from the network (e.g. logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network class that inherits nn.Module\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # The constructor\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        \n",
    "        # Call the parent's constructor to initialize itself\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Create a sequential with layers\n",
    "        self.net = nn.Sequential(nn.Linear(in_features = obs_size, out_features = hidden_size),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(in_features = hidden_size, out_features = n_actions))\n",
    "    \n",
    "    # Forward function\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define two helper classes:\n",
    "- __EpisodeStep:__ This will be used to represent one single step that the agent make in the episode. It stores the observation, the completed action. We'll use episode steps from elite episodes as training data.\n",
    "- __Episode:__ This is a single episode which stores the total (un-discounted) reward, and a collection of EpisodeStep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nametuple for episode (it stores reward and collection of EpisodeStep)\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "\n",
    "# Nametuple for episode step (it stores observation and action)\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates batches with episodes\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    \"\"\"\n",
    "    Generate batches with episodes.\n",
    "    \n",
    "    PARAMETERS\n",
    "    =================\n",
    "        - env: The environment\n",
    "        - net: The network\n",
    "        - batch_size: Size of the batches\n",
    "        \n",
    "    RETURNS\n",
    "    =================\n",
    "        - batch: Yielding batches\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a list for batch\n",
    "    batch = []\n",
    "    \n",
    "    # Initialize the episode reward\n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    # Initialize a list for episode steps\n",
    "    episode_steps = []\n",
    "    \n",
    "    # Reset the environment and get the first observation\n",
    "    obs = env.reset()\n",
    "    \n",
    "    # Create a softmax layer (for converting the network's output to a probability distribution of actions)\n",
    "    sm = nn.Softmax(dim = 1)\n",
    "    \n",
    "    # Infinite loop\n",
    "    while True:\n",
    "        \n",
    "        # Convert the current observation to a PyTorch tensor\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        \n",
    "        # Pass the observation to the network and get the action probabilities\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        \n",
    "        # Get the data (since tensors track gradients) + convert to NumPy array + Get the first batch element to obtain a one-dimensional vector of action probabilities\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        \n",
    "        # Random sample of action from probability distribution of actions\n",
    "        action = np.random.choice(len(act_probs), p = act_probs)\n",
    "        \n",
    "        # Render environment\n",
    "        #env.render()\n",
    "        \n",
    "        # Take action and get the next observation, reward, the indication of the episode ending\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        \n",
    "        # Add the reward to episode's total reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Add (observation, action) pair to episode steps - We save the observation that was used to choose the action, and not the observation returned by the environment. Keep these tiny detailes in mind.\n",
    "        episode_steps.append(EpisodeStep(observation = obs, action = action))\n",
    "        \n",
    "        # If episode is ended - When the stick fallen down in CartPole Environment\n",
    "        if is_done:\n",
    "\n",
    "            # Append the finalized episode to the batch\n",
    "            batch.append(Episode(reward = episode_reward, steps = episode_steps))\n",
    "\n",
    "            # Reset the total reward accumulator\n",
    "            episode_reward = 0.0\n",
    "\n",
    "            # Clean the list of episode steps\n",
    "            episode_steps = []\n",
    "\n",
    "            # Reset the environment (to start over) and get the observation\n",
    "            next_obs = env.reset()\n",
    "\n",
    "            # If the batch has reached the desired count of episodes\n",
    "            if len(batch) == batch_size:\n",
    "\n",
    "                # Yield the batch\n",
    "                yield batch\n",
    "\n",
    "                # Empty the batch\n",
    "                batch = []\n",
    "        \n",
    "        # Update the current observation\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the boundary reward (which is used to filter elite episodes )\n",
    "def filter_batch(batch, percentile):\n",
    "    \"\"\"\n",
    "    Given batch of episodes and percentile value, this function calculates a boundary reward, which is used \n",
    "    to filter elite episodes to train on. \n",
    "    \n",
    "    PARAMETERS\n",
    "    ==================\n",
    "        - batch\n",
    "        - percentile\n",
    "        \n",
    "    RETURNS\n",
    "    ==================\n",
    "        - train_obs_v: The training observations\n",
    "        - train_act_v: The training actions\n",
    "        - reward_bound: The boundary of reward (used only for TensorBoard)\n",
    "        - reward_mean: The mean reward (used only for TensorBoard)\n",
    "    \"\"\"\n",
    "    # Get the rewards of each batch\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    \n",
    "    # Obtain the boundary reward\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    \n",
    "    # Calculate the mean reward (used for monitoring)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    \n",
    "    # Initialize an empty list for training observations\n",
    "    train_obs = []\n",
    "    \n",
    "    # Initialize an empty list for training actions\n",
    "    train_act = []\n",
    "    \n",
    "    # Iterate through each episode in the batch\n",
    "    for example in batch:\n",
    "        \n",
    "        # If reward is below boundary reward then go back at the start of loop\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "            \n",
    "        # Populate the list of observations we will train on    \n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        \n",
    "        # Populate the list of actions we will train on    \n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    # Convert the observations from elite episodes into tensors\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    \n",
    "    # Convert the actions from elite episodes into tensors\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    \n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.695, reward_mean=18.3, reward_bound=22.0\n",
      "1: loss=0.673, reward_mean=27.6, reward_bound=33.0\n",
      "2: loss=0.653, reward_mean=43.6, reward_bound=45.0\n",
      "3: loss=0.638, reward_mean=40.1, reward_bound=39.5\n",
      "4: loss=0.617, reward_mean=41.6, reward_bound=47.5\n",
      "5: loss=0.613, reward_mean=49.6, reward_bound=59.0\n",
      "6: loss=0.597, reward_mean=55.0, reward_bound=60.5\n",
      "7: loss=0.595, reward_mean=57.2, reward_bound=56.5\n",
      "8: loss=0.567, reward_mean=75.7, reward_bound=101.0\n",
      "9: loss=0.598, reward_mean=52.7, reward_bound=55.0\n",
      "10: loss=0.546, reward_mean=63.8, reward_bound=74.0\n",
      "11: loss=0.579, reward_mean=56.7, reward_bound=55.0\n",
      "12: loss=0.552, reward_mean=64.6, reward_bound=72.0\n",
      "13: loss=0.539, reward_mean=63.5, reward_bound=66.0\n",
      "14: loss=0.532, reward_mean=73.9, reward_bound=87.0\n",
      "15: loss=0.542, reward_mean=71.8, reward_bound=82.0\n",
      "16: loss=0.526, reward_mean=82.6, reward_bound=88.5\n",
      "17: loss=0.527, reward_mean=90.1, reward_bound=100.5\n",
      "18: loss=0.533, reward_mean=77.7, reward_bound=79.0\n",
      "19: loss=0.545, reward_mean=80.4, reward_bound=92.5\n",
      "20: loss=0.544, reward_mean=83.8, reward_bound=83.5\n",
      "21: loss=0.529, reward_mean=85.2, reward_bound=93.0\n",
      "22: loss=0.528, reward_mean=116.7, reward_bound=129.0\n",
      "23: loss=0.523, reward_mean=117.4, reward_bound=141.5\n",
      "24: loss=0.523, reward_mean=129.4, reward_bound=151.0\n",
      "25: loss=0.516, reward_mean=127.9, reward_bound=172.5\n",
      "26: loss=0.522, reward_mean=132.4, reward_bound=142.0\n",
      "27: loss=0.526, reward_mean=164.6, reward_bound=200.0\n",
      "28: loss=0.518, reward_mean=161.6, reward_bound=200.0\n",
      "29: loss=0.517, reward_mean=184.1, reward_bound=200.0\n",
      "30: loss=0.530, reward_mean=183.8, reward_bound=200.0\n",
      "31: loss=0.525, reward_mean=186.0, reward_bound=200.0\n",
      "32: loss=0.537, reward_mean=198.6, reward_bound=200.0\n",
      "33: loss=0.529, reward_mean=195.6, reward_bound=200.0\n",
      "34: loss=0.532, reward_mean=200.0, reward_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "# Execute the main program\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Make the CartPole-v0 environment\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    \n",
    "    # Create a monitor to write videos of the agent's performance\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    \n",
    "    # Get the observation size\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    \n",
    "    # Get the number of actions\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    # Initialize the network\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    \n",
    "    # Initialize the cross entropy loss\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize the Adam optimizer\n",
    "    optimizer = optim.Adam(params = net.parameters(), lr = 0.01)\n",
    "    \n",
    "    # Writer of data (for TensorBoard)\n",
    "    writer = SummaryWriter(comment = \"-cartpole\")\n",
    "\n",
    "    # Iterate through the batches (which are a list of episode objects)\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        \n",
    "        # Perform filtering of the elite episodes and get observations, actions, the reward boundary (used for filtering), and the mean reward \n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        \n",
    "        # Zero out the gradients of our network\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass the observations to the network and get the action scores\n",
    "        action_scores_v = net(obs_v)\n",
    "        \n",
    "        # Calculates the cross-entropy between the network output and the actions that the agent took - The idea of this is to reinforce our network to carry out those \"elite\" actions which have led to good rewards.\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        \n",
    "        # Calculate the gradients on the loss\n",
    "        loss_v.backward()\n",
    "        \n",
    "        # Make optimizer to adjust the network\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print out the loss, mean loss, and boundary reward\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        \n",
    "        # Write the values for loss, mean loss, and boundary reward to TensorBoard\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        \n",
    "        # If mean reward becomes greater than 199 then stop the training\n",
    "        if reward_m > 199:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "            \n",
    "    # Close the writer\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.12.2 at http://localhost:6006 (Press CTRL+C to quit)\r\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir runs --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above, when the mean reward becomes greater than 199, we stop our training. Why 199? In Gym, the CartPole environment is considered to be solved when the mean reward for last 100 episodes is greater than 195, but our method converges so quickly that 100 episodes are usually what we need. The properly trained agent can balance the stick infinitely long (obtaining any amount of score), but the length of the episode in CartPole is limited to 200 steps (if you look at the environment variable of CartPole, you may notice the TimeLimit wrapper, which stops the episode after 200 steps). With all this in mind, we will stop training after the mean reward in the batch is greater than 199, which is a good indication that our agent knows how to balance the stick as a pro.\n",
    "\n",
    "\n",
    "The RL training in here usually doesn't take the agent more than 50 batches to solve the environment. My experiments show something from 25 to 45 episodes, which is a really good learning performance (remember, we need to play only 16 episodes for every batch). TensorBoard shows our agent consistently making progress, pushing the upper boundary at almost every batch (there are some periods of rolling down, but most of the time it improves).\n",
    "\n",
    "\n",
    "<img width=\"800px\" src=\"assets/report_training.png\">\n",
    "\n",
    "To check our agent in action, you can enable Monitor by uncommenting the next line after the environment creation. After restarting (possibly with xvfb-run to provide\n",
    "a virtual X11 display), our program will create a mon directory with videos recorded at different training steps:\n",
    "\n",
    "                        rl_book_samples/Chapter04$ xvfb-run -s \"-screen 0 640x480x24\" ./01_\n",
    "                        cartpole.py\n",
    "                        [2017-10-04 13:52:23,806] Making new env: CartPole-v0\n",
    "                        [2017-10-04 13:52:23,814] Creating monitor directory mon\n",
    "                        [2017-10-04 13:52:23,920] Starting new video recorder writing to mon/\n",
    "                        openaigym.video.0.4430.video000000.mp4\n",
    "                        [2017-10-04 13:52:25,229] Starting new video recorder writing to mon/\n",
    "                        openaigym.video.0.4430.video000001.mp4\n",
    "                        [2017-10-04 13:52:25,771] Starting new video recorder writing to mon/\n",
    "                        openaigym.video.0.4430.video000008.mp4\n",
    "                        0: loss=0.682, reward_mean=18.9, reward_bound=20.5\n",
    "                        [2017-10-04 13:52:26,297] Starting new video recorder writing to mon/\n",
    "                        openaigym.video.0.4430.video000027.mp4\n",
    "                        1: loss=0.687, reward_mean=16.6, reward_bound=19.0\n",
    "                        2: loss=0.677, reward_mean=21.1, reward_bound=21.0\n",
    "                        [2017-10-04 13:52:26,964] Starting new video recorder writing to mon/\n",
    "                        openaigym.video.0.4430.video000064.mp4\n",
    "                        3: loss=0.653, reward_mean=33.2, reward_bound=48.5\n",
    "                        4: loss=0.642, reward_mean=37.4, reward_bound=42.5\n",
    "                        .........\n",
    "                        29: loss=0.561, reward_mean=111.6, reward_bound=122.0\n",
    "                        30: loss=0.540, reward_mean=135.1, reward_bound=166.0\n",
    "                        [2017-10-04 13:52:40,176] Starting new video recorder writing to mon/\n",
    "                        openaigym.video.0.4430.video000512.mp4\n",
    "                        31: loss=0.546, reward_mean=147.5, reward_bound=179.5\n",
    "                        32: loss=0.559, reward_mean=140.0, reward_bound=171.5\n",
    "                        33: loss=0.558, reward_mean=160.4, reward_bound=200.0\n",
    "                        34: loss=0.547, reward_mean=167.6, reward_bound=195.5\n",
    "                        35: loss=0.550, reward_mean=179.5, reward_bound=200.0\n",
    "                        36: loss=0.563, reward_mean=173.9, reward_bound=200.0\n",
    "                        37: loss=0.542, reward_mean=162.9, reward_bound=200.0\n",
    "                        38: loss=0.552, reward_mean=159.1, reward_bound=200.0\n",
    "                        39: loss=0.548, reward_mean=189.6, reward_bound=200.0\n",
    "                        40: loss=0.546, reward_mean=191.1, reward_bound=200.0\n",
    "                        41: loss=0.548, reward_mean=199.1, reward_bound=200.0\n",
    "                        Solved!\n",
    "                        \n",
    "As you can see from the output, it turns a periodical recording of the agent's activity into separate video files, which can give you an idea of what your agent's sessions look like.\n",
    "\n",
    "<img width=\"400px\" src=\"./assets/cartpole_visualize.png\">\n",
    "\n",
    "Let's now pause a bit and think about what's just happened. Our neural network has learned how to play the environment purely from observations and rewards, without any one word interpretation of observed values. The environment could easily be\n",
    "not a cart with a stick but, say, a warehouse model with product quantities as an observation and money earned as a reward. Our implementation doesn't depend on environment details. This is the beauty of the RL model, and in the next section, we'll look at how exactly the same method can be applied to a different environment from the Gym collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 4. Cross-entropy on FrozenLake\n",
    "\n",
    "---\n",
    "\n",
    "The next environment we'll try to solve using the cross-entropy method is __FrozenLake__. This environment has the following \n",
    "- This environment is a grid world of 4x4.\n",
    "- The agent can move in four directions: up, down, left, and right. \n",
    "- The agent always starts at a top-left position, and its goal is to reach the bottom-right cell of the grid. \n",
    "- There are holes in the fixed cells of the grid and if you get into those holes, your reward is 0.0 and the episode ends.\n",
    "- If the agent reaches the destination cell, then it obtains the reward 1.0 and the episode ends.\n",
    "- To make life more complicated, the world is slippery (it's a frozen lake after all), so the agent's actions do not always turn out as expected: there is a 33% chance that it will slip to the right or to the left. You want the agent to move left, for example, but there is a 33% probability that it will indeed move left, a 33% chance that it will end up in the cell above, and a 33% chance that it will end up in the cell below. As we'll see at the end of the section, this makes progress difficult.\n",
    "\n",
    "<img width=\"300png\" src=\"assets/FrozenLake environment.png\">\n",
    "\n",
    "Let's look how this environment is represented in Gym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the gym library\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the FrozenLake environment\n",
    "e = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our __observation space__ is discrete, which means that it's just a number from zero to 15 (inclusive). Obviously, this number is our current position in the grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the observation space\n",
    "e.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __action space__ is also discrete, but can be from zero to three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the action space\n",
    "e.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the environment and return the initial observation\n",
    "e.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Render the environment\n",
    "e.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network from the CartPole example expects a vector of numbers. To get this, we can apply the traditional \"one-hot encoding\" of discrete inputs, which means that input to our network will have 16 float numbers and zero everywhere, except the index that we'll encode. To minimize changes in our code, we can use the ObservationWrapper class from Gym and implement our DiscreteOneHotWrapper class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classs for one-hot encoding the discrete inputs\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    \n",
    "    # The constructor\n",
    "    def __init__(self, env):\n",
    "        \n",
    "        # Call the parent's constructor to initialize itself\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        \n",
    "        # Check to make sure that the observation space is discrete type\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        \n",
    "        # The observation space\n",
    "        self.observation_space = gym.spaces.Box(low = 0.0, \n",
    "                                                high = 1.0, \n",
    "                                                shape = (env.observation_space.n, ), \n",
    "                                                dtype = np.float32)\n",
    "        \n",
    "    # TODO\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that wrapper applied to the environment, both the observation space and action space are 100% compatible with our CartPole solution (source code __Chapter04/02_ frozenlake_naive.py__). However, by launching it, we can see that this doesn't improve the score over time.\n",
    "\n",
    "<img width=\"700px\" src=\"./assets/report.png\">\n",
    "\n",
    "To understand what's going on, we need to look deeper at the reward structure of both environments. In CartPole, every step of the environment gives us the reward 1.0, until the moment that the pole falls. So, the longer our agent balanced the pole, the more reward it obtained. Due to randomness in our agent's behavior, different episodes were of different lengths, which gave us a pretty normal distribution of the episodes' rewards. After choosing a reward boundary, we rejected less successful episodes and learned how to repeat better ones (by training on successful episodes' data).\n",
    "\n",
    "This is shown in the following diagram:\n",
    "\n",
    "<img width=\"600px\" src=\"./assets/reward_distribution.png\">\n",
    "\n",
    "In the FrozenLake environment, episodes and their reward look different. We get\n",
    "the reward of 1.0 only when we reach the goal, and this reward says nothing about how good each episode was. Was it quick and efficient or did we make four rounds on the lake before we randomly stepped into the final cell? We don't know, it's just 1.0 reward and that's it. The distribution of rewards for our episodes are also problematic. There are only two kinds of episodes possible, with zero reward (failed) and one reward (successful), and failed episodes will obviously dominate in the beginning of the training. So, our percentile selection of \"elite\" episodes is totally wrong and gives us bad examples to train on. This is the reason for our training failure.\n",
    "\n",
    "<img width=\"600px\" src=\"./assets/reward_distribution_2.png\">\n",
    "\n",
    "This example shows us the limitations of the cross-entropy method:\n",
    "- For training, our episodes have to be finite and, preferably, short\n",
    "- The total reward for the episodes should have enough variability to separate good episodes from bad ones\n",
    "- There is no intermediate indication about whether the agent has succeeded or failed\n",
    "\n",
    "Later in the book, we'll become familiar with other methods, which address these limitations. For now, if you're curious about how FrozenLake can be solved using cross-entropy, here is a list of tweaks of the code that you need to make (the full example is in __Chapter04/03_frozenlake_tweaked.py__):\n",
    "- __Larger batches of played episodes:__ <br>In CartPole, it was enough to have 16 episodes on every iteration, but FrozenLake requires at least 100 just to get some successful episodes.<br><br>\n",
    "- __Discount factor applied to reward:__ <br>To make the total reward for the episode depend on episode length, and add variety in episodes, we can use a discounted total reward with the discount factor 0.9 or 0.95. In this case, the reward for shorter episodes will be higher than the reward for longer ones.<br><br>\n",
    "- __Keeping \"elite\" episodes for a longer time:__ <br>In the CartPole training, we sampled episodes from the environment, trained on the best ones, and threw them away. In FrozenLake, a successful episode is a much rarer animal, so we need to keep them for several iterations to train on them.<br><br>\n",
    "- __Decrease learning rate:__ <br>This will give our network time to average more training samples.<br><br>\n",
    "- __Much longer training time:__ <br>Due to the sparsity of successful episodes, and the random outcome of our actions, it's much harder for our network to get an idea of the best behavior to perform in any particular situation. To reach 50% successful episodes, about 5k training iterations are required.<br><br>\n",
    "\n",
    "To incorporate all these into our code, we need to change the filter_batch function to calculate discounted reward and return \"elite\" episodes for us to keep:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the boundary reward (which is used to filter elite episodes )\n",
    "def filter_batch(batch, percentile):\n",
    "    \n",
    "    # Get the discounted rewards of each batch\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))\n",
    "    \n",
    "    # Obtain the boundary reward\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "    \n",
    "    # Initialize an empty list for training observations\n",
    "    train_obs = []\n",
    "    \n",
    "    # Initialize an empty list for training actions\n",
    "    train_act = []\n",
    "    \n",
    "    # Initialize an empty list for elite batch\n",
    "    elite_batch = []\n",
    "    \n",
    "    # Iterate through each episode and discounted reward in the batch\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        \n",
    "        # If discounted reward is higher than boundary reward\n",
    "        if discounted_reward > reward_bound:\n",
    "            \n",
    "            # Populate the list of observations we will train on    \n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            \n",
    "            # Populate the list of actions we will train on    \n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            \n",
    "            # Append the episode to elite_batch list\n",
    "            elite_batch.append(example)\n",
    "            \n",
    "    return elite_batch, train_obs, train_act, reward_bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, in the training loop, we will store previous \"elite\" episodes to pass them to the preceding function on the next training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list for the full batch\n",
    "full_batch = []\n",
    "\n",
    "# Iterate through batches\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    \n",
    "    # Get the mean reward\n",
    "    reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "    \n",
    "    # Get the full batch, observations, actions, and boundary reward\n",
    "    full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
    "    \n",
    "    # If full_batch is not true then start the loop again\n",
    "    if not full_batch:\n",
    "        continue\n",
    "        \n",
    "    # Convert the observations into a tensor    \n",
    "    obs_v = torch.FloatTensor(obs)\n",
    "    \n",
    "    # Convert the actions into a tensor\n",
    "    acts_v = torch.LongTensor(acts)\n",
    "    \n",
    "    # Get the last 500 batches and delete the rest\n",
    "    full_batch = full_batch[-500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the code is the same, except that the learning rate decreased 10 times and the BATCH_SIZE was set to 100. After a period of patient waiting (the new version takes about one and a half hours to finish 10k iterations), we can see that the training of the model stopped improving around 55% of solved episodes. There are ways\n",
    "to address this (by applying entropy loss regularization, for example), but those techniques will be discussed in the upcoming chapters.\n",
    "\n",
    "<img width=\"700px\" src=\"./assets/convergence.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final point to note here is the effect of \"slipperiness\" in the FrozenLake environment. Each of our actions with 33% probability is replaced with the 90Â° rotated one (the \"up\" action, for instance, will succeed with 0.33 probability and with 0.33 chance that it will be replaced with the \"left\" action and 0.33 with the \"right\" action).\n",
    "\n",
    "The nonslippery version is in __Chapter04/04_frozenlake_nonslippery.py__, and the only difference is in the environment creation (we need to peek into the core of Gym to create the instance of the environment with tweaked arguments):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Create the FrozenLake environment that is not slippery\n",
    "    env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "    \n",
    "    # Give a time limit of 100 steps\n",
    "    env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "    \n",
    "    # Create a One-Hot Encoding for each of the states in the environment\n",
    "    env = DiscreteOneHotWrapper(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect is dramatic! The nonslippery version of the environment can be solved in 120-140 batch iterations, which is 100 times faster than the noisy environment:\n",
    "\n",
    "                    rl_book_samples/Chapter04$ ./04_frozenlake_nonslippery.py\n",
    "                    0: loss=1.379, reward_mean=0.010, reward_bound=0.000, batch=1\n",
    "                    1: loss=1.375, reward_mean=0.010, reward_bound=0.000, batch=2\n",
    "                    2: loss=1.359, reward_mean=0.010, reward_bound=0.000, batch=3\n",
    "                    3: loss=1.361, reward_mean=0.010, reward_bound=0.000, batch=4\n",
    "                    4: loss=1.355, reward_mean=0.000, reward_bound=0.000, batch=4\n",
    "                    5: loss=1.342, reward_mean=0.010, reward_bound=0.000, batch=5\n",
    "                    6: loss=1.353, reward_mean=0.020, reward_bound=0.000, batch=7\n",
    "                    7: loss=1.351, reward_mean=0.040, reward_bound=0.000, batch=11\n",
    "                    ......\n",
    "                    124: loss=0.484, reward_mean=0.680, reward_bound=0.000, batch=68\n",
    "                    125: loss=0.373, reward_mean=0.710, reward_bound=0.430, batch=114\n",
    "                    126: loss=0.305, reward_mean=0.690, reward_bound=0.478, batch=133\n",
    "                    128: loss=0.413, reward_mean=0.790, reward_bound=0.478, batch=73\n",
    "                    129: loss=0.297, reward_mean=0.810, reward_bound=0.478, batch=108\n",
    "                    Solved!\n",
    "                 \n",
    "<br>                 \n",
    "<img width=\"700px\" src=\"./assets/convergence_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 5. Theoretical background of the cross-entropy method\n",
    "\n",
    "---\n",
    "\n",
    "This section is optional and included for readers who are interested in why the method works. If you wish, you can refer to the original paper on cross-entropy, which will be given at the end of the section.\n",
    "The basis of the cross-entropy method lies in the importance sampling theorem, which states this:\n",
    "\n",
    "<img width=\"600px\" src=\"assets/formula_1.png\">\n",
    "\n",
    "In our RL case, H(x) is a reward value obtained by some policy x and p(x) is a distribution of all possible policies. We don't want to maximize our reward by searching all possible policies, instead we want to find a way to approximate p(x)H(x) by q(x), iteratively minimizing the distance between them. The distance between two probability distributions is calculated by Kullback-Leibler (KL) divergence which is as follows:\n",
    "\n",
    "<img width=\"600px\" src=\"assets/formula_2.png\">\n",
    "\n",
    "The first term in KL is called entropy and doesn't depend on that, so could be omitted during the minimization. The second term is called cross-entropy and is a very common optimization objective in DL.\n",
    "\n",
    "\n",
    "Combining both formulas, we can get an iterative algorithm, which starts with q0(x) = p(x) and on every step improves. This is an approximation of p(x)H(x) with an update:\n",
    "\n",
    "<img width=\"500px\" src=\"assets/formula_3.png\">\n",
    "\n",
    "This is a generic cross-entropy method, which can be significantly simplified in our RL case. Firstly, we replace our H(x) with an indicator function, which is 1 when the reward for the episode is above the threshold and 0 if the reward is below. Our policy update will look like this:\n",
    "\n",
    "<img width=\"500px\" src=\"assets/formula_4.png\">\n",
    "\n",
    "Strictly speaking, the preceding formula misses the normalization term, but it still works in practice without it. So, the method is quite clear: we sample episodes using our current policy (starting with some random initial policy) and minimize the negative log likelihood of the most successful samples and our policy.\n",
    "There is a whole book dedicated to this method, written by Dirk P. Kroese. A shorter description can be found in the Cross-Entropy Method paper by Dirk P.Kroese ( https://people.smp.uq.edu.au/DirkKroese/ps/eormsCE.pdf )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 6. FrozenLake Naive\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, gym.spaces\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.387, reward_mean=0.0, reward_bound=0.0\n",
      "1: loss=1.367, reward_mean=0.0, reward_bound=0.0\n",
      "2: loss=1.340, reward_mean=0.0, reward_bound=0.0\n",
      "3: loss=1.342, reward_mean=0.0, reward_bound=0.0\n",
      "4: loss=1.267, reward_mean=0.1, reward_bound=0.0\n",
      "5: loss=1.237, reward_mean=0.0, reward_bound=0.0\n",
      "6: loss=1.219, reward_mean=0.1, reward_bound=0.0\n",
      "7: loss=1.134, reward_mean=0.0, reward_bound=0.0\n",
      "8: loss=1.134, reward_mean=0.0, reward_bound=0.0\n",
      "9: loss=1.086, reward_mean=0.0, reward_bound=0.0\n",
      "10: loss=1.014, reward_mean=0.0, reward_bound=0.0\n",
      "11: loss=1.081, reward_mean=0.0, reward_bound=0.0\n",
      "12: loss=0.939, reward_mean=0.0, reward_bound=0.0\n",
      "13: loss=1.055, reward_mean=0.0, reward_bound=0.0\n",
      "14: loss=0.941, reward_mean=0.0, reward_bound=0.0\n",
      "15: loss=0.981, reward_mean=0.0, reward_bound=0.0\n",
      "16: loss=0.939, reward_mean=0.0, reward_bound=0.0\n",
      "17: loss=0.930, reward_mean=0.0, reward_bound=0.0\n",
      "18: loss=0.995, reward_mean=0.0, reward_bound=0.0\n",
      "19: loss=0.910, reward_mean=0.1, reward_bound=0.0\n",
      "20: loss=0.952, reward_mean=0.0, reward_bound=0.0\n",
      "21: loss=0.994, reward_mean=0.0, reward_bound=0.0\n",
      "22: loss=0.899, reward_mean=0.0, reward_bound=0.0\n",
      "23: loss=0.948, reward_mean=0.1, reward_bound=0.0\n",
      "24: loss=1.039, reward_mean=0.0, reward_bound=0.0\n",
      "25: loss=1.090, reward_mean=0.0, reward_bound=0.0\n",
      "26: loss=0.960, reward_mean=0.0, reward_bound=0.0\n",
      "27: loss=0.972, reward_mean=0.0, reward_bound=0.0\n",
      "28: loss=1.068, reward_mean=0.0, reward_bound=0.0\n",
      "29: loss=1.110, reward_mean=0.0, reward_bound=0.0\n",
      "30: loss=1.011, reward_mean=0.0, reward_bound=0.0\n",
      "31: loss=0.920, reward_mean=0.0, reward_bound=0.0\n",
      "32: loss=1.086, reward_mean=0.0, reward_bound=0.0\n",
      "33: loss=1.047, reward_mean=0.0, reward_bound=0.0\n",
      "34: loss=0.932, reward_mean=0.0, reward_bound=0.0\n",
      "35: loss=1.109, reward_mean=0.0, reward_bound=0.0\n",
      "36: loss=0.992, reward_mean=0.0, reward_bound=0.0\n",
      "37: loss=0.959, reward_mean=0.0, reward_bound=0.0\n",
      "38: loss=0.954, reward_mean=0.0, reward_bound=0.0\n",
      "39: loss=1.037, reward_mean=0.0, reward_bound=0.0\n",
      "40: loss=0.885, reward_mean=0.0, reward_bound=0.0\n",
      "41: loss=0.876, reward_mean=0.0, reward_bound=0.0\n",
      "42: loss=0.851, reward_mean=0.0, reward_bound=0.0\n",
      "43: loss=0.856, reward_mean=0.0, reward_bound=0.0\n",
      "44: loss=0.900, reward_mean=0.0, reward_bound=0.0\n",
      "45: loss=0.883, reward_mean=0.1, reward_bound=0.0\n",
      "46: loss=0.826, reward_mean=0.0, reward_bound=0.0\n",
      "47: loss=0.890, reward_mean=0.0, reward_bound=0.0\n",
      "48: loss=0.878, reward_mean=0.0, reward_bound=0.0\n",
      "49: loss=0.845, reward_mean=0.0, reward_bound=0.0\n",
      "50: loss=0.766, reward_mean=0.0, reward_bound=0.0\n",
      "51: loss=0.913, reward_mean=0.0, reward_bound=0.0\n",
      "52: loss=0.839, reward_mean=0.0, reward_bound=0.0\n",
      "53: loss=0.898, reward_mean=0.0, reward_bound=0.0\n",
      "54: loss=0.923, reward_mean=0.0, reward_bound=0.0\n",
      "55: loss=0.800, reward_mean=0.0, reward_bound=0.0\n",
      "56: loss=0.780, reward_mean=0.0, reward_bound=0.0\n",
      "57: loss=0.772, reward_mean=0.0, reward_bound=0.0\n",
      "58: loss=0.861, reward_mean=0.0, reward_bound=0.0\n",
      "59: loss=0.784, reward_mean=0.0, reward_bound=0.0\n",
      "60: loss=0.736, reward_mean=0.1, reward_bound=0.0\n",
      "61: loss=0.779, reward_mean=0.0, reward_bound=0.0\n",
      "62: loss=0.874, reward_mean=0.0, reward_bound=0.0\n",
      "63: loss=0.664, reward_mean=0.0, reward_bound=0.0\n",
      "64: loss=0.852, reward_mean=0.0, reward_bound=0.0\n",
      "65: loss=0.763, reward_mean=0.0, reward_bound=0.0\n",
      "66: loss=0.723, reward_mean=0.0, reward_bound=0.0\n",
      "67: loss=0.711, reward_mean=0.0, reward_bound=0.0\n",
      "68: loss=0.836, reward_mean=0.0, reward_bound=0.0\n",
      "69: loss=0.814, reward_mean=0.0, reward_bound=0.0\n",
      "70: loss=0.787, reward_mean=0.0, reward_bound=0.0\n",
      "71: loss=0.815, reward_mean=0.0, reward_bound=0.0\n",
      "72: loss=0.664, reward_mean=0.0, reward_bound=0.0\n",
      "73: loss=0.745, reward_mean=0.0, reward_bound=0.0\n",
      "74: loss=0.777, reward_mean=0.0, reward_bound=0.0\n",
      "75: loss=0.856, reward_mean=0.0, reward_bound=0.0\n",
      "76: loss=0.821, reward_mean=0.0, reward_bound=0.0\n",
      "77: loss=0.782, reward_mean=0.0, reward_bound=0.0\n",
      "78: loss=0.683, reward_mean=0.0, reward_bound=0.0\n",
      "79: loss=0.649, reward_mean=0.0, reward_bound=0.0\n",
      "80: loss=0.795, reward_mean=0.1, reward_bound=0.0\n",
      "81: loss=0.678, reward_mean=0.0, reward_bound=0.0\n",
      "82: loss=0.657, reward_mean=0.0, reward_bound=0.0\n",
      "83: loss=0.460, reward_mean=0.0, reward_bound=0.0\n",
      "84: loss=0.527, reward_mean=0.1, reward_bound=0.0\n",
      "85: loss=0.608, reward_mean=0.0, reward_bound=0.0\n",
      "86: loss=0.514, reward_mean=0.1, reward_bound=0.0\n",
      "87: loss=0.616, reward_mean=0.0, reward_bound=0.0\n",
      "88: loss=0.562, reward_mean=0.0, reward_bound=0.0\n",
      "89: loss=0.674, reward_mean=0.0, reward_bound=0.0\n",
      "90: loss=0.341, reward_mean=0.0, reward_bound=0.0\n",
      "91: loss=0.587, reward_mean=0.1, reward_bound=0.0\n",
      "92: loss=0.517, reward_mean=0.0, reward_bound=0.0\n",
      "93: loss=0.435, reward_mean=0.0, reward_bound=0.0\n",
      "94: loss=0.508, reward_mean=0.0, reward_bound=0.0\n",
      "95: loss=0.487, reward_mean=0.1, reward_bound=0.0\n",
      "96: loss=0.460, reward_mean=0.0, reward_bound=0.0\n",
      "97: loss=0.422, reward_mean=0.1, reward_bound=0.0\n",
      "98: loss=0.495, reward_mean=0.0, reward_bound=0.0\n",
      "99: loss=0.346, reward_mean=0.0, reward_bound=0.0\n",
      "100: loss=0.444, reward_mean=0.0, reward_bound=0.0\n",
      "101: loss=0.443, reward_mean=0.0, reward_bound=0.0\n",
      "102: loss=0.584, reward_mean=0.0, reward_bound=0.0\n",
      "103: loss=0.461, reward_mean=0.0, reward_bound=0.0\n",
      "104: loss=0.475, reward_mean=0.0, reward_bound=0.0\n",
      "105: loss=0.408, reward_mean=0.0, reward_bound=0.0\n",
      "106: loss=0.399, reward_mean=0.0, reward_bound=0.0\n",
      "107: loss=0.359, reward_mean=0.0, reward_bound=0.0\n",
      "108: loss=0.392, reward_mean=0.1, reward_bound=0.0\n",
      "109: loss=0.521, reward_mean=0.0, reward_bound=0.0\n",
      "110: loss=0.551, reward_mean=0.1, reward_bound=0.0\n",
      "111: loss=0.411, reward_mean=0.1, reward_bound=0.0\n",
      "112: loss=0.311, reward_mean=0.0, reward_bound=0.0\n",
      "113: loss=0.379, reward_mean=0.1, reward_bound=0.0\n",
      "114: loss=0.459, reward_mean=0.0, reward_bound=0.0\n",
      "115: loss=0.276, reward_mean=0.0, reward_bound=0.0\n",
      "116: loss=0.235, reward_mean=0.1, reward_bound=0.0\n",
      "117: loss=0.370, reward_mean=0.0, reward_bound=0.0\n",
      "118: loss=0.323, reward_mean=0.0, reward_bound=0.0\n",
      "119: loss=0.191, reward_mean=0.1, reward_bound=0.0\n",
      "120: loss=0.255, reward_mean=0.0, reward_bound=0.0\n",
      "121: loss=0.361, reward_mean=0.0, reward_bound=0.0\n",
      "122: loss=0.381, reward_mean=0.1, reward_bound=0.0\n",
      "123: loss=0.356, reward_mean=0.0, reward_bound=0.0\n",
      "124: loss=0.351, reward_mean=0.1, reward_bound=0.0\n",
      "125: loss=0.330, reward_mean=0.1, reward_bound=0.0\n",
      "126: loss=0.265, reward_mean=0.1, reward_bound=0.0\n",
      "127: loss=0.333, reward_mean=0.1, reward_bound=0.0\n",
      "128: loss=0.522, reward_mean=0.0, reward_bound=0.0\n",
      "129: loss=0.223, reward_mean=0.1, reward_bound=0.0\n",
      "130: loss=0.412, reward_mean=0.1, reward_bound=0.0\n",
      "131: loss=0.297, reward_mean=0.1, reward_bound=0.0\n",
      "132: loss=0.228, reward_mean=0.1, reward_bound=0.0\n",
      "133: loss=0.284, reward_mean=0.0, reward_bound=0.0\n",
      "134: loss=0.432, reward_mean=0.0, reward_bound=0.0\n",
      "135: loss=0.160, reward_mean=0.0, reward_bound=0.0\n",
      "136: loss=0.194, reward_mean=0.1, reward_bound=0.0\n",
      "137: loss=0.278, reward_mean=0.0, reward_bound=0.0\n",
      "138: loss=0.266, reward_mean=0.0, reward_bound=0.0\n",
      "139: loss=0.316, reward_mean=0.0, reward_bound=0.0\n",
      "140: loss=0.243, reward_mean=0.0, reward_bound=0.0\n",
      "141: loss=0.353, reward_mean=0.0, reward_bound=0.0\n",
      "142: loss=0.292, reward_mean=0.0, reward_bound=0.0\n",
      "143: loss=0.065, reward_mean=0.0, reward_bound=0.0\n",
      "144: loss=0.242, reward_mean=0.1, reward_bound=0.0\n",
      "145: loss=0.239, reward_mean=0.0, reward_bound=0.0\n",
      "146: loss=0.321, reward_mean=0.0, reward_bound=0.0\n",
      "147: loss=0.494, reward_mean=0.0, reward_bound=0.0\n",
      "148: loss=0.394, reward_mean=0.0, reward_bound=0.0\n",
      "149: loss=0.224, reward_mean=0.1, reward_bound=0.0\n",
      "150: loss=0.251, reward_mean=0.0, reward_bound=0.0\n",
      "151: loss=0.471, reward_mean=0.1, reward_bound=0.0\n",
      "152: loss=0.209, reward_mean=0.1, reward_bound=0.0\n",
      "153: loss=0.301, reward_mean=0.0, reward_bound=0.0\n",
      "154: loss=0.337, reward_mean=0.0, reward_bound=0.0\n",
      "155: loss=0.253, reward_mean=0.1, reward_bound=0.0\n",
      "156: loss=0.367, reward_mean=0.0, reward_bound=0.0\n",
      "157: loss=0.407, reward_mean=0.0, reward_bound=0.0\n",
      "158: loss=0.331, reward_mean=0.0, reward_bound=0.0\n",
      "159: loss=0.352, reward_mean=0.0, reward_bound=0.0\n",
      "160: loss=0.402, reward_mean=0.0, reward_bound=0.0\n",
      "161: loss=0.582, reward_mean=0.0, reward_bound=0.0\n",
      "162: loss=0.417, reward_mean=0.0, reward_bound=0.0\n",
      "163: loss=0.486, reward_mean=0.0, reward_bound=0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164: loss=0.363, reward_mean=0.0, reward_bound=0.0\n",
      "165: loss=0.463, reward_mean=0.1, reward_bound=0.0\n",
      "166: loss=0.412, reward_mean=0.0, reward_bound=0.0\n",
      "167: loss=0.241, reward_mean=0.0, reward_bound=0.0\n",
      "168: loss=0.352, reward_mean=0.0, reward_bound=0.0\n",
      "169: loss=0.366, reward_mean=0.0, reward_bound=0.0\n",
      "170: loss=0.199, reward_mean=0.0, reward_bound=0.0\n",
      "171: loss=0.207, reward_mean=0.0, reward_bound=0.0\n",
      "172: loss=0.175, reward_mean=0.0, reward_bound=0.0\n",
      "173: loss=0.346, reward_mean=0.0, reward_bound=0.0\n",
      "174: loss=0.262, reward_mean=0.0, reward_bound=0.0\n",
      "175: loss=0.420, reward_mean=0.0, reward_bound=0.0\n",
      "176: loss=0.115, reward_mean=0.0, reward_bound=0.0\n",
      "177: loss=0.212, reward_mean=0.0, reward_bound=0.0\n",
      "178: loss=0.117, reward_mean=0.0, reward_bound=0.0\n",
      "179: loss=0.205, reward_mean=0.0, reward_bound=0.0\n",
      "180: loss=0.249, reward_mean=0.0, reward_bound=0.0\n",
      "181: loss=0.106, reward_mean=0.0, reward_bound=0.0\n",
      "182: loss=0.112, reward_mean=0.0, reward_bound=0.0\n",
      "183: loss=0.147, reward_mean=0.0, reward_bound=0.0\n",
      "184: loss=0.072, reward_mean=0.0, reward_bound=0.0\n",
      "185: loss=0.241, reward_mean=0.0, reward_bound=0.0\n",
      "186: loss=0.156, reward_mean=0.0, reward_bound=0.0\n",
      "187: loss=0.133, reward_mean=0.0, reward_bound=0.0\n",
      "188: loss=0.118, reward_mean=0.0, reward_bound=0.0\n",
      "189: loss=0.065, reward_mean=0.0, reward_bound=0.0\n",
      "190: loss=0.106, reward_mean=0.0, reward_bound=0.0\n",
      "191: loss=0.178, reward_mean=0.0, reward_bound=0.0\n",
      "192: loss=0.070, reward_mean=0.0, reward_bound=0.0\n",
      "193: loss=0.206, reward_mean=0.1, reward_bound=0.0\n",
      "194: loss=0.164, reward_mean=0.0, reward_bound=0.0\n",
      "195: loss=0.101, reward_mean=0.0, reward_bound=0.0\n",
      "196: loss=0.125, reward_mean=0.0, reward_bound=0.0\n",
      "197: loss=0.133, reward_mean=0.0, reward_bound=0.0\n",
      "198: loss=0.164, reward_mean=0.0, reward_bound=0.0\n",
      "199: loss=0.068, reward_mean=0.0, reward_bound=0.0\n",
      "200: loss=0.016, reward_mean=0.0, reward_bound=0.0\n",
      "201: loss=0.115, reward_mean=0.0, reward_bound=0.0\n",
      "202: loss=0.061, reward_mean=0.0, reward_bound=0.0\n",
      "203: loss=0.013, reward_mean=0.0, reward_bound=0.0\n",
      "204: loss=0.143, reward_mean=0.0, reward_bound=0.0\n",
      "205: loss=0.072, reward_mean=0.0, reward_bound=0.0\n",
      "206: loss=0.049, reward_mean=0.0, reward_bound=0.0\n",
      "207: loss=0.014, reward_mean=0.0, reward_bound=0.0\n",
      "208: loss=0.012, reward_mean=0.0, reward_bound=0.0\n",
      "209: loss=0.280, reward_mean=0.0, reward_bound=0.0\n",
      "210: loss=0.057, reward_mean=0.0, reward_bound=0.0\n",
      "211: loss=0.131, reward_mean=0.0, reward_bound=0.0\n",
      "212: loss=0.009, reward_mean=0.0, reward_bound=0.0\n",
      "213: loss=0.013, reward_mean=0.0, reward_bound=0.0\n",
      "214: loss=0.080, reward_mean=0.0, reward_bound=0.0\n",
      "215: loss=0.167, reward_mean=0.0, reward_bound=0.0\n",
      "216: loss=0.053, reward_mean=0.0, reward_bound=0.0\n",
      "217: loss=0.065, reward_mean=0.0, reward_bound=0.0\n",
      "218: loss=0.060, reward_mean=0.0, reward_bound=0.0\n",
      "219: loss=0.051, reward_mean=0.0, reward_bound=0.0\n",
      "220: loss=0.168, reward_mean=0.0, reward_bound=0.0\n",
      "221: loss=0.075, reward_mean=0.0, reward_bound=0.0\n",
      "222: loss=0.050, reward_mean=0.0, reward_bound=0.0\n",
      "223: loss=0.015, reward_mean=0.0, reward_bound=0.0\n",
      "224: loss=0.150, reward_mean=0.0, reward_bound=0.0\n",
      "225: loss=0.058, reward_mean=0.0, reward_bound=0.0\n",
      "226: loss=0.075, reward_mean=0.0, reward_bound=0.0\n",
      "227: loss=0.082, reward_mean=0.0, reward_bound=0.0\n",
      "228: loss=0.159, reward_mean=0.0, reward_bound=0.0\n",
      "229: loss=0.090, reward_mean=0.0, reward_bound=0.0\n",
      "230: loss=0.054, reward_mean=0.0, reward_bound=0.0\n",
      "231: loss=0.017, reward_mean=0.0, reward_bound=0.0\n",
      "232: loss=0.104, reward_mean=0.0, reward_bound=0.0\n",
      "233: loss=0.278, reward_mean=0.1, reward_bound=0.0\n",
      "234: loss=0.047, reward_mean=0.0, reward_bound=0.0\n",
      "235: loss=0.166, reward_mean=0.0, reward_bound=0.0\n",
      "236: loss=0.025, reward_mean=0.0, reward_bound=0.0\n",
      "237: loss=0.118, reward_mean=0.0, reward_bound=0.0\n",
      "238: loss=0.202, reward_mean=0.0, reward_bound=0.0\n",
      "239: loss=0.021, reward_mean=0.0, reward_bound=0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-naive\")\n",
    "\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        if reward_m > 0.8:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 7. FrozenLake Tweaked\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import gym.spaces\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.394, reward_mean=0.020, reward_bound=0.000, batch=2\n",
      "1: loss=1.386, reward_mean=0.010, reward_bound=0.000, batch=3\n",
      "2: loss=1.386, reward_mean=0.010, reward_bound=0.000, batch=4\n",
      "3: loss=1.382, reward_mean=0.000, reward_bound=0.000, batch=4\n",
      "4: loss=1.384, reward_mean=0.010, reward_bound=0.000, batch=5\n",
      "5: loss=1.378, reward_mean=0.040, reward_bound=0.000, batch=9\n",
      "6: loss=1.373, reward_mean=0.010, reward_bound=0.000, batch=10\n",
      "7: loss=1.373, reward_mean=0.010, reward_bound=0.000, batch=11\n",
      "8: loss=1.370, reward_mean=0.000, reward_bound=0.000, batch=11\n",
      "9: loss=1.368, reward_mean=0.010, reward_bound=0.000, batch=12\n",
      "10: loss=1.371, reward_mean=0.010, reward_bound=0.000, batch=13\n",
      "11: loss=1.369, reward_mean=0.000, reward_bound=0.000, batch=13\n",
      "12: loss=1.368, reward_mean=0.020, reward_bound=0.000, batch=15\n",
      "13: loss=1.367, reward_mean=0.000, reward_bound=0.000, batch=15\n",
      "14: loss=1.365, reward_mean=0.000, reward_bound=0.000, batch=15\n",
      "15: loss=1.364, reward_mean=0.010, reward_bound=0.000, batch=16\n",
      "16: loss=1.367, reward_mean=0.040, reward_bound=0.000, batch=20\n",
      "17: loss=1.369, reward_mean=0.020, reward_bound=0.000, batch=22\n",
      "18: loss=1.368, reward_mean=0.000, reward_bound=0.000, batch=22\n",
      "19: loss=1.369, reward_mean=0.030, reward_bound=0.000, batch=25\n",
      "20: loss=1.366, reward_mean=0.020, reward_bound=0.000, batch=27\n",
      "21: loss=1.364, reward_mean=0.010, reward_bound=0.000, batch=28\n",
      "22: loss=1.362, reward_mean=0.030, reward_bound=0.000, batch=31\n",
      "23: loss=1.358, reward_mean=0.050, reward_bound=0.000, batch=36\n",
      "24: loss=1.357, reward_mean=0.000, reward_bound=0.000, batch=36\n",
      "25: loss=1.356, reward_mean=0.030, reward_bound=0.000, batch=39\n",
      "26: loss=1.355, reward_mean=0.010, reward_bound=0.000, batch=40\n",
      "27: loss=1.356, reward_mean=0.010, reward_bound=0.000, batch=41\n",
      "28: loss=1.354, reward_mean=0.020, reward_bound=0.000, batch=43\n",
      "29: loss=1.352, reward_mean=0.060, reward_bound=0.000, batch=49\n",
      "30: loss=1.351, reward_mean=0.030, reward_bound=0.000, batch=52\n",
      "31: loss=1.350, reward_mean=0.020, reward_bound=0.000, batch=54\n",
      "32: loss=1.349, reward_mean=0.000, reward_bound=0.000, batch=54\n",
      "33: loss=1.350, reward_mean=0.040, reward_bound=0.000, batch=58\n",
      "34: loss=1.348, reward_mean=0.010, reward_bound=0.000, batch=59\n",
      "35: loss=1.349, reward_mean=0.020, reward_bound=0.000, batch=61\n",
      "36: loss=1.347, reward_mean=0.010, reward_bound=0.000, batch=62\n",
      "37: loss=1.347, reward_mean=0.000, reward_bound=0.000, batch=62\n",
      "38: loss=1.345, reward_mean=0.040, reward_bound=0.000, batch=66\n",
      "39: loss=1.345, reward_mean=0.010, reward_bound=0.000, batch=67\n",
      "40: loss=1.344, reward_mean=0.010, reward_bound=0.000, batch=68\n",
      "41: loss=1.343, reward_mean=0.010, reward_bound=0.000, batch=69\n",
      "42: loss=1.343, reward_mean=0.020, reward_bound=0.000, batch=71\n",
      "43: loss=1.342, reward_mean=0.000, reward_bound=0.000, batch=71\n",
      "44: loss=1.341, reward_mean=0.010, reward_bound=0.000, batch=72\n",
      "45: loss=1.339, reward_mean=0.010, reward_bound=0.000, batch=73\n",
      "46: loss=1.338, reward_mean=0.000, reward_bound=0.000, batch=73\n",
      "47: loss=1.338, reward_mean=0.000, reward_bound=0.000, batch=73\n",
      "48: loss=1.336, reward_mean=0.020, reward_bound=0.000, batch=75\n",
      "49: loss=1.335, reward_mean=0.030, reward_bound=0.000, batch=78\n",
      "50: loss=1.334, reward_mean=0.040, reward_bound=0.000, batch=82\n",
      "51: loss=1.333, reward_mean=0.010, reward_bound=0.000, batch=83\n",
      "52: loss=1.332, reward_mean=0.020, reward_bound=0.000, batch=85\n",
      "53: loss=1.331, reward_mean=0.020, reward_bound=0.000, batch=87\n",
      "54: loss=1.331, reward_mean=0.010, reward_bound=0.000, batch=88\n",
      "55: loss=1.330, reward_mean=0.020, reward_bound=0.000, batch=90\n",
      "56: loss=1.330, reward_mean=0.020, reward_bound=0.000, batch=92\n",
      "57: loss=1.329, reward_mean=0.020, reward_bound=0.000, batch=94\n",
      "58: loss=1.328, reward_mean=0.010, reward_bound=0.000, batch=95\n",
      "59: loss=1.327, reward_mean=0.010, reward_bound=0.000, batch=96\n",
      "60: loss=1.326, reward_mean=0.010, reward_bound=0.000, batch=97\n",
      "61: loss=1.323, reward_mean=0.030, reward_bound=0.000, batch=100\n",
      "62: loss=1.320, reward_mean=0.020, reward_bound=0.000, batch=102\n",
      "63: loss=1.319, reward_mean=0.020, reward_bound=0.000, batch=104\n",
      "64: loss=1.319, reward_mean=0.000, reward_bound=0.000, batch=104\n",
      "65: loss=1.316, reward_mean=0.030, reward_bound=0.000, batch=107\n",
      "66: loss=1.316, reward_mean=0.020, reward_bound=0.000, batch=109\n",
      "67: loss=1.315, reward_mean=0.000, reward_bound=0.000, batch=109\n",
      "68: loss=1.313, reward_mean=0.010, reward_bound=0.000, batch=110\n",
      "69: loss=1.313, reward_mean=0.030, reward_bound=0.000, batch=113\n",
      "70: loss=1.311, reward_mean=0.070, reward_bound=0.000, batch=120\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    random.seed(12345)\n",
    "    env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-tweaked\")\n",
    "\n",
    "    full_batch = []\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "        full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
    "        if not full_batch:\n",
    "            continue\n",
    "        obs_v = torch.FloatTensor(obs)\n",
    "        acts_v = torch.LongTensor(acts)\n",
    "        full_batch = full_batch[-500:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f, batch=%d\" % (\n",
    "            iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch)))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
    "        if reward_mean > 0.8:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 8. FrozenLake Non-Slippery\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym.wrappers\n",
    "import gym.envs.toy_text.frozen_lake\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 100\n",
    "PERCENTILE = 30\n",
    "GAMMA = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))\n",
    "    reward_bound = np.percentile(disc_rewards, percentile)\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    elite_batch = []\n",
    "    for example, discounted_reward in zip(batch, disc_rewards):\n",
    "        if discounted_reward > reward_bound:\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            elite_batch.append(example)\n",
    "\n",
    "    return elite_batch, train_obs, train_act, reward_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=1.373, reward_mean=0.010, reward_bound=0.000, batch=1\n",
      "1: loss=1.381, reward_mean=0.020, reward_bound=0.000, batch=3\n",
      "2: loss=1.386, reward_mean=0.010, reward_bound=0.000, batch=4\n",
      "3: loss=1.384, reward_mean=0.010, reward_bound=0.000, batch=5\n",
      "4: loss=1.379, reward_mean=0.000, reward_bound=0.000, batch=5\n",
      "5: loss=1.375, reward_mean=0.000, reward_bound=0.000, batch=5\n",
      "6: loss=1.375, reward_mean=0.020, reward_bound=0.000, batch=7\n",
      "7: loss=1.372, reward_mean=0.020, reward_bound=0.000, batch=9\n",
      "8: loss=1.370, reward_mean=0.020, reward_bound=0.000, batch=11\n",
      "9: loss=1.366, reward_mean=0.010, reward_bound=0.000, batch=12\n",
      "10: loss=1.362, reward_mean=0.020, reward_bound=0.000, batch=14\n",
      "11: loss=1.358, reward_mean=0.000, reward_bound=0.000, batch=14\n",
      "12: loss=1.353, reward_mean=0.040, reward_bound=0.000, batch=18\n",
      "13: loss=1.354, reward_mean=0.020, reward_bound=0.000, batch=20\n",
      "14: loss=1.351, reward_mean=0.050, reward_bound=0.000, batch=25\n",
      "15: loss=1.349, reward_mean=0.060, reward_bound=0.000, batch=31\n",
      "16: loss=1.345, reward_mean=0.010, reward_bound=0.000, batch=32\n",
      "17: loss=1.341, reward_mean=0.030, reward_bound=0.000, batch=35\n",
      "18: loss=1.338, reward_mean=0.040, reward_bound=0.000, batch=39\n",
      "19: loss=1.332, reward_mean=0.020, reward_bound=0.000, batch=41\n",
      "20: loss=1.328, reward_mean=0.020, reward_bound=0.000, batch=43\n",
      "21: loss=1.325, reward_mean=0.030, reward_bound=0.000, batch=46\n",
      "22: loss=1.320, reward_mean=0.030, reward_bound=0.000, batch=49\n",
      "23: loss=1.316, reward_mean=0.030, reward_bound=0.000, batch=52\n",
      "24: loss=1.315, reward_mean=0.050, reward_bound=0.000, batch=57\n",
      "25: loss=1.310, reward_mean=0.030, reward_bound=0.000, batch=60\n",
      "26: loss=1.305, reward_mean=0.050, reward_bound=0.000, batch=65\n",
      "27: loss=1.300, reward_mean=0.040, reward_bound=0.000, batch=69\n",
      "28: loss=1.295, reward_mean=0.070, reward_bound=0.000, batch=76\n",
      "29: loss=1.291, reward_mean=0.040, reward_bound=0.000, batch=80\n",
      "30: loss=1.286, reward_mean=0.080, reward_bound=0.000, batch=88\n",
      "31: loss=1.282, reward_mean=0.060, reward_bound=0.000, batch=94\n",
      "32: loss=1.277, reward_mean=0.080, reward_bound=0.000, batch=102\n",
      "33: loss=1.272, reward_mean=0.090, reward_bound=0.000, batch=111\n",
      "34: loss=1.267, reward_mean=0.080, reward_bound=0.000, batch=119\n",
      "35: loss=1.264, reward_mean=0.090, reward_bound=0.000, batch=128\n",
      "36: loss=1.259, reward_mean=0.130, reward_bound=0.000, batch=141\n",
      "37: loss=1.255, reward_mean=0.060, reward_bound=0.000, batch=147\n",
      "38: loss=1.251, reward_mean=0.050, reward_bound=0.000, batch=152\n",
      "39: loss=1.247, reward_mean=0.050, reward_bound=0.000, batch=157\n",
      "40: loss=1.244, reward_mean=0.050, reward_bound=0.000, batch=162\n",
      "41: loss=1.238, reward_mean=0.080, reward_bound=0.000, batch=170\n",
      "42: loss=1.233, reward_mean=0.100, reward_bound=0.000, batch=180\n",
      "43: loss=1.232, reward_mean=0.070, reward_bound=0.000, batch=187\n",
      "44: loss=1.227, reward_mean=0.090, reward_bound=0.000, batch=196\n",
      "45: loss=1.224, reward_mean=0.050, reward_bound=0.000, batch=201\n",
      "46: loss=1.217, reward_mean=0.100, reward_bound=0.023, batch=210\n",
      "47: loss=1.203, reward_mean=0.130, reward_bound=0.084, batch=217\n",
      "48: loss=1.198, reward_mean=0.100, reward_bound=0.109, batch=223\n",
      "49: loss=1.187, reward_mean=0.080, reward_bound=0.122, batch=223\n",
      "50: loss=1.178, reward_mean=0.100, reward_bound=0.135, batch=225\n",
      "51: loss=1.159, reward_mean=0.130, reward_bound=0.167, batch=220\n",
      "52: loss=1.145, reward_mean=0.120, reward_bound=0.185, batch=221\n",
      "53: loss=1.125, reward_mean=0.120, reward_bound=0.206, batch=215\n",
      "54: loss=1.110, reward_mean=0.110, reward_bound=0.229, batch=211\n",
      "55: loss=1.104, reward_mean=0.080, reward_bound=0.150, batch=217\n",
      "56: loss=1.081, reward_mean=0.150, reward_bound=0.254, batch=202\n",
      "57: loss=1.059, reward_mean=0.200, reward_bound=0.282, batch=196\n",
      "58: loss=1.034, reward_mean=0.220, reward_bound=0.314, batch=184\n",
      "59: loss=1.038, reward_mean=0.140, reward_bound=0.000, batch=198\n",
      "60: loss=1.034, reward_mean=0.120, reward_bound=0.104, batch=208\n",
      "61: loss=1.021, reward_mean=0.130, reward_bound=0.231, batch=215\n",
      "62: loss=1.006, reward_mean=0.250, reward_bound=0.314, batch=217\n",
      "63: loss=0.968, reward_mean=0.140, reward_bound=0.349, batch=175\n",
      "64: loss=0.954, reward_mean=0.260, reward_bound=0.356, batch=192\n",
      "65: loss=0.944, reward_mean=0.200, reward_bound=0.349, batch=203\n",
      "66: loss=0.883, reward_mean=0.170, reward_bound=0.387, batch=156\n",
      "67: loss=0.903, reward_mean=0.220, reward_bound=0.000, batch=178\n",
      "68: loss=0.881, reward_mean=0.270, reward_bound=0.282, batch=193\n",
      "69: loss=0.855, reward_mean=0.230, reward_bound=0.349, batch=199\n",
      "70: loss=0.841, reward_mean=0.310, reward_bound=0.405, batch=209\n",
      "71: loss=0.771, reward_mean=0.220, reward_bound=0.430, batch=144\n",
      "72: loss=0.791, reward_mean=0.330, reward_bound=0.314, batch=169\n",
      "73: loss=0.770, reward_mean=0.350, reward_bound=0.349, batch=181\n",
      "74: loss=0.756, reward_mean=0.350, reward_bound=0.387, batch=193\n",
      "75: loss=0.724, reward_mean=0.300, reward_bound=0.430, batch=182\n",
      "76: loss=0.714, reward_mean=0.260, reward_bound=0.400, batch=197\n",
      "77: loss=0.701, reward_mean=0.310, reward_bound=0.469, batch=208\n",
      "78: loss=0.690, reward_mean=0.390, reward_bound=0.478, batch=228\n",
      "79: loss=0.682, reward_mean=0.330, reward_bound=0.478, batch=240\n",
      "80: loss=0.577, reward_mean=0.430, reward_bound=0.478, batch=136\n",
      "81: loss=0.580, reward_mean=0.500, reward_bound=0.430, batch=160\n",
      "82: loss=0.571, reward_mean=0.380, reward_bound=0.430, batch=179\n",
      "83: loss=0.565, reward_mean=0.500, reward_bound=0.478, batch=203\n",
      "84: loss=0.526, reward_mean=0.430, reward_bound=0.478, batch=193\n",
      "85: loss=0.514, reward_mean=0.290, reward_bound=0.510, batch=205\n",
      "87: loss=0.846, reward_mean=0.380, reward_bound=0.000, batch=38\n",
      "88: loss=0.811, reward_mean=0.410, reward_bound=0.000, batch=79\n",
      "89: loss=0.743, reward_mean=0.510, reward_bound=0.295, batch=125\n",
      "90: loss=0.681, reward_mean=0.440, reward_bound=0.349, batch=149\n",
      "91: loss=0.633, reward_mean=0.480, reward_bound=0.387, batch=166\n",
      "92: loss=0.555, reward_mean=0.540, reward_bound=0.430, batch=163\n",
      "93: loss=0.446, reward_mean=0.560, reward_bound=0.478, batch=124\n",
      "94: loss=0.438, reward_mean=0.510, reward_bound=0.478, batch=143\n",
      "95: loss=0.429, reward_mean=0.630, reward_bound=0.478, batch=168\n",
      "97: loss=0.680, reward_mean=0.560, reward_bound=0.000, batch=56\n",
      "98: loss=0.630, reward_mean=0.570, reward_bound=0.349, batch=106\n",
      "99: loss=0.574, reward_mean=0.560, reward_bound=0.387, batch=142\n",
      "100: loss=0.509, reward_mean=0.540, reward_bound=0.430, batch=151\n",
      "101: loss=0.396, reward_mean=0.620, reward_bound=0.478, batch=126\n",
      "102: loss=0.390, reward_mean=0.570, reward_bound=0.478, batch=149\n",
      "104: loss=0.547, reward_mean=0.540, reward_bound=0.000, batch=54\n",
      "105: loss=0.453, reward_mean=0.650, reward_bound=0.430, batch=89\n",
      "106: loss=0.373, reward_mean=0.720, reward_bound=0.478, batch=100\n",
      "107: loss=0.369, reward_mean=0.630, reward_bound=0.478, batch=131\n",
      "109: loss=0.535, reward_mean=0.610, reward_bound=0.000, batch=61\n",
      "110: loss=0.430, reward_mean=0.610, reward_bound=0.430, batch=100\n",
      "111: loss=0.340, reward_mean=0.540, reward_bound=0.478, batch=100\n",
      "112: loss=0.335, reward_mean=0.610, reward_bound=0.478, batch=130\n",
      "114: loss=0.499, reward_mean=0.690, reward_bound=0.000, batch=69\n",
      "115: loss=0.409, reward_mean=0.700, reward_bound=0.430, batch=113\n",
      "116: loss=0.320, reward_mean=0.770, reward_bound=0.478, batch=128\n",
      "118: loss=0.548, reward_mean=0.670, reward_bound=0.000, batch=67\n",
      "119: loss=0.366, reward_mean=0.740, reward_bound=0.430, batch=113\n",
      "120: loss=0.308, reward_mean=0.680, reward_bound=0.478, batch=135\n",
      "122: loss=0.552, reward_mean=0.710, reward_bound=0.338, batch=70\n",
      "123: loss=0.301, reward_mean=0.770, reward_bound=0.478, batch=89\n",
      "125: loss=0.350, reward_mean=0.780, reward_bound=0.464, batch=70\n",
      "126: loss=0.301, reward_mean=0.730, reward_bound=0.478, batch=105\n",
      "128: loss=0.337, reward_mean=0.780, reward_bound=0.430, batch=65\n",
      "130: loss=0.372, reward_mean=0.820, reward_bound=0.478, batch=71\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    random.seed(12345)\n",
    "    env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
    "    #env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
    "    env = DiscreteOneHotWrapper(env)\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
    "    writer = SummaryWriter(comment=\"-frozenlake-nonslippery\")\n",
    "\n",
    "    full_batch = []\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
    "        full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
    "        if not full_batch:\n",
    "            continue\n",
    "        obs_v = torch.FloatTensor(obs)\n",
    "        acts_v = torch.LongTensor(acts)\n",
    "        full_batch = full_batch[-500:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f, batch=%d\" % (\n",
    "            iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch)))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_mean, iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_bound, iter_no)\n",
    "        if reward_mean > 0.8:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 9. Summary\n",
    "\n",
    "---\n",
    "\n",
    "In this chapter, we became familiar with the first RL method cross-entropy, which is simple but quite powerful, despite its limitations. We applied it to a CartPole environment (with huge success) and to FrozenLake (with much more modest success). This chapter ends the introductory part of the book.\n",
    "In the upcoming chapters, we will explore more complex, but more powerful tools of deep RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___THE END___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
